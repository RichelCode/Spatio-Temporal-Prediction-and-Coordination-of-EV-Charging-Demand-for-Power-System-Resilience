{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44741853-7e6e-4d6b-800a-11ca6a8546bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# IMPORTANT: limit CPU thread explosions (helps stop Paperspace kernels from dying)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import json\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2288e84a-6db7-4f74-bd83-c9de94166ae2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install scikit-learn joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5af6740e-5258-41fe-a1ec-2b274f64908b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import MultiTaskElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from joblib import Parallel, delayed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "181cf45e-d401-41d0-9d9e-0e7922a42d01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: Quadro P5000\n",
      "X: (2208, 1821, 6) (T,N,F)\n",
      "Y: (2208, 1821) (T,N)\n",
      "IN_LEN/OUT_LEN: 24 72\n",
      "N stations: 1821\n",
      "train/val/test starts: 1009 289 673\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "if DEVICE == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "DATA_PATH = Path(\"artifacts/pems_graph_dataset_strict.npz\")\n",
    "assert DATA_PATH.exists(), f\"Missing: {DATA_PATH}\"\n",
    "\n",
    "data = np.load(DATA_PATH, allow_pickle=True)\n",
    "\n",
    "X = data[\"X\"].astype(np.float32)         # (T, N, F)\n",
    "Y = data[\"Y\"].astype(np.float32)         # (T, N)   raw flow\n",
    "A = data[\"A\"].astype(np.float32)         # (N, N)\n",
    "stations = data[\"stations\"]\n",
    "timestamps = data[\"timestamps\"]\n",
    "\n",
    "train_starts = data[\"train_starts\"].astype(np.int64)\n",
    "val_starts   = data[\"val_starts\"].astype(np.int64)\n",
    "test_starts  = data[\"test_starts\"].astype(np.int64)\n",
    "\n",
    "IN_LEN  = int(np.array(data[\"in_len\"]).item())\n",
    "OUT_LEN = int(np.array(data[\"out_len\"]).item())\n",
    "\n",
    "flow_mean  = data[\"flow_mean\"].astype(np.float32)   # (N,)\n",
    "flow_std   = data[\"flow_std\"].astype(np.float32)    # (N,)\n",
    "speed_mean = data[\"speed_mean\"].astype(np.float32)  # (N,)\n",
    "speed_std  = data[\"speed_std\"].astype(np.float32)   # (N,)\n",
    "\n",
    "T, N, Fdim = X.shape\n",
    "print(\"X:\", X.shape, \"(T,N,F)\")\n",
    "print(\"Y:\", Y.shape, \"(T,N)\")\n",
    "print(\"IN_LEN/OUT_LEN:\", IN_LEN, OUT_LEN)\n",
    "print(\"N stations:\", N)\n",
    "print(\"train/val/test starts:\", len(train_starts), len(val_starts), len(test_starts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "446c23ff-f101-4d76-ada7-8a4e43bdbd55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF_all: (2208, 4)\n"
     ]
    }
   ],
   "source": [
    "def time_encoding(dt_index: pd.DatetimeIndex) -> np.ndarray:\n",
    "    hours = dt_index.hour.values\n",
    "    dow   = dt_index.dayofweek.values\n",
    "    hour_sin = np.sin(2*np.pi*hours/24.0)\n",
    "    hour_cos = np.cos(2*np.pi*hours/24.0)\n",
    "    dow_sin  = np.sin(2*np.pi*dow/7.0)\n",
    "    dow_cos  = np.cos(2*np.pi*dow/7.0)\n",
    "    return np.stack([hour_sin, hour_cos, dow_sin, dow_cos], axis=1).astype(np.float32)\n",
    "\n",
    "TF_all = time_encoding(pd.to_datetime(timestamps))  # (T,4)\n",
    "print(\"TF_all:\", TF_all.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7608e08-3960-4d5a-9bf3-513c054b9c4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity (scaled) flow mean/std ~ 0/1 on TRAIN-ish slice:\n",
      "Y_scaled mean/std: -781.403564453125 30704.76953125\n"
     ]
    }
   ],
   "source": [
    "# Avoid divide-by-zero\n",
    "flow_std  = np.maximum(flow_std,  1e-6).astype(np.float32)\n",
    "speed_std = np.maximum(speed_std, 1e-6).astype(np.float32)\n",
    "\n",
    "X_scaled = X.copy()\n",
    "# assume channel0=flow, channel1=speed (your pipeline)\n",
    "X_scaled[:, :, 0] = (X_scaled[:, :, 0] - flow_mean[None, :])  / flow_std[None, :]\n",
    "X_scaled[:, :, 1] = (X_scaled[:, :, 1] - speed_mean[None, :]) / speed_std[None, :]\n",
    "\n",
    "Y_scaled = (Y - flow_mean[None, :]) / flow_std[None, :]\n",
    "\n",
    "print(\"Sanity (scaled) flow mean/std ~ 0/1 on TRAIN-ish slice:\")\n",
    "print(\"Y_scaled mean/std:\", float(Y_scaled.mean()), float(Y_scaled.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73932264-08bc-4e37-a79a-eb7d6941e169",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch x: torch.Size([8, 6, 1821, 24]) Batch y: torch.Size([8, 72, 1821]) Batch tf: torch.Size([8, 72, 4])\n"
     ]
    }
   ],
   "source": [
    "X_fnt = np.transpose(X_scaled, (2, 1, 0)).copy()  # (F, N, T)\n",
    "\n",
    "class FastPemsWindowDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_fnt, Y_scaled, TF_all, starts, in_len, out_len):\n",
    "        self.X_fnt = X_fnt\n",
    "        self.Y = Y_scaled\n",
    "        self.TF = TF_all\n",
    "        self.starts = np.asarray(starts, dtype=np.int64)\n",
    "        self.in_len = int(in_len)\n",
    "        self.out_len = int(out_len)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.starts)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        s = int(self.starts[i])\n",
    "        x = self.X_fnt[:, :, s:s+self.in_len]                    # (F,N,IN)\n",
    "        y = self.Y[s+self.in_len:s+self.in_len+self.out_len, :]  # (OUT,N)\n",
    "        tf = self.TF[s+self.in_len:s+self.in_len+self.out_len]   # (OUT,4)\n",
    "        return torch.from_numpy(x), torch.from_numpy(y), torch.from_numpy(tf)\n",
    "\n",
    "train_ds = FastPemsWindowDataset(X_fnt, Y_scaled, TF_all, train_starts, IN_LEN, OUT_LEN)\n",
    "val_ds   = FastPemsWindowDataset(X_fnt, Y_scaled, TF_all, val_starts,   IN_LEN, OUT_LEN)\n",
    "test_ds  = FastPemsWindowDataset(X_fnt, Y_scaled, TF_all, test_starts,  IN_LEN, OUT_LEN)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0, pin_memory=False)\n",
    "val_loader   = torch.utils.data.DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=False)\n",
    "test_loader  = torch.utils.data.DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=False)\n",
    "\n",
    "xb, yb, tfb = next(iter(train_loader))\n",
    "print(\"Batch x:\", xb.shape, \"Batch y:\", yb.shape, \"Batch tf:\", tfb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27779656-4c9e-4c99-b671-9cc3c2aa5dc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EVAL_HORIZONS = [12, 24, 48, 72]\n",
    "H = len(EVAL_HORIZONS)\n",
    "\n",
    "flow_mean_t = torch.tensor(flow_mean, dtype=torch.float32, device=DEVICE).view(1, 1, -1)\n",
    "flow_std_t  = torch.tensor(flow_std,  dtype=torch.float32, device=DEVICE).view(1, 1, -1)\n",
    "\n",
    "def print_metrics(title, metrics):\n",
    "    print(\"\\n\" + title)\n",
    "    for h in sorted(metrics.keys()):\n",
    "        print(f\"  {h:>3}h  MAE={metrics[h]['MAE']:.3f}  RMSE={metrics[h]['RMSE']:.3f}\")\n",
    "\n",
    "def avg_mae(metrics):\n",
    "    return float(np.mean([metrics[h][\"MAE\"] for h in metrics]))\n",
    "\n",
    "@torch.inference_mode()\n",
    "def eval_horizons_fast(model, loader):\n",
    "    model.eval()\n",
    "    acc = {h: {\"abs\": 0.0, \"sq\": 0.0, \"count\": 0} for h in EVAL_HORIZONS}\n",
    "\n",
    "    for xb, yb, tfb in tqdm(loader, desc=\"Eval\", leave=False):\n",
    "        xb  = xb.to(DEVICE, non_blocking=True)\n",
    "        yb  = yb.to(DEVICE, non_blocking=True)\n",
    "        tfb = tfb.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        pred = model(xb, tfb)  # scaled (B,OUT,N)\n",
    "\n",
    "        pred_u = pred * flow_std_t + flow_mean_t\n",
    "        true_u = yb   * flow_std_t + flow_mean_t\n",
    "\n",
    "        for h in EVAL_HORIZONS:\n",
    "            idx = h - 1\n",
    "            err = pred_u[:, idx, :] - true_u[:, idx, :]\n",
    "            acc[h][\"abs\"]   += float(err.abs().sum())\n",
    "            acc[h][\"sq\"]    += float((err**2).sum())\n",
    "            acc[h][\"count\"] += err.numel()\n",
    "\n",
    "    metrics = {}\n",
    "    for h in EVAL_HORIZONS:\n",
    "        mae = acc[h][\"abs\"] / acc[h][\"count\"]\n",
    "        rmse = (acc[h][\"sq\"] / acc[h][\"count\"]) ** 0.5\n",
    "        metrics[h] = {\"MAE\": float(mae), \"RMSE\": float(rmse)}\n",
    "    return metrics\n",
    "\n",
    "def make_run_dir(model_name: str) -> Path:\n",
    "    ts = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = Path(\"artifacts/runs\") / f\"{ts}_{model_name}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=False)\n",
    "    return run_dir\n",
    "\n",
    "def save_metrics_files(run_dir: Path, split_name: str, metrics: dict):\n",
    "    (run_dir / f\"{split_name}_metrics.json\").write_text(json.dumps(metrics, indent=2))\n",
    "    rows = []\n",
    "    for h in sorted(metrics.keys()):\n",
    "        rows.append({\"horizon\": h, \"MAE\": metrics[h][\"MAE\"], \"RMSE\": metrics[h][\"RMSE\"]})\n",
    "    pd.DataFrame(rows).to_csv(run_dir / f\"{split_name}_metrics.csv\", index=False)\n",
    "\n",
    "def append_results_summary(model_name: str, run_dir: Path, test_metrics: dict):\n",
    "    summary_path = Path(\"artifacts/results_summary.csv\")\n",
    "    row = {\n",
    "        \"timestamp\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"model_name\": model_name,\n",
    "        \"run_dir\": str(run_dir),\n",
    "    }\n",
    "    for h in EVAL_HORIZONS:\n",
    "        row[f\"test_MAE_{h}h\"] = test_metrics[h][\"MAE\"]\n",
    "        row[f\"test_RMSE_{h}h\"] = test_metrics[h][\"RMSE\"]\n",
    "\n",
    "    df_new = pd.DataFrame([row])\n",
    "    if summary_path.exists():\n",
    "        df_old = pd.read_csv(summary_path)\n",
    "        df = pd.concat([df_old, df_new], ignore_index=True)\n",
    "    else:\n",
    "        df = df_new\n",
    "    df.to_csv(summary_path, index=False)\n",
    "    return summary_path\n",
    "\n",
    "def save_preds_npz_and_csv_subset(\n",
    "    run_dir: Path,\n",
    "    pred_u: np.ndarray,   # (S,N,H)\n",
    "    true_u: np.ndarray,   # (S,N,H)\n",
    "    starts: np.ndarray,\n",
    "    max_stations_csv: int = 300,\n",
    "):\n",
    "    # NPZ (full)\n",
    "    np.savez_compressed(\n",
    "        run_dir / \"test_pred_true_selected_horizons.npz\",\n",
    "        pred=pred_u.astype(np.float32),\n",
    "        true=true_u.astype(np.float32),\n",
    "        horizons=np.array(EVAL_HORIZONS, dtype=np.int64),\n",
    "        starts=starts.astype(np.int64),\n",
    "        stations=stations,\n",
    "        timestamps=timestamps\n",
    "    )\n",
    "\n",
    "    # CSV subset (manageable)\n",
    "    K = min(max_stations_csv, N)\n",
    "    keep = np.arange(K)\n",
    "\n",
    "    frames = []\n",
    "    for j, h in enumerate(EVAL_HORIZONS):\n",
    "        idx = starts + IN_LEN + (h - 1)\n",
    "        ts_h = pd.to_datetime(timestamps[idx])\n",
    "\n",
    "        df_h = pd.DataFrame({\n",
    "            \"start_idx\": np.repeat(starts, K),\n",
    "            \"timestamp\": np.repeat(ts_h, K),\n",
    "            \"station\": np.tile(np.array(stations)[keep], len(starts)),\n",
    "            \"horizon_h\": h,\n",
    "            \"y_true\": true_u[:, keep, j].reshape(-1),\n",
    "            \"y_pred\": pred_u[:, keep, j].reshape(-1),\n",
    "        })\n",
    "        frames.append(df_h)\n",
    "\n",
    "    df_out = pd.concat(frames, ignore_index=True)\n",
    "    df_out.to_csv(run_dir / \"test_pred_true_selected_horizons.csv\", index=False)\n",
    "    return run_dir / \"test_pred_true_selected_horizons.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6c7345-198a-42f0-8ac0-ab1396f54181",
   "metadata": {},
   "source": [
    "## LSTM MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98ef7f28-7104-47a4-b1b0-a3210efe7f37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTM_Baseline(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_len: int, hidden: int = 64, layers: int = 1, dropout: float = 0.1, tf_dim: int = 4):\n",
    "        super().__init__()\n",
    "        self.out_len = out_len\n",
    "        self.hidden = hidden\n",
    "        self.lstm = nn.LSTM(input_size=in_dim, hidden_size=hidden, num_layers=layers, dropout=(dropout if layers > 1 else 0.0), batch_first=True)\n",
    "        self.head = nn.Linear(hidden + tf_dim, 1)\n",
    "\n",
    "    def forward(self, x, tf):\n",
    "        # x: (B,F,N,IN) -> (B,N,IN,F) -> (B*N, IN, F)\n",
    "        B, F, Nn, INL = x.shape\n",
    "        x_seq = x.permute(0, 2, 3, 1).contiguous().view(B * Nn, INL, F)\n",
    "\n",
    "        out, (h, c) = self.lstm(x_seq)\n",
    "        h_last = h[-1]  # (B*N, hidden)\n",
    "\n",
    "        # tf: (B, OUT, 4) -> repeat per node -> (B*N, OUT, 4)\n",
    "        tf_rep = tf.unsqueeze(1).expand(B, Nn, self.out_len, tf.shape[-1]).contiguous().view(B * Nn, self.out_len, tf.shape[-1])\n",
    "\n",
    "        h_rep = h_last.unsqueeze(1).expand(B * Nn, self.out_len, self.hidden)\n",
    "        z = torch.cat([h_rep, tf_rep], dim=-1)           # (B*N, OUT, hidden+4)\n",
    "        y = self.head(z).squeeze(-1)                    # (B*N, OUT)\n",
    "        y = y.view(B, Nn, self.out_len).permute(0, 2, 1) # (B, OUT, N)\n",
    "        return y\n",
    "\n",
    "def train_torch_and_save(model_name: str, model: nn.Module, epochs=40, lr=1e-3, weight_decay=1e-4, clip=5.0, patience=6, eval_every=2):\n",
    "    run_dir = make_run_dir(model_name)\n",
    "    print(\"Run dir:\", run_dir)\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    loss_fn = nn.SmoothL1Loss(beta=1.0)\n",
    "\n",
    "    best = float(\"inf\")\n",
    "    bad = 0\n",
    "    best_state = None\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "\n",
    "        for xb, yb, tfb in tqdm(train_loader, desc=f\"Train {epoch}/{epochs}\", leave=False):\n",
    "            xb = xb.to(DEVICE, non_blocking=True)\n",
    "            yb = yb.to(DEVICE, non_blocking=True)\n",
    "            tfb = tfb.to(DEVICE, non_blocking=True)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            pred = model(xb, tfb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            opt.step()\n",
    "            running += float(loss.item())\n",
    "\n",
    "        if epoch % eval_every == 0:\n",
    "            val_m = eval_horizons_fast(model, val_loader)\n",
    "            score = avg_mae(val_m)\n",
    "            print(f\"\\nEpoch {epoch}: train_loss={running/len(train_loader):.6f}  val_avg_MAE={score:.3f}\")\n",
    "            print_metrics(\"VAL\", val_m)\n",
    "\n",
    "            history.append({\"epoch\": epoch, \"train_loss\": running/len(train_loader), \"val_avg_MAE\": score, **{f\"val_MAE_{h}h\": val_m[h][\"MAE\"] for h in EVAL_HORIZONS}})\n",
    "\n",
    "            if score < best:\n",
    "                best = score\n",
    "                bad = 0\n",
    "                best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            else:\n",
    "                bad += 1\n",
    "                if bad >= patience:\n",
    "                    print(f\"\\nEarly stopping. Best val_avg_MAE={best:.3f}\")\n",
    "                    break\n",
    "\n",
    "    # save history\n",
    "    if len(history) > 0:\n",
    "        pd.DataFrame(history).to_csv(run_dir / \"history.csv\", index=False)\n",
    "\n",
    "    # load best + save checkpoint\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    torch.save(model.state_dict(), run_dir / \"best.pt\")\n",
    "\n",
    "    # TEST\n",
    "    print(\"\\nEvaluating on TEST set...\")\n",
    "    test_m = eval_horizons_fast(model, test_loader)\n",
    "    print_metrics(f\"{model_name} â€” TEST\", test_m)\n",
    "\n",
    "    save_metrics_files(run_dir, \"test\", test_m)\n",
    "\n",
    "    # Collect & save preds (selected horizons only)\n",
    "    S = len(test_starts)\n",
    "    pred_u = np.zeros((S, N, len(EVAL_HORIZONS)), dtype=np.float32)\n",
    "    true_u = np.zeros((S, N, len(EVAL_HORIZONS)), dtype=np.float32)\n",
    "\n",
    "    model.eval()\n",
    "    pos = 0\n",
    "    with torch.inference_mode():\n",
    "        for xb, yb, tfb in tqdm(test_loader, desc=\"Collect preds\", leave=False):\n",
    "            bsz = xb.shape[0]\n",
    "            xb  = xb.to(DEVICE)\n",
    "            yb  = yb.to(DEVICE)\n",
    "            tfb = tfb.to(DEVICE)\n",
    "\n",
    "            pred = model(xb, tfb)  # scaled (B,OUT,N)\n",
    "            pred_u_b = (pred * flow_std_t + flow_mean_t)  # (B,OUT,N)\n",
    "            true_u_b = (yb   * flow_std_t + flow_mean_t)\n",
    "\n",
    "            for j, h in enumerate(EVAL_HORIZONS):\n",
    "                idx = h - 1\n",
    "                pred_u[pos:pos+bsz, :, j] = pred_u_b[:, idx, :].detach().cpu().numpy()\n",
    "                true_u[pos:pos+bsz, :, j] = true_u_b[:, idx, :].detach().cpu().numpy()\n",
    "\n",
    "            pos += bsz\n",
    "\n",
    "    csv_path = save_preds_npz_and_csv_subset(run_dir, pred_u, true_u, test_starts, max_stations_csv=300)\n",
    "    summary_path = append_results_summary(model_name, run_dir, test_m)\n",
    "\n",
    "    print(\"\\nSaved run outputs to:\", run_dir)\n",
    "    print(\" - best checkpoint:\", run_dir / \"best.pt\")\n",
    "    print(\" - history:\", run_dir / \"history.csv\")\n",
    "    print(\" - test metrics:\", run_dir / \"test_metrics.json\")\n",
    "    print(\" - preds npz:\", run_dir / \"test_pred_true_selected_horizons.npz\")\n",
    "    print(\" - preds csv:\", csv_path)\n",
    "    print(\" - master summary:\", summary_path)\n",
    "\n",
    "    return run_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f66978cc-854b-43f6-a6fe-0e15004fc1ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run dir: artifacts/runs/20260210_191832_LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.69 GiB. GPU 0 has a total capacty of 15.88 GiB of which 341.69 MiB is free. Process 2300183 has 13.59 GiB memory in use. Process 2696991 has 1.96 GiB memory in use. Of the allocated memory 157.09 MiB is allocated by PyTorch, and 1.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m lstm_base \u001b[38;5;241m=\u001b[39m LSTM_Baseline(in_dim\u001b[38;5;241m=\u001b[39mFdim, out_len\u001b[38;5;241m=\u001b[39mOUT_LEN, hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m----> 2\u001b[0m run_dir_lstm \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_torch_and_save\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLSTM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlstm_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m run_dir_lstm\n",
      "Cell \u001b[0;32mIn[9], line 49\u001b[0m, in \u001b[0;36mtrain_torch_and_save\u001b[0;34m(model_name, model, epochs, lr, weight_decay, clip, patience, eval_every)\u001b[0m\n\u001b[1;32m     46\u001b[0m tfb \u001b[38;5;241m=\u001b[39m tfb\u001b[38;5;241m.\u001b[39mto(DEVICE, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     48\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 49\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtfb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, yb)\n\u001b[1;32m     51\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m, in \u001b[0;36mLSTM_Baseline.forward\u001b[0;34m(self, x, tf)\u001b[0m\n\u001b[1;32m     11\u001b[0m B, F, Nn, INL \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     12\u001b[0m x_seq \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(B \u001b[38;5;241m*\u001b[39m Nn, INL, F)\n\u001b[0;32m---> 14\u001b[0m out, (h, c) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m h_last \u001b[38;5;241m=\u001b[39m h[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# (B*N, hidden)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# tf: (B, OUT, 4) -> repeat per node -> (B*N, OUT, 4)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:879\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    876\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    882\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    883\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.69 GiB. GPU 0 has a total capacty of 15.88 GiB of which 341.69 MiB is free. Process 2300183 has 13.59 GiB memory in use. Process 2696991 has 1.96 GiB memory in use. Of the allocated memory 157.09 MiB is allocated by PyTorch, and 1.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "lstm_base = LSTM_Baseline(in_dim=Fdim, out_len=OUT_LEN, hidden=64, layers=1, dropout=0.1).to(DEVICE)\n",
    "run_dir_lstm = train_torch_and_save(\"LSTM\", lstm_base, epochs=40, patience=6, eval_every=2)\n",
    "run_dir_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a34902fd-11ad-4c44-bdba-764b8fb2c60e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Feb 10 19:34:08 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Quadro P5000                   On  |   00000000:00:05.0 Off |                  Off |\n",
      "| 26%   33C    P8              6W /  180W |   15925MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642fc28d-49e0-4a96-809e-95d3c448f5d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79fd0e56-d6da-4f9d-8d63-267f2a38bb8f",
   "metadata": {},
   "source": [
    "# Simulation Study: Controlled Spatio-Temporal Data Generation (PeMS-like)\n",
    "\n",
    "## Goal\n",
    "We generate synthetic spatio-temporal data with **known ground-truth dynamics** so we can evaluate\n",
    "whether different forecasting models exploit:\n",
    "- **Temporal dependence** (autoregressive structure + seasonality)\n",
    "- **Spatial dependence** (graph diffusion / neighbor influence)\n",
    "\n",
    "We keep the *graph topology* and *timestamps* from the PeMS-derived dataset to preserve realism\n",
    "(number of stations, time indexing, and sampling frequency), but we replace the observed flow/speed\n",
    "with simulated signals.\n",
    "\n",
    "## Key idea\n",
    "We introduce a spatial coupling coefficient **α ∈ [0, 1]**:\n",
    "- α = 0: stations evolve independently (no spatial structure)\n",
    "- α > 0: stations interact via the road-network adjacency matrix (graph diffusion)\n",
    "\n",
    "A good graph-based model should improve as α increases.\n",
    "\n",
    "## Outputs\n",
    "This notebook produces `.npz` files compatible with our training pipeline:\n",
    "- X: (T, N, 6) with [flow, speed, hour_sin, hour_cos, dow_sin, dow_cos]\n",
    "- Y: (T, N) flow target\n",
    "- A, stations, timestamps, train/val/test starts, in_len, out_len\n",
    "- flow_mean/std and speed_mean/std computed **using training indices only** (no leakage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c993ed4a-83a2-440d-b422-5f3e4e11bea5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE: /notebooks/Spatio-Temporal-Prediction-and-Coordination-of-EV-Charging-Demand-for-Power-System-Resilience\n",
      "DATA_PATH_REAL exists: True\n",
      "SIM_DIR: /notebooks/Spatio-Temporal-Prediction-and-Coordination-of-EV-Charging-Demand-for-Power-System-Resilience/artifacts/sim_datasets\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------------------------\n",
    "# Paths (match your repo layout)\n",
    "# ----------------------------\n",
    "BASE = Path(\".\").resolve()\n",
    "ART = BASE / \"artifacts\"\n",
    "DATA_PATH_REAL = ART / \"pems_graph_dataset_strict.npz\"   # your existing dataset file\n",
    "SIM_DIR = ART / \"sim_datasets\"\n",
    "SIM_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"BASE:\", BASE)\n",
    "print(\"DATA_PATH_REAL exists:\", DATA_PATH_REAL.exists())\n",
    "print(\"SIM_DIR:\", SIM_DIR)\n",
    "\n",
    "# ----------------------------\n",
    "# Reproducibility\n",
    "# ----------------------------\n",
    "GLOBAL_SEED = 42\n",
    "rng_global = np.random.default_rng(GLOBAL_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57a0049d-10b4-4cc2-b18d-d197dd2f45f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: (1821, 1821)\n",
      "Timestamps: 2208 | range: 2024-10-01T00:00:00.000000000 -> 2024-12-31T23:00:00.000000000\n",
      "Stations: 1821\n",
      "IN_LEN: 24 OUT_LEN: 72\n",
      "train/val/test starts: 1009 289 673\n"
     ]
    }
   ],
   "source": [
    "assert DATA_PATH_REAL.exists(), f\"Missing file: {DATA_PATH_REAL}\"\n",
    "\n",
    "ds = np.load(DATA_PATH_REAL, allow_pickle=True)\n",
    "\n",
    "A = ds[\"A\"].astype(np.float32)              # (N,N)\n",
    "stations = ds[\"stations\"]                   # (N,)\n",
    "timestamps = ds[\"timestamps\"].astype(\"datetime64[ns]\")  # (T,)\n",
    "\n",
    "train_starts = ds[\"train_starts\"].astype(np.int64)\n",
    "val_starts   = ds[\"val_starts\"].astype(np.int64)\n",
    "test_starts  = ds[\"test_starts\"].astype(np.int64)\n",
    "\n",
    "IN_LEN  = int(np.array(ds[\"in_len\"]).item())\n",
    "OUT_LEN = int(np.array(ds[\"out_len\"]).item())\n",
    "\n",
    "T = len(timestamps)\n",
    "N = A.shape[0]\n",
    "\n",
    "print(\"A:\", A.shape)\n",
    "print(\"Timestamps:\", T, \"| range:\", timestamps.min(), \"->\", timestamps.max())\n",
    "print(\"Stations:\", N)\n",
    "print(\"IN_LEN:\", IN_LEN, \"OUT_LEN:\", OUT_LEN)\n",
    "print(\"train/val/test starts:\", len(train_starts), len(val_starts), len(test_starts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb0e07de-0391-4145-a587-729b6ed8b4a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF: (2208, 4) min/max: -1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "def time_encoding(dt_index: pd.DatetimeIndex) -> np.ndarray:\n",
    "    \"\"\"Return (T,4): [hour_sin, hour_cos, dow_sin, dow_cos].\"\"\"\n",
    "    hours = dt_index.hour.values\n",
    "    dow   = dt_index.dayofweek.values\n",
    "    hour_sin = np.sin(2*np.pi*hours/24.0)\n",
    "    hour_cos = np.cos(2*np.pi*hours/24.0)\n",
    "    dow_sin  = np.sin(2*np.pi*dow/7.0)\n",
    "    dow_cos  = np.cos(2*np.pi*dow/7.0)\n",
    "    return np.stack([hour_sin, hour_cos, dow_sin, dow_cos], axis=1).astype(np.float32)\n",
    "\n",
    "dt_idx = pd.to_datetime(timestamps)\n",
    "TF = time_encoding(dt_idx)  # (T,4)\n",
    "\n",
    "print(\"TF:\", TF.shape, \"min/max:\", float(TF.min()), float(TF.max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44fda929-ebc8-4ad8-8dac-f185bde502c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF: (2208, 4) min/max: -1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "def time_encoding(dt_index: pd.DatetimeIndex) -> np.ndarray:\n",
    "    \"\"\"Return (T,4): [hour_sin, hour_cos, dow_sin, dow_cos].\"\"\"\n",
    "    hours = dt_index.hour.values\n",
    "    dow   = dt_index.dayofweek.values\n",
    "    hour_sin = np.sin(2*np.pi*hours/24.0)\n",
    "    hour_cos = np.cos(2*np.pi*hours/24.0)\n",
    "    dow_sin  = np.sin(2*np.pi*dow/7.0)\n",
    "    dow_cos  = np.cos(2*np.pi*dow/7.0)\n",
    "    return np.stack([hour_sin, hour_cos, dow_sin, dow_cos], axis=1).astype(np.float32)\n",
    "\n",
    "dt_idx = pd.to_datetime(timestamps)\n",
    "TF = time_encoding(dt_idx)  # (T,4)\n",
    "\n",
    "print(\"TF:\", TF.shape, \"min/max:\", float(TF.min()), float(TF.max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5017b1e-ab1c-47cb-bd5d-1373c6292391",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: (1821, 1821)\n",
      "Row sum check (first 5): [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "def make_random_walk_matrix(A: np.ndarray, add_self_loops: bool = True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build a row-normalized adjacency (random-walk matrix) for diffusion.\n",
    "    Returns P where each row sums to 1 (approximately).\n",
    "    \"\"\"\n",
    "    A = A.astype(np.float32)\n",
    "    # symmetrize (undirected)\n",
    "    A = np.maximum(A, A.T)\n",
    "\n",
    "    if add_self_loops:\n",
    "        A = A + np.eye(A.shape[0], dtype=np.float32)\n",
    "\n",
    "    row_sum = A.sum(axis=1)\n",
    "    row_sum = np.where(row_sum == 0, 1.0, row_sum)\n",
    "    P = (A.T / row_sum).T\n",
    "    return P.astype(np.float32)\n",
    "\n",
    "P = make_random_walk_matrix(A, add_self_loops=True)\n",
    "\n",
    "print(\"P:\", P.shape)\n",
    "print(\"Row sum check (first 5):\", P.sum(axis=1)[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3090a81-2f99-4a0d-bf6a-d690af0fdf2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def simulate_latent_process(\n",
    "    P: np.ndarray,\n",
    "    TF: np.ndarray,\n",
    "    rho: float,\n",
    "    alpha: float,\n",
    "    noise_sigma: float,\n",
    "    seed: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Simulate latent state z_t ∈ R^N with graph diffusion + seasonality:\n",
    "\n",
    "        z_{t+1} = rho * z_t + alpha * (P z_t) + s(t) + eps_t\n",
    "        z_{t+1} = tanh(z_{t+1})   # stabilizes dynamics\n",
    "\n",
    "    where s(t) is node-specific daily/weekly seasonality built from TF.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    z : (T,N) float32\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    T = TF.shape[0]\n",
    "    N = P.shape[0]\n",
    "\n",
    "    # node-specific amplitudes (keeps stations heterogeneous)\n",
    "    amp_daily  = rng.uniform(0.6, 1.6, size=N).astype(np.float32)\n",
    "    amp_weekly = rng.uniform(0.2, 0.9, size=N).astype(np.float32)\n",
    "\n",
    "    # Build seasonality from TF: hour_sin/cos + dow_sin/cos\n",
    "    hour_sin, hour_cos, dow_sin, dow_cos = TF.T  # each is (T,)\n",
    "    daily_signal  = (hour_sin + 0.5*hour_cos).astype(np.float32)\n",
    "    weekly_signal = (dow_sin  + 0.5*dow_cos ).astype(np.float32)\n",
    "\n",
    "    z = np.zeros((T, N), dtype=np.float32)\n",
    "    z[0] = rng.normal(0, 1.0, size=N).astype(np.float32)\n",
    "\n",
    "    for t in range(T - 1):\n",
    "        spatial = P @ z[t]  # (N,)\n",
    "        seasonal = 0.6 * daily_signal[t] * amp_daily + 0.4 * weekly_signal[t] * amp_weekly\n",
    "        eps = rng.normal(0, noise_sigma, size=N).astype(np.float32)\n",
    "\n",
    "        z_next = rho * z[t] + alpha * spatial + seasonal + eps\n",
    "        z[t+1] = np.tanh(z_next)  # stabilizes + adds mild nonlinearity\n",
    "\n",
    "    return z\n",
    "\n",
    "def latent_to_flow_speed(z: np.ndarray, seed: int):\n",
    "    \"\"\"\n",
    "    Map latent z_t to observable flow and speed.\n",
    "    Flow is positive; speed decreases with flow (simple traffic-like relation).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed + 10_000)\n",
    "    T, N = z.shape\n",
    "\n",
    "    # node-specific base demand + sensitivity\n",
    "    base_flow  = rng.uniform(120, 420, size=N).astype(np.float32)\n",
    "    scale_flow = rng.uniform(60, 160, size=N).astype(np.float32)\n",
    "\n",
    "    flow = base_flow[None, :] + scale_flow[None, :] * z\n",
    "    flow = np.clip(flow, 0.0, None).astype(np.float32)\n",
    "\n",
    "    # speed roughly inversely related to flow\n",
    "    vmax = 70.0\n",
    "    k = rng.uniform(0.03, 0.08, size=N).astype(np.float32)  # how strongly flow reduces speed\n",
    "    speed_noise = rng.normal(0, 2.0, size=(T, N)).astype(np.float32)\n",
    "\n",
    "    speed = vmax - flow * k[None, :] + speed_noise\n",
    "    speed = np.clip(speed, 0.0, vmax).astype(np.float32)\n",
    "\n",
    "    return flow, speed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68d6eaf1-dac2-4aef-ba03-513da3793184",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z: (2208, 1821) flow: (2208, 1821) speed: (2208, 1821)\n",
      "flow stats: 268.8896179199219 121.799072265625 min/max: 0.0 759.8511352539062\n",
      "speed stats: 55.09693908691406 8.136998176574707 min/max: 16.539337158203125 70.0\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Choose one scenario first\n",
    "# ----------------------------\n",
    "SIM_CONFIG = {\n",
    "    \"name\": \"sim_pems_like\",\n",
    "    \"seed\": 123,\n",
    "    \"rho\": 0.70,          # temporal persistence\n",
    "    \"alpha\": 0.30,        # spatial coupling strength (key experimental knob)\n",
    "    \"noise_sigma\": 0.05,  # latent noise\n",
    "}\n",
    "\n",
    "z = simulate_latent_process(\n",
    "    P=P,\n",
    "    TF=TF,\n",
    "    rho=SIM_CONFIG[\"rho\"],\n",
    "    alpha=SIM_CONFIG[\"alpha\"],\n",
    "    noise_sigma=SIM_CONFIG[\"noise_sigma\"],\n",
    "    seed=SIM_CONFIG[\"seed\"],\n",
    ")\n",
    "\n",
    "flow, speed = latent_to_flow_speed(z, seed=SIM_CONFIG[\"seed\"])\n",
    "\n",
    "print(\"z:\", z.shape, \"flow:\", flow.shape, \"speed:\", speed.shape)\n",
    "print(\"flow stats:\", float(flow.mean()), float(flow.std()), \"min/max:\", float(flow.min()), float(flow.max()))\n",
    "print(\"speed stats:\", float(speed.mean()), float(speed.std()), \"min/max:\", float(speed.min()), float(speed.max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9471525-498a-455b-aa4e-da258b33f0c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (2208, 1821, 6) Y: (2208, 1821)\n"
     ]
    }
   ],
   "source": [
    "# X: (T,N,6) with [flow, speed, hour_sin, hour_cos, dow_sin, dow_cos]\n",
    "X = np.zeros((T, N, 6), dtype=np.float32)\n",
    "X[:, :, 0] = flow\n",
    "X[:, :, 1] = speed\n",
    "\n",
    "# time features repeated across stations\n",
    "X[:, :, 2] = TF[:, 0:1]  # hour_sin\n",
    "X[:, :, 3] = TF[:, 1:1+1]  # hour_cos\n",
    "X[:, :, 4] = TF[:, 2:2+1]  # dow_sin\n",
    "X[:, :, 5] = TF[:, 3:3+1]  # dow_cos\n",
    "\n",
    "# Y target: flow (T,N)\n",
    "Y = flow.astype(np.float32)\n",
    "\n",
    "print(\"X:\", X.shape, \"Y:\", Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2993d98-9cde-41ac-93f7-77de2a928bec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique training time indices: 1104 of T= 2208\n",
      "flow_mean/std: (1821,) (1821,)\n",
      "speed_mean/std: (1821,) (1821,)\n"
     ]
    }
   ],
   "source": [
    "def compute_train_indices(train_starts, in_len, out_len):\n",
    "    \"\"\"\n",
    "    Build the set of time indices touched by training samples (inputs + outputs),\n",
    "    so scaling uses only training data.\n",
    "    \"\"\"\n",
    "    train_starts = np.asarray(train_starts, dtype=np.int64)\n",
    "\n",
    "    idx_in = train_starts[:, None] + np.arange(in_len)[None, :]\n",
    "    idx_out = train_starts[:, None] + in_len + np.arange(out_len)[None, :]\n",
    "\n",
    "    idx = np.unique(np.concatenate([idx_in.ravel(), idx_out.ravel()]))\n",
    "    idx = idx[(idx >= 0) & (idx < T)]\n",
    "    return idx\n",
    "\n",
    "train_idx = compute_train_indices(train_starts, IN_LEN, OUT_LEN)\n",
    "print(\"Unique training time indices:\", len(train_idx), \"of T=\", T)\n",
    "\n",
    "flow_train = flow[train_idx]    # (T_train,N)\n",
    "speed_train = speed[train_idx]\n",
    "\n",
    "flow_mean = flow_train.mean(axis=0).astype(np.float32)\n",
    "flow_std  = flow_train.std(axis=0).astype(np.float32)\n",
    "speed_mean = speed_train.mean(axis=0).astype(np.float32)\n",
    "speed_std  = speed_train.std(axis=0).astype(np.float32)\n",
    "\n",
    "# avoid tiny std\n",
    "flow_std = np.maximum(flow_std, 1e-3)\n",
    "speed_std = np.maximum(speed_std, 1e-3)\n",
    "\n",
    "print(\"flow_mean/std:\", flow_mean.shape, flow_std.shape)\n",
    "print(\"speed_mean/std:\", speed_mean.shape, speed_std.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7518614-c76f-4402-b99a-96f78f7de5e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /notebooks/Spatio-Temporal-Prediction-and-Coordination-of-EV-Charging-Demand-for-Power-System-Resilience/artifacts/sim_datasets/sim_pems_like_alpha0.30_seed123.npz\n",
      "Saved config: /notebooks/Spatio-Temporal-Prediction-and-Coordination-of-EV-Charging-Demand-for-Power-System-Resilience/artifacts/sim_datasets/sim_pems_like_alpha0.30_seed123.json\n"
     ]
    }
   ],
   "source": [
    "sim_name = f\"{SIM_CONFIG['name']}_alpha{SIM_CONFIG['alpha']:.2f}_seed{SIM_CONFIG['seed']}\"\n",
    "out_npz = SIM_DIR / f\"{sim_name}.npz\"\n",
    "out_json = SIM_DIR / f\"{sim_name}.json\"\n",
    "\n",
    "np.savez_compressed(\n",
    "    out_npz,\n",
    "    X=X,\n",
    "    Y=Y,\n",
    "    A=A,\n",
    "    stations=stations,\n",
    "    timestamps=timestamps,\n",
    "    train_starts=train_starts,\n",
    "    val_starts=val_starts,\n",
    "    test_starts=test_starts,\n",
    "    in_len=np.array(IN_LEN),\n",
    "    out_len=np.array(OUT_LEN),\n",
    "    flow_mean=flow_mean,\n",
    "    flow_std=flow_std,\n",
    "    speed_mean=speed_mean,\n",
    "    speed_std=speed_std,\n",
    ")\n",
    "\n",
    "with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(SIM_CONFIG, f, indent=2)\n",
    "\n",
    "print(\"Saved:\", out_npz)\n",
    "print(\"Saved config:\", out_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ab222ce-4472-4bd9-b96c-9262d324e717",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /notebooks/Spatio-Temporal-Prediction-and-Coordination-of-EV-Charging-Demand-for-Power-System-Resilience/artifacts/sim_datasets/sim_pems_like_alpha0.30_seed123.npz\n",
      "Saved config: /notebooks/Spatio-Temporal-Prediction-and-Coordination-of-EV-Charging-Demand-for-Power-System-Resilience/artifacts/sim_datasets/sim_pems_like_alpha0.30_seed123.json\n"
     ]
    }
   ],
   "source": [
    "sim_name = f\"{SIM_CONFIG['name']}_alpha{SIM_CONFIG['alpha']:.2f}_seed{SIM_CONFIG['seed']}\"\n",
    "out_npz = SIM_DIR / f\"{sim_name}.npz\"\n",
    "out_json = SIM_DIR / f\"{sim_name}.json\"\n",
    "\n",
    "np.savez_compressed(\n",
    "    out_npz,\n",
    "    X=X,\n",
    "    Y=Y,\n",
    "    A=A,\n",
    "    stations=stations,\n",
    "    timestamps=timestamps,\n",
    "    train_starts=train_starts,\n",
    "    val_starts=val_starts,\n",
    "    test_starts=test_starts,\n",
    "    in_len=np.array(IN_LEN),\n",
    "    out_len=np.array(OUT_LEN),\n",
    "    flow_mean=flow_mean,\n",
    "    flow_std=flow_std,\n",
    "    speed_mean=speed_mean,\n",
    "    speed_std=speed_std,\n",
    ")\n",
    "\n",
    "with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(SIM_CONFIG, f, indent=2)\n",
    "\n",
    "print(\"Saved:\", out_npz)\n",
    "print(\"Saved config:\", out_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e1fd0e-ade0-4c97-b00e-53fccec2f06b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

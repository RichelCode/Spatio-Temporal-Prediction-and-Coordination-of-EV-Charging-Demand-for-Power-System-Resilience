{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da6209b3",
   "metadata": {},
   "source": [
    "# Systematic Retraining Notebook (PeMS D3 2024)\n",
    "\n",
    "This notebook is a **clean, repeatable pipeline** to:\n",
    "\n",
    "1. Load the already-preprocessed graph dataset (`.npz`)\n",
    "2. Train multiple deep models with **identical splits + metrics**\n",
    "3. Run controlled hyperparameter sweeps\n",
    "4. Export a **single summary table + plots**\n",
    "\n",
    "**Goal:** re-train and *improve* deep learning models (especially **GraphWaveNet–GRU–LSTM**) in a fair, leakage-free way.\n",
    "\n",
    "---\n",
    "\n",
    "## What you must set\n",
    "- `DATASET_NPZ` path (e.g., `artifacts/pems_graph_dataset_strict.npz`)\n",
    "- `OUT_DIR` for checkpoints + logs\n",
    "\n",
    "> Tip: Start by running the *Repro* configs to match your previous results. Once reproducible, expand to the *Improve* configs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0139dc28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.1.1+cu121\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 0) Imports + device\n",
    "# ============================================================\n",
    "import os, json, math, random, time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bca46fc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1) Reproducibility utilities\n",
    "# ============================================================\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Deterministic flags (can slow down; set False if needed for speed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fedc61c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_NPZ: artifacts/pems_graph_dataset_strict.npz\n",
      "OUT_DIR: /notebooks/Spatio-Temporal-Prediction-and-Coordination-of-EV-Charging-Demand-for-Power-System-Resilience/retrain_runs_v1\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 2) Paths + global settings (EDIT THIS)\n",
    "# ============================================================\n",
    "DATASET_NPZ = Path(\"artifacts/pems_graph_dataset_strict.npz\")\n",
    "\n",
    "OUT_DIR = Path(\"retrain_runs_v1\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EVAL_HORIZONS = [12, 24, 48, 72]  # hours\n",
    "\n",
    "DEFAULTS = dict(\n",
    "    seed=42,\n",
    "    batch_size=16,\n",
    "    num_workers=0,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    max_epochs=50,\n",
    "    patience=8,\n",
    "    grad_clip=1.0,\n",
    "    loss=\"huber\",       # \"huber\" or \"mse\"\n",
    "    huber_delta=1.0,\n",
    "    use_amp=True,\n",
    ")\n",
    "\n",
    "print(\"DATASET_NPZ:\", DATASET_NPZ)\n",
    "print(\"OUT_DIR:\", OUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fed95890",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (2208, 1821, 6) Y: (2208, 1821)\n",
      "A: (1821, 1821) stations: (1821,)\n",
      "IN_LEN: 24 OUT_LEN: 72\n",
      "starts: 1009 289 673\n",
      "X_fnt: (6, 1821, 2208) Y_nt: (1821, 2208) T_total: 2208\n",
      "Ready: loaders built.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3) Load dataset (.npz) and build scaled arrays (matches your prior notebooks)\n",
    "# ============================================================\n",
    "assert DATASET_NPZ.exists(), f\"Dataset file not found: {DATASET_NPZ}\"\n",
    "\n",
    "data = np.load(DATASET_NPZ, allow_pickle=True)\n",
    "\n",
    "X = data[\"X\"].astype(np.float32)          # (T, N, F)\n",
    "Y = data[\"Y\"].astype(np.float32)          # (T, N) or (T, N, 1)\n",
    "A = data[\"A\"].astype(np.float32)          # (N, N)\n",
    "stations = data[\"stations\"]\n",
    "timestamps = pd.to_datetime(data[\"timestamps\"])\n",
    "\n",
    "train_starts = data[\"train_starts\"].astype(np.int64)\n",
    "val_starts   = data[\"val_starts\"].astype(np.int64)\n",
    "test_starts  = data[\"test_starts\"].astype(np.int64)\n",
    "\n",
    "IN_LEN = int(data[\"in_len\"][0])\n",
    "OUT_LEN = int(data[\"out_len\"][0])\n",
    "\n",
    "flow_mean = data[\"flow_mean\"].astype(np.float32)    # (N,)\n",
    "flow_std  = data[\"flow_std\"].astype(np.float32)     # (N,)\n",
    "speed_mean = data[\"speed_mean\"].astype(np.float32)  # (N,)\n",
    "speed_std  = data[\"speed_std\"].astype(np.float32)   # (N,)\n",
    "\n",
    "print(\"X:\", X.shape, \"Y:\", Y.shape)\n",
    "print(\"A:\", A.shape, \"stations:\", stations.shape)\n",
    "print(\"IN_LEN:\", IN_LEN, \"OUT_LEN:\", OUT_LEN)\n",
    "print(\"starts:\", len(train_starts), len(val_starts), len(test_starts))\n",
    "\n",
    "# Convert Y to (T,N)\n",
    "if Y.ndim == 3 and Y.shape[-1] == 1:\n",
    "    Y = Y[..., 0]\n",
    "assert Y.ndim == 2, f\"Unexpected Y shape: {Y.shape}\"\n",
    "\n",
    "# Scale targets (this is what your earlier notebook assumes when it 'unscales' for metrics)\n",
    "Y_scaled = (Y - flow_mean[None, :]) / (flow_std[None, :] + 1e-6)\n",
    "\n",
    "# Scale input features per-node: channel 0 = flow, channel 1 = speed\n",
    "X_scaled = X.copy()\n",
    "X_scaled[..., 0] = (X_scaled[..., 0] - flow_mean[None, :]) / (flow_std[None, :] + 1e-6)\n",
    "X_scaled[..., 1] = (X_scaled[..., 1] - speed_mean[None, :]) / (speed_std[None, :] + 1e-6)\n",
    "\n",
    "# Transpose for fast slicing: (T,N,F) -> (F,N,T)\n",
    "X_fnt = np.transpose(X_scaled, (2, 1, 0)).copy()  # (F, N, T)\n",
    "Y_nt  = np.transpose(Y_scaled, (1, 0)).copy()     # (N, T)\n",
    "\n",
    "T_total = X.shape[0]\n",
    "print(\"X_fnt:\", X_fnt.shape, \"Y_nt:\", Y_nt.shape, \"T_total:\", T_total)\n",
    "\n",
    "# Precompute time features for every timestamp\n",
    "hour = timestamps.hour.values\n",
    "dow  = timestamps.dayofweek.values\n",
    "tf_all = np.stack([\n",
    "    np.sin(2*np.pi*hour/24.0),\n",
    "    np.cos(2*np.pi*hour/24.0),\n",
    "    np.sin(2*np.pi*dow/7.0),\n",
    "    np.cos(2*np.pi*dow/7.0),\n",
    "], axis=1).astype(np.float32)  # (T,4)\n",
    "\n",
    "# Horizon indices (0-based inside the OUT_LEN window)\n",
    "HORIZON_IDXS = [h - 1 for h in EVAL_HORIZONS]\n",
    "assert max(HORIZON_IDXS) < OUT_LEN, f\"OUT_LEN={OUT_LEN} is too small for horizons {EVAL_HORIZONS}\"\n",
    "\n",
    "class PemsWindowDataset(Dataset):\n",
    "    def __init__(self, starts: np.ndarray):\n",
    "        self.starts = starts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.starts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t0 = int(self.starts[idx])\n",
    "\n",
    "        # x: (F,N,IN)\n",
    "        x = X_fnt[:, :, t0 : t0 + IN_LEN]\n",
    "\n",
    "        # y: (OUT, N) from Y_nt (N,T)\n",
    "        y = Y_nt[:, t0 + IN_LEN : t0 + IN_LEN + OUT_LEN].T\n",
    "\n",
    "        # tf: (OUT, 4)\n",
    "        tf = tf_all[t0 + IN_LEN : t0 + IN_LEN + OUT_LEN]\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(x),             # (F,N,IN)\n",
    "            torch.from_numpy(tf),            # (OUT,4)\n",
    "            torch.from_numpy(y),             # (OUT,N)\n",
    "        )\n",
    "\n",
    "def make_loader(starts, batch_size, shuffle):\n",
    "    ds = PemsWindowDataset(starts)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle,\n",
    "                      num_workers=DEFAULTS[\"num_workers\"], pin_memory=True)\n",
    "\n",
    "train_loader = make_loader(train_starts, DEFAULTS[\"batch_size\"], shuffle=True)\n",
    "val_loader   = make_loader(val_starts,   DEFAULTS[\"batch_size\"], shuffle=False)\n",
    "test_loader  = make_loader(test_starts,  DEFAULTS[\"batch_size\"], shuffle=False)\n",
    "\n",
    "print(\"Ready: loaders built.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "522de813",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4) Metrics (reported in ORIGINAL units: vehicles/hour)\n",
    "# ============================================================\n",
    "FLOW_MEAN_T = torch.from_numpy(flow_mean).float().view(1, 1, -1).to(device)  # (1,1,N)\n",
    "FLOW_STD_T  = torch.from_numpy(flow_std).float().view(1, 1, -1).to(device)   # (1,1,N)\n",
    "\n",
    "def unscale_flow(y_scaled: torch.Tensor) -> torch.Tensor:\n",
    "    # y_scaled: (B, OUT, N)\n",
    "    return y_scaled * (FLOW_STD_T + 1e-6) + FLOW_MEAN_T\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_metrics(y_true_scaled: torch.Tensor, y_pred_scaled: torch.Tensor, horizon_idxs):\n",
    "    '''\n",
    "    Inputs are SCALED (normalized) flow.\n",
    "    Output metrics are in ORIGINAL units (vehicles/hour).\n",
    "    '''\n",
    "    y_true = unscale_flow(y_true_scaled)\n",
    "    y_pred = unscale_flow(y_pred_scaled)\n",
    "\n",
    "    out = {}\n",
    "    for h, hi in zip(EVAL_HORIZONS, horizon_idxs):\n",
    "        err = y_pred[:, hi, :] - y_true[:, hi, :]\n",
    "        mae = err.abs().mean().item()\n",
    "        rmse = (err.pow(2).mean().sqrt()).item()\n",
    "        out[f\"MAE@{h}\"] = mae\n",
    "        out[f\"RMSE@{h}\"] = rmse\n",
    "    out[\"MAE_avg\"] = float(np.mean([out[f\"MAE@{h}\"] for h in EVAL_HORIZONS]))\n",
    "    out[\"RMSE_avg\"] = float(np.mean([out[f\"RMSE@{h}\"] for h in EVAL_HORIZONS]))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6e165f",
   "metadata": {},
   "source": [
    "## 5) Model zoo\n",
    "\n",
    "All models follow:\n",
    "\n",
    "- `forward(x, tf) -> y_hat_scaled`  \n",
    "  where `x` is `(B, F, N, IN_LEN)`, `tf` is `(B, OUT_LEN, 4)`, and `y_hat_scaled` is `(B, OUT_LEN, N)`.\n",
    "\n",
    "Targets are **scaled** in training; we unscale only for reporting MAE/RMSE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d418a089",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5A) LSTM baseline (time-aware head)\n",
    "# ============================================================\n",
    "class LSTM_Baseline(nn.Module):\n",
    "    def __init__(self, num_nodes: int, in_dim: int, in_len: int, out_len: int,\n",
    "                 hidden_size: int = 128, num_layers: int = 1, dropout: float = 0.2, time_dim: int = 32):\n",
    "        super().__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.in_dim = in_dim\n",
    "        self.in_len = in_len\n",
    "        self.out_len = out_len\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=in_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(4, time_dim), nn.ReLU(), nn.Linear(time_dim, hidden_size)\n",
    "        )\n",
    "        self.out = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, tf):\n",
    "        # x: (B,F,N,IN)\n",
    "        B, F, N, T = x.shape\n",
    "        x_seq = x.permute(0, 2, 3, 1).reshape(B * N, T, F)  # (B*N, IN, F)\n",
    "\n",
    "        h_seq, _ = self.lstm(x_seq)         # (B*N, IN, H)\n",
    "        h_last = h_seq[:, -1, :]            # (B*N, H)\n",
    "\n",
    "        tf_flat = tf.reshape(B * self.out_len, 4)                # (B*OUT, 4)\n",
    "        te = self.time_mlp(tf_flat).view(B, self.out_len, -1)    # (B, OUT, H)\n",
    "\n",
    "        h_last = h_last.view(B, N, -1)                           # (B, N, H)\n",
    "        h_rep = h_last.unsqueeze(1).expand(-1, self.out_len, -1, -1)  # (B, OUT, N, H)\n",
    "\n",
    "        z = h_rep + te.unsqueeze(2)                              # (B, OUT, N, H)\n",
    "        y_hat = self.out(z).squeeze(-1)                          # (B, OUT, N)  (scaled)\n",
    "        return y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11fec566",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5B) GraphWaveNet encoder + optional RNN refinement\n",
    "#     (supports sparse + optional adaptive dense adjacency)\n",
    "# ============================================================\n",
    "class NConvMixed(nn.Module):\n",
    "    # Graph multiplication supporting sparse OR dense adjacency.\n",
    "    def forward(self, x, A):\n",
    "        # x: (B,C,N,T), A: sparse (N,N) or dense (N,N)\n",
    "        if A.is_sparse:\n",
    "            B, C, N, T = x.shape\n",
    "            x_r = x.permute(2, 0, 1, 3).reshape(N, -1)      # (N, B*C*T)\n",
    "            x_r = torch.sparse.mm(A, x_r)                   # (N, B*C*T)\n",
    "            return x_r.reshape(N, B, C, T).permute(1, 2, 0, 3)\n",
    "        else:\n",
    "            return torch.einsum(\"bcjt,ij->bcit\", x, A)\n",
    "\n",
    "class DiffusionGraphConvMixed(nn.Module):\n",
    "    def __init__(self, c_in, c_out, supports, order=1, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.nconv = NConvMixed()\n",
    "        self.supports = supports\n",
    "        self.order = order\n",
    "        self.dropout = dropout\n",
    "        c_total = c_in * (1 + len(supports) * order)\n",
    "        self.mlp = nn.Conv2d(c_total, c_out, kernel_size=(1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = [x]\n",
    "        for A in self.supports:\n",
    "            x1 = self.nconv(x, A)\n",
    "            out.append(x1)\n",
    "            for _ in range(2, self.order + 1):\n",
    "                x1 = self.nconv(x1, A)\n",
    "                out.append(x1)\n",
    "        h = torch.cat(out, dim=1)\n",
    "        h = self.mlp(h)\n",
    "        return F.dropout(h, p=self.dropout, training=self.training)\n",
    "\n",
    "class GWNLayer(nn.Module):\n",
    "    def __init__(self, residual_channels, dilation_channels, skip_channels,\n",
    "                 kernel_size, dilation, supports, dropout=0.0, order=1):\n",
    "        super().__init__()\n",
    "        self.filter_conv = nn.Conv2d(residual_channels, dilation_channels, (1, kernel_size),\n",
    "                                     dilation=(1, dilation))\n",
    "        self.gate_conv   = nn.Conv2d(residual_channels, dilation_channels, (1, kernel_size),\n",
    "                                     dilation=(1, dilation))\n",
    "        self.dropout = dropout\n",
    "        self.gconv = DiffusionGraphConvMixed(dilation_channels, residual_channels, supports, order=order, dropout=dropout)\n",
    "        self.res_conv  = nn.Conv2d(residual_channels, residual_channels, kernel_size=(1,1))\n",
    "        self.skip_conv = nn.Conv2d(residual_channels, skip_channels,   kernel_size=(1,1))\n",
    "        self.bn = nn.BatchNorm2d(residual_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        filt = torch.tanh(self.filter_conv(x))\n",
    "        gate = torch.sigmoid(self.gate_conv(x))\n",
    "        x_t  = filt * gate\n",
    "\n",
    "        skip = self.skip_conv(x_t)\n",
    "        x_g  = self.gconv(x_t)\n",
    "        x_r  = self.res_conv(x_g)\n",
    "\n",
    "        x = x_r + x[..., -x_r.shape[-1]:]\n",
    "        x = self.bn(x)\n",
    "        return x, skip\n",
    "\n",
    "class GraphWaveNetEncoder(nn.Module):\n",
    "    def __init__(self, num_nodes, in_dim, residual_channels=32, dilation_channels=32,\n",
    "                 skip_channels=128, end_channels=256, kernel_size=2,\n",
    "                 blocks=2, layers=4, dropout=0.1, order=1,\n",
    "                 supports=None, adaptive_adj=False, adp_dim=10):\n",
    "        super().__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.in_dim = in_dim\n",
    "        self.residual_channels = residual_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.blocks = blocks\n",
    "        self.layers = layers\n",
    "        self.order = order\n",
    "\n",
    "        self.supports_static = supports or []\n",
    "        self.adaptive_adj = adaptive_adj\n",
    "        if adaptive_adj:\n",
    "            self.nodevec1 = nn.Parameter(torch.randn(num_nodes, adp_dim))\n",
    "            self.nodevec2 = nn.Parameter(torch.randn(adp_dim, num_nodes))\n",
    "\n",
    "        self.start_conv = nn.Conv2d(in_dim, residual_channels, kernel_size=(1,1))\n",
    "        self.gwn_layers = nn.ModuleList()\n",
    "\n",
    "        receptive_field = 1\n",
    "        for _ in range(blocks):\n",
    "            dilation = 1\n",
    "            for _ in range(layers):\n",
    "                self.gwn_layers.append(\n",
    "                    GWNLayer(residual_channels, dilation_channels, skip_channels,\n",
    "                             kernel_size, dilation, supports=[], dropout=dropout, order=order)\n",
    "                )\n",
    "                receptive_field += (kernel_size - 1) * dilation\n",
    "                dilation *= 2\n",
    "        self.receptive_field = receptive_field\n",
    "\n",
    "        self.end_conv_1 = nn.Conv2d(skip_channels, end_channels, kernel_size=(1,1))\n",
    "        self.end_conv_2 = nn.Conv2d(end_channels, residual_channels, kernel_size=(1,1))\n",
    "\n",
    "    def _build_supports(self):\n",
    "        sups = list(self.supports_static)\n",
    "        if self.adaptive_adj:\n",
    "            adp = F.softmax(F.relu(self.nodevec1 @ self.nodevec2), dim=1)  # (N,N)\n",
    "            sups.append(adp)\n",
    "        return sups\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, F, N, T = x.shape\n",
    "        if T < self.receptive_field:\n",
    "            x = F.pad(x, (self.receptive_field - T, 0, 0, 0))\n",
    "\n",
    "        x = self.start_conv(x)\n",
    "\n",
    "        supports = self._build_supports()\n",
    "        skip_sum = 0.0\n",
    "\n",
    "        layer_idx = 0\n",
    "        for _ in range(self.blocks):\n",
    "            dilation = 1\n",
    "            for _ in range(self.layers):\n",
    "                self.gwn_layers[layer_idx].gconv.supports = supports\n",
    "                pad = (self.kernel_size - 1) * dilation\n",
    "                x_in = F.pad(x, (pad, 0, 0, 0))\n",
    "                x, skip = self.gwn_layers[layer_idx](x_in)\n",
    "                skip_sum = skip_sum + skip\n",
    "                layer_idx += 1\n",
    "                dilation *= 2\n",
    "\n",
    "        x = F.relu(skip_sum)\n",
    "        x = F.relu(self.end_conv_1(x))\n",
    "        x = self.end_conv_2(x)\n",
    "        return x\n",
    "\n",
    "class GraphWaveNetRNN(nn.Module):\n",
    "    # GraphWaveNet encoder -> optional GRU -> optional LSTM -> time-aware horizon head.\n",
    "    def __init__(self, num_nodes, in_dim, in_len, out_len, supports,\n",
    "                 residual_channels=32, dilation_channels=32, skip_channels=128, end_channels=256,\n",
    "                 kernel_size=2, blocks=2, layers=4, dropout=0.1, order=1,\n",
    "                 adaptive_adj=False, adp_dim=10,\n",
    "                 rnn_mode=\"gru_lstm\", rnn_hidden=128, time_dim=32):\n",
    "        super().__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.in_len = in_len\n",
    "        self.out_len = out_len\n",
    "        self.rnn_mode = rnn_mode\n",
    "\n",
    "        self.encoder = GraphWaveNetEncoder(\n",
    "            num_nodes=num_nodes, in_dim=in_dim,\n",
    "            residual_channels=residual_channels, dilation_channels=dilation_channels,\n",
    "            skip_channels=skip_channels, end_channels=end_channels,\n",
    "            kernel_size=kernel_size, blocks=blocks, layers=layers,\n",
    "            dropout=dropout, order=order, supports=supports,\n",
    "            adaptive_adj=adaptive_adj, adp_dim=adp_dim\n",
    "        )\n",
    "\n",
    "        enc_dim = residual_channels\n",
    "        if rnn_mode in (\"gru\", \"gru_lstm\"):\n",
    "            self.gru = nn.GRU(enc_dim, rnn_hidden, batch_first=True)\n",
    "            enc_dim = rnn_hidden\n",
    "        else:\n",
    "            self.gru = None\n",
    "\n",
    "        if rnn_mode in (\"lstm\", \"gru_lstm\"):\n",
    "            self.lstm = nn.LSTM(enc_dim, rnn_hidden, batch_first=True)\n",
    "            enc_dim = rnn_hidden\n",
    "        else:\n",
    "            self.lstm = None\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(4, time_dim), nn.ReLU(), nn.Linear(time_dim, enc_dim)\n",
    "        )\n",
    "        self.out = nn.Linear(enc_dim, 1)\n",
    "\n",
    "    def forward(self, x, tf):\n",
    "        E = self.encoder(x)                              # (B, C, N, Tenc)\n",
    "        B, C, N, T = E.shape\n",
    "        E_seq = E.permute(0, 2, 3, 1).contiguous()        # (B, N, T, C)\n",
    "        if T > self.in_len:\n",
    "            E_seq = E_seq[:, :, -self.in_len:, :]\n",
    "\n",
    "        seq = E_seq.reshape(B * N, E_seq.shape[2], E_seq.shape[3])\n",
    "\n",
    "        if self.gru is not None:\n",
    "            seq, _ = self.gru(seq)\n",
    "        if self.lstm is not None:\n",
    "            seq, _ = self.lstm(seq)\n",
    "\n",
    "        h_last = seq[:, -1, :].view(B, N, -1)\n",
    "\n",
    "        tf_flat = tf.reshape(B * self.out_len, 4)\n",
    "        te = self.time_mlp(tf_flat).view(B, self.out_len, -1)\n",
    "\n",
    "        h_rep = h_last.unsqueeze(1).expand(-1, self.out_len, -1, -1)\n",
    "        z = h_rep + te.unsqueeze(2)\n",
    "        y_hat = self.out(z).squeeze(-1)                  # scaled\n",
    "        return y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b4d441d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5C) STGCN baseline (Chebyshev graph conv + temporal GLU)\n",
    "# ============================================================\n",
    "def nconv_sparse(x: torch.Tensor, A_sp: torch.Tensor) -> torch.Tensor:\n",
    "    B, C, N, T = x.shape\n",
    "    x_r = x.permute(2, 0, 1, 3).reshape(N, -1)\n",
    "    x_r = torch.sparse.mm(A_sp, x_r)\n",
    "    return x_r.reshape(N, B, C, T).permute(1, 2, 0, 3)\n",
    "\n",
    "class TemporalConvGLU(nn.Module):\n",
    "    def __init__(self, c_in: int, c_out: int, kt: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.kt = kt\n",
    "        self.conv = nn.Conv2d(c_in, 2*c_out, kernel_size=(1, kt))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.pad(x, (self.kt - 1, 0, 0, 0))\n",
    "        z = self.conv(x)\n",
    "        a, b = z.chunk(2, dim=1)\n",
    "        return self.dropout(a * torch.sigmoid(b))\n",
    "\n",
    "class ChebGraphConv(nn.Module):\n",
    "    def __init__(self, c_in: int, c_out: int, Ks: int, L_sp: torch.Tensor):\n",
    "        super().__init__()\n",
    "        assert Ks >= 1\n",
    "        self.Ks = Ks\n",
    "        self.L_sp = L_sp\n",
    "        self.theta = nn.Conv2d(Ks * c_in, c_out, kernel_size=(1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = [x]\n",
    "        if self.Ks > 1:\n",
    "            x1 = nconv_sparse(x, self.L_sp)\n",
    "            out.append(x1)\n",
    "            for _ in range(2, self.Ks):\n",
    "                x2 = 2 * nconv_sparse(out[-1], self.L_sp) - out[-2]\n",
    "                out.append(x2)\n",
    "        return self.theta(torch.cat(out, dim=1))\n",
    "\n",
    "class STConvBlock(nn.Module):\n",
    "    def __init__(self, c_in, c_t, c_s, c_out, kt, Ks, L_sp, dropout):\n",
    "        super().__init__()\n",
    "        self.temp1 = TemporalConvGLU(c_in,  c_t,  kt=kt, dropout=dropout)\n",
    "        self.gconv = ChebGraphConv(c_t, c_s, Ks=Ks, L_sp=L_sp)\n",
    "        self.temp2 = TemporalConvGLU(c_s,  c_out, kt=kt, dropout=dropout)\n",
    "        self.res = nn.Conv2d(c_in, c_out, kernel_size=(1,1)) if c_in != c_out else None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_in = x\n",
    "        x = self.temp1(x)\n",
    "        x = F.relu(self.gconv(x))\n",
    "        x = self.temp2(x)\n",
    "        if self.res is not None:\n",
    "            x_in = self.res(x_in)\n",
    "        return self.dropout(F.relu(x + x_in))\n",
    "\n",
    "def scaled_laplacian_sparse(A: np.ndarray) -> torch.Tensor:\n",
    "    N = A.shape[0]\n",
    "    A = A.astype(np.float32)\n",
    "    D = np.diag(A.sum(axis=1) + 1e-6)\n",
    "    L = D - A\n",
    "    v = np.random.randn(N).astype(np.float32)\n",
    "    for _ in range(50):\n",
    "        v = L @ v\n",
    "        v = v / (np.linalg.norm(v) + 1e-6)\n",
    "    lam = float(v @ (L @ v))\n",
    "    L_tilde = (2.0 / (lam + 1e-6)) * L - np.eye(N, dtype=np.float32)\n",
    "    return torch.from_numpy(L_tilde).to_sparse()\n",
    "\n",
    "class STGCN_MultiHorizon(nn.Module):\n",
    "    def __init__(self, num_nodes, in_dim, out_len, L_sp,\n",
    "                 kt=3, Ks=3, dropout=0.1, c_t=64, c_s=16, c_out=64, blocks=2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        c_in = in_dim\n",
    "        for _ in range(blocks):\n",
    "            layers.append(STConvBlock(c_in=c_in, c_t=c_t, c_s=c_s, c_out=c_out,\n",
    "                                      kt=kt, Ks=Ks, L_sp=L_sp, dropout=dropout))\n",
    "            c_in = c_out\n",
    "        self.blocks = nn.ModuleList(layers)\n",
    "        self.head = nn.Conv1d(c_out, out_len, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, tf=None):\n",
    "        h = x\n",
    "        for blk in self.blocks:\n",
    "            h = blk(h)\n",
    "        h_last = h[:, :, :, -1]\n",
    "        return self.head(h_last)  # (B, OUT, N) scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414f1f9a",
   "metadata": {},
   "source": [
    "## 6) Training / validation / test runner (early stopping + checkpointing)\n",
    "\n",
    "- Loss is computed in **scaled space** (stable training).\n",
    "- Metrics are reported in **vehicles/hour** (unscaled).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4fdc1e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6) Training utilities\n",
    "# ============================================================\n",
    "def make_loss(loss_name: str, huber_delta: float = 1.0):\n",
    "    if loss_name.lower() == \"mse\":\n",
    "        return nn.MSELoss()\n",
    "    if loss_name.lower() == \"huber\":\n",
    "        return nn.HuberLoss(delta=huber_delta)\n",
    "    raise ValueError(f\"Unknown loss: {loss_name}\")\n",
    "\n",
    "def save_json(obj, path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, loss_fn, amp=False):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    all_metrics = []\n",
    "    for x, tf, y in loader:\n",
    "        x = x.to(device, non_blocking=True).float()\n",
    "        tf = tf.to(device, non_blocking=True).float()\n",
    "        y = y.to(device, non_blocking=True).float()\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=(amp and device.type == \"cuda\")):\n",
    "            y_hat = model(x, tf)              # (B, OUT, N) scaled\n",
    "            loss = loss_fn(y_hat, y)\n",
    "        losses.append(loss.item())\n",
    "        all_metrics.append(compute_metrics(y, y_hat, HORIZON_IDXS))\n",
    "\n",
    "    out = {k: float(np.mean([m[k] for m in all_metrics])) for k in all_metrics[0].keys()}\n",
    "    out[\"loss\"] = float(np.mean(losses))\n",
    "    return out\n",
    "\n",
    "def train_one_experiment(name: str, model: nn.Module, cfg: dict):\n",
    "    run_dir = OUT_DIR / name\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    seed_everything(cfg[\"seed\"])\n",
    "    model = model.to(device)\n",
    "\n",
    "    loss_fn = make_loss(cfg[\"loss\"], cfg.get(\"huber_delta\", 1.0))\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=cfg[\"lr\"], weight_decay=cfg[\"weight_decay\"])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, mode=\"min\", factor=0.5, patience=3)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(cfg[\"use_amp\"] and device.type == \"cuda\"))\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    best_path = run_dir / \"best.pt\"\n",
    "    history = []\n",
    "    patience_left = cfg[\"patience\"]\n",
    "\n",
    "    for epoch in range(1, cfg[\"max_epochs\"] + 1):\n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "\n",
    "        for x, tf, y in train_loader:\n",
    "            x = x.to(device, non_blocking=True).float()\n",
    "            tf = tf.to(device, non_blocking=True).float()\n",
    "            y = y.to(device, non_blocking=True).float()\n",
    "\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=(cfg[\"use_amp\"] and device.type == \"cuda\")):\n",
    "                y_hat = model(x, tf)\n",
    "                loss = loss_fn(y_hat, y)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            if cfg[\"grad_clip\"] is not None:\n",
    "                scaler.unscale_(optim)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg[\"grad_clip\"])\n",
    "            scaler.step(optim)\n",
    "            scaler.update()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        val_out = eval_epoch(model, val_loader, loss_fn, amp=cfg[\"use_amp\"])\n",
    "        val_mae_avg = val_out[\"MAE_avg\"]\n",
    "        scheduler.step(val_mae_avg)\n",
    "\n",
    "        epoch_out = dict(\n",
    "            epoch=epoch,\n",
    "            train_loss=float(np.mean(train_losses)),\n",
    "            lr=float(optim.param_groups[0][\"lr\"]),\n",
    "            **{f\"val_{k}\": v for k, v in val_out.items()}\n",
    "        )\n",
    "        history.append(epoch_out)\n",
    "\n",
    "        dt = time.time() - t0\n",
    "        print(f\"[{name}] epoch {epoch:03d} | train_loss={epoch_out['train_loss']:.4f} | \"\n",
    "              f\"val_MAE_avg={val_mae_avg:.2f} veh/h | lr={epoch_out['lr']:.2e} | {dt:.1f}s\")\n",
    "\n",
    "        if val_mae_avg < best_val - 1e-6:\n",
    "            best_val = val_mae_avg\n",
    "            patience_left = cfg[\"patience\"]\n",
    "            torch.save({\"model\": model.state_dict(), \"cfg\": cfg}, best_path)\n",
    "        else:\n",
    "            patience_left -= 1\n",
    "            if patience_left <= 0:\n",
    "                print(f\"[{name}] Early stopping at epoch {epoch}. Best val MAE_avg={best_val:.2f}\")\n",
    "                break\n",
    "\n",
    "    ckpt = torch.load(best_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    test_out = eval_epoch(model, test_loader, loss_fn, amp=cfg[\"use_amp\"])\n",
    "\n",
    "    save_json({\"cfg\": cfg, \"history\": history, \"best_val_MAE_avg\": best_val, \"test\": test_out},\n",
    "              run_dir / \"results.json\")\n",
    "\n",
    "    return {\"name\": name, \"best_val_MAE_avg\": best_val, **{f\"test_{k}\": v for k, v in test_out.items()}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d040ba",
   "metadata": {},
   "source": [
    "## 7) Experiment definitions\n",
    "\n",
    "This is the **one place** you edit to control what gets trained.\n",
    "\n",
    "To strengthen the proposed model:\n",
    "- `adaptive_adj=True` (learned adjacency like GraphWaveNet)\n",
    "- bigger channels (residual 64, skip 256)\n",
    "- lower LR (e.g., `5e-4`) + longer training\n",
    "- if you change `IN_LEN`, rebuild the `.npz` dataset first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97de89fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiments: ['LSTM_repro', 'STGCN_repro', 'GWN_repro_noRNN', 'GWN_GRU_LSTM_adaptive', 'GWN_LSTM_adaptive']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 7) Define experiments\n",
    "# ============================================================\n",
    "N = X.shape[1]\n",
    "F_in = X.shape[2]\n",
    "\n",
    "# Static supports (forward/backward random-walk)\n",
    "D = np.diag(A.sum(axis=1) + 1e-6)\n",
    "A_rw = np.linalg.solve(D, A)\n",
    "A_rw_T = np.linalg.solve(D, A.T)\n",
    "\n",
    "A_rw_sp   = torch.from_numpy(A_rw).to_sparse().to(device)\n",
    "A_rw_T_sp = torch.from_numpy(A_rw_T).to_sparse().to(device)\n",
    "supports_static = [A_rw_sp, A_rw_T_sp]\n",
    "\n",
    "L_sp = scaled_laplacian_sparse(A).to(device)\n",
    "\n",
    "EXPERIMENTS = [\n",
    "    # -------------------------\n",
    "    # Reproduce (sanity checks)\n",
    "    # -------------------------\n",
    "    dict(\n",
    "        name=\"LSTM_repro\",\n",
    "        model=lambda: LSTM_Baseline(num_nodes=N, in_dim=2, in_len=IN_LEN, out_len=OUT_LEN,\n",
    "                                    hidden_size=128, num_layers=1, dropout=0.2, time_dim=32),\n",
    "        cfg=dict(DEFAULTS, lr=1e-3, max_epochs=30, patience=6),\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"STGCN_repro\",\n",
    "        model=lambda: STGCN_MultiHorizon(num_nodes=N, in_dim=2, out_len=OUT_LEN, L_sp=L_sp,\n",
    "                                         kt=3, Ks=3, dropout=0.1, c_t=64, c_s=16, c_out=64, blocks=2),\n",
    "        cfg=dict(DEFAULTS, lr=1e-3, max_epochs=30, patience=6),\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"GWN_repro_noRNN\",\n",
    "        model=lambda: GraphWaveNetRNN(num_nodes=N, in_dim=2, in_len=IN_LEN, out_len=OUT_LEN,\n",
    "                                      supports=supports_static,\n",
    "                                      residual_channels=32, dilation_channels=32, skip_channels=128, end_channels=256,\n",
    "                                      blocks=2, layers=4, dropout=0.1, order=1,\n",
    "                                      adaptive_adj=False, rnn_mode=\"none\", rnn_hidden=128),\n",
    "        cfg=dict(DEFAULTS, lr=1e-3, max_epochs=30, patience=6),\n",
    "    ),\n",
    "\n",
    "    # -------------------------\n",
    "    # Improve (stronger settings)\n",
    "    # -------------------------\n",
    "    dict(\n",
    "        name=\"GWN_GRU_LSTM_adaptive\",\n",
    "        model=lambda: GraphWaveNetRNN(num_nodes=N, in_dim=2, in_len=IN_LEN, out_len=OUT_LEN,\n",
    "                                      supports=supports_static,\n",
    "                                      residual_channels=64, dilation_channels=64, skip_channels=256, end_channels=512,\n",
    "                                      blocks=3, layers=4, dropout=0.2, order=1,\n",
    "                                      adaptive_adj=True, adp_dim=20,\n",
    "                                      rnn_mode=\"gru_lstm\", rnn_hidden=256),\n",
    "        cfg=dict(DEFAULTS, lr=5e-4, max_epochs=60, patience=10, grad_clip=1.0, weight_decay=1e-4),\n",
    "    ),\n",
    "    dict(\n",
    "        name=\"GWN_LSTM_adaptive\",\n",
    "        model=lambda: GraphWaveNetRNN(num_nodes=N, in_dim=2, in_len=IN_LEN, out_len=OUT_LEN,\n",
    "                                      supports=supports_static,\n",
    "                                      residual_channels=64, dilation_channels=64, skip_channels=256, end_channels=512,\n",
    "                                      blocks=3, layers=4, dropout=0.2, order=1,\n",
    "                                      adaptive_adj=True, adp_dim=20,\n",
    "                                      rnn_mode=\"lstm\", rnn_hidden=256),\n",
    "        cfg=dict(DEFAULTS, lr=5e-4, max_epochs=60, patience=10),\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\"Experiments:\", [e[\"name\"] for e in EXPERIMENTS])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91e164d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Running: LSTM_repro\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[3072, 1]' is invalid for input of size 1024",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning:\u001b[39m\u001b[38;5;124m\"\u001b[39m, name)\n\u001b[0;32m---> 11\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(out)\n\u001b[1;32m     14\u001b[0m df_results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_MAE_avg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 67\u001b[0m, in \u001b[0;36mtrain_one_experiment\u001b[0;34m(name, model, cfg)\u001b[0m\n\u001b[1;32m     64\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(enabled\u001b[38;5;241m=\u001b[39m(cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_amp\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m---> 67\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(y_hat, y)\n\u001b[1;32m     70\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 30\u001b[0m, in \u001b[0;36mLSTM_Baseline.forward\u001b[0;34m(self, x, tf)\u001b[0m\n\u001b[1;32m     27\u001b[0m B, F, N, T \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     28\u001b[0m x_seq \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B \u001b[38;5;241m*\u001b[39m N, T, F)  \u001b[38;5;66;03m# (B*N, IN, F)\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m h_seq, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_seq\u001b[49m\u001b[43m)\u001b[49m         \u001b[38;5;66;03m# (B*N, IN, H)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m h_last \u001b[38;5;241m=\u001b[39m h_seq[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]            \u001b[38;5;66;03m# (B*N, H)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m tf_flat \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(B \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_len, \u001b[38;5;241m4\u001b[39m)                \u001b[38;5;66;03m# (B*OUT, 4)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:879\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    876\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    882\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    883\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[3072, 1]' is invalid for input of size 1024"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 8) Run all experiments and create a summary table\n",
    "# ============================================================\n",
    "results = []\n",
    "for exp in EXPERIMENTS:\n",
    "    name = exp[\"name\"]\n",
    "    cfg = exp[\"cfg\"]\n",
    "    model = exp[\"model\"]()\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Running:\", name)\n",
    "    out = train_one_experiment(name, model, cfg)\n",
    "    results.append(out)\n",
    "\n",
    "df_results = pd.DataFrame(results).sort_values(\"test_MAE_avg\")\n",
    "display(df_results)\n",
    "\n",
    "df_results.to_csv(OUT_DIR / \"summary.csv\", index=False)\n",
    "print(\"Saved summary:\", (OUT_DIR / \"summary.csv\").resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1496a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9) Plot MAE/RMSE vs horizon for the best runs\n",
    "# ============================================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "topk = min(5, len(df_results))\n",
    "df_top = df_results.head(topk).copy()\n",
    "\n",
    "x = np.array(EVAL_HORIZONS)\n",
    "for metric in [\"MAE\", \"RMSE\"]:\n",
    "    plt.figure()\n",
    "    for _, row in df_top.iterrows():\n",
    "        y = [row[f\"test_{metric}@{h}\"] for h in EVAL_HORIZONS]\n",
    "        plt.plot(x, y, marker=\"o\", label=row[\"name\"])\n",
    "    plt.xlabel(\"Horizon (hours)\")\n",
    "    plt.ylabel(metric + \" (vehicles/hour)\")\n",
    "    plt.title(f\"Top-{topk} Test {metric} vs Horizon\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4d821e",
   "metadata": {},
   "source": [
    "## 10) To beat Random Forest (and keep the comparison fair)\n",
    "\n",
    "If your Random Forest uses engineered covariates (lags/rolling stats/neighbors) but deep models only see (flow,speed),\n",
    "RF can win because it has more information.\n",
    "\n",
    "To give deep models a fair shot, rebuild the `.npz` so `X[..., :]` includes the engineered features too, then rerun.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093cccb2-9fd4-4886-8cef-0f8003d91601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae80ea7a-9ce4-4133-bfa2-955d8e2dfa0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

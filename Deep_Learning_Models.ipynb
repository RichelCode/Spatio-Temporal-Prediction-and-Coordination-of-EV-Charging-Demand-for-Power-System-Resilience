{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9795e03-a5db-4abb-947f-65a818b13483",
   "metadata": {},
   "source": [
    "# Multi-Horizon Traffic Forecasting on PeMS (Graph Models)\n",
    "\n",
    "## Goal (Paper Claim)\n",
    "Build a leakage-safe, reproducible pipeline on PeMS traffic data and evaluate multi-horizon forecasting models fairly.\n",
    "\n",
    "Primary goal:\n",
    "- Demonstrate the proposed **GraphWaveNet-GRU-LSTM** performs best on PeMS under the same train/val/test protocol.\n",
    "\n",
    "Key principles:\n",
    "- No time leakage (all statistics computed from train only).\n",
    "- One shared dataset representation for all deep models: **X ∈ R^{T×N×F}, Y ∈ R^{T×N}**.\n",
    "- One fixed evaluation harness (same horizons, same metrics, same seeds).\n",
    "- Strong baselines + ablations:\n",
    "  - HA / Persistence\n",
    "  - GRU / LSTM (non-graph)\n",
    "  - GraphWaveNet\n",
    "  - GraphWaveNet+GRU\n",
    "  - GraphWaveNet+LSTM\n",
    "  - **GraphWaveNet+GRU+LSTM (proposed)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dceeef19-2130-4a23-931a-06196ed31757",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T19:36:12.615948Z",
     "iopub.status.busy": "2026-02-07T19:36:12.615649Z",
     "iopub.status.idle": "2026-02-07T19:36:20.583394Z",
     "shell.execute_reply": "2026-02-07T19:36:20.582642Z",
     "shell.execute_reply.started": "2026-02-07T19:36:12.615922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb15451a-d0ef-44ab-a5ae-c33eebe63c39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T19:36:29.430693Z",
     "iopub.status.busy": "2026-02-07T19:36:29.430355Z",
     "iopub.status.idle": "2026-02-07T19:36:33.161009Z",
     "shell.execute_reply": "2026-02-07T19:36:33.160283Z",
     "shell.execute_reply.started": "2026-02-07T19:36:29.430665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install numpy pandas openpyxl scikit-learn torch tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70d3f8bf-2c00-4b4c-acfb-5ce948648cf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T19:36:50.204022Z",
     "iopub.status.busy": "2026-02-07T19:36:50.203372Z",
     "iopub.status.idle": "2026-02-07T19:36:52.731520Z",
     "shell.execute_reply": "2026-02-07T19:36:52.730792Z",
     "shell.execute_reply.started": "2026-02-07T19:36:50.203991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.1.1+cu121\n",
      "Device: cuda\n",
      "GPU: Quadro P5000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def set_seed(seed: int = 42, deterministic: bool = True):\n",
    "    \"\"\"\n",
    "    Sets seeds for reproducibility.\n",
    "    deterministic=True makes results more reproducible but can reduce speed.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    if deterministic:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    else:\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED, deterministic=True)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Device:\", DEVICE)\n",
    "if DEVICE == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91ddbd0-0ab3-4784-8fe2-930de04d11c3",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "We fix:\n",
    "- Input window length (`IN_LEN`) and forecast horizon length (`OUT_LEN`)\n",
    "- Train/val/test boundaries (time-based split)\n",
    "- Station inclusion rule (coverage threshold)\n",
    "- Output dataset artifact path (so every model uses the same processed dataset)\n",
    "\n",
    "Important:\n",
    "GraphWaveNet expects a consistent node set and continuous time axis,\n",
    "so we build a clean matrix format (timestamp × station).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "907ef442-d82f-4601-ba92-8f0b62467aa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T20:30:36.024501Z",
     "iopub.status.busy": "2026-02-07T20:30:36.023555Z",
     "iopub.status.idle": "2026-02-07T20:30:36.031533Z",
     "shell.execute_reply": "2026-02-07T20:30:36.030825Z",
     "shell.execute_reply.started": "2026-02-07T20:30:36.024473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will save processed dataset to: artifacts/pems_graph_dataset.npz\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Paths (your files are visible in the Paperspace file pane)\n",
    "# -------------------------\n",
    "TRAFFIC_CSV = Path(\"cleaned_traffic_data.csv\")\n",
    "META_XLSX   = Path(\"pems_output.xlsx\")\n",
    "\n",
    "assert TRAFFIC_CSV.exists(), f\"Missing {TRAFFIC_CSV}\"\n",
    "assert META_XLSX.exists(), f\"Missing {META_XLSX}\"\n",
    "\n",
    "# -------------------------\n",
    "# Split boundaries (same as your earlier work)\n",
    "# -------------------------\n",
    "TRAIN_END = pd.Timestamp(\"2024-11-15 23:59:59\")\n",
    "VAL_END   = pd.Timestamp(\"2024-11-30 23:59:59\")\n",
    "\n",
    "# -------------------------\n",
    "# Forecast setup\n",
    "# -------------------------\n",
    "IN_LEN  = 24     # hours of history used as input\n",
    "OUT_LEN = 72     # predict next 72 hours (we will evaluate at 12/24/48/72)\n",
    "\n",
    "EVAL_HORIZONS = [12, 24, 48, 72]  # hours ahead\n",
    "\n",
    "# -------------------------\n",
    "# Station coverage threshold\n",
    "# -------------------------\n",
    "# 1.0 means station must have ALL timestamps present.\n",
    "# 0.98 is often a good compromise if some stations are missing few points.\n",
    "COVERAGE_THRESHOLD = 0.98\n",
    "\n",
    "# -------------------------\n",
    "# Adjacency setup (static graph baseline)\n",
    "# -------------------------\n",
    "K_NEIGHBORS = 2   # connect up to 2 upstream + 2 downstream along the freeway chain\n",
    "\n",
    "# -------------------------\n",
    "# Output artifact (important for reproducibility)\n",
    "# -------------------------\n",
    "OUT_DIR = Path(\"artifacts\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "DATASET_NPZ = OUT_DIR / \"pems_graph_dataset.npz\"\n",
    "print(\"Will save processed dataset to:\", DATASET_NPZ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714660ee-bbf4-44ce-b128-8d23ca17d379",
   "metadata": {},
   "source": [
    "## Load raw traffic + metadata\n",
    "\n",
    "We:\n",
    "1) Load cleaned traffic data\n",
    "2) Load station metadata\n",
    "3) Standardize column names\n",
    "4) Merge metadata onto traffic records (inner join so every station has metadata)\n",
    "5) Verify timestamp parsing and basic integrity checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2e82616-9813-4b63-9281-dd4766488c4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T20:32:05.462983Z",
     "iopub.status.busy": "2026-02-07T20:32:05.462422Z",
     "iopub.status.idle": "2026-02-07T20:32:05.468279Z",
     "shell.execute_reply": "2026-02-07T20:32:05.467288Z",
     "shell.execute_reply.started": "2026-02-07T20:32:05.462959Z"
    }
   },
   "outputs": [],
   "source": [
    "def require_col(df: pd.DataFrame, candidates, friendly_name: str):\n",
    "    \"\"\"\n",
    "    Find the first matching column in candidates.\n",
    "    Raise a helpful error if not found.\n",
    "    \"\"\"\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise KeyError(\n",
    "        f\"Could not find column for '{friendly_name}'. Tried: {candidates}\\n\"\n",
    "        f\"Available columns: {list(df.columns)}\"\n",
    "    )\n",
    "\n",
    "def to_datetime_safe(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_datetime(s, errors=\"coerce\")\n",
    "\n",
    "def pct_missing(s: pd.Series) -> float:\n",
    "    return float(s.isna().mean() * 100.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "891ee42c-db17-4a5a-8760-c2d5307dcc93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T20:32:15.478248Z",
     "iopub.status.busy": "2026-02-07T20:32:15.477319Z",
     "iopub.status.idle": "2026-02-07T20:32:35.416432Z",
     "shell.execute_reply": "2026-02-07T20:32:35.415675Z",
     "shell.execute_reply.started": "2026-02-07T20:32:15.478222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traffic shape: (4114680, 42)\n",
      "Meta shape: (1861, 15)\n",
      "After basic parsing: (4114680, 42)\n",
      "Timestamp range: 2024-10-01 00:00:00 → 2024-12-31 23:00:00\n",
      "Meta columns (peek): ['Fwy', 'District', 'County', 'City', 'CA PM', 'Abs PM', 'Length', 'station', 'Name', 'Lanes', 'Type', 'Sensor Type', 'HOV', 'MS ID', 'IRM']\n",
      "Merged df shape: (4051621, 56)\n",
      "Unique stations: 1861\n"
     ]
    }
   ],
   "source": [
    "traffic_raw = pd.read_csv(TRAFFIC_CSV)\n",
    "meta_raw = pd.read_excel(META_XLSX)\n",
    "\n",
    "print(\"Traffic shape:\", traffic_raw.shape)\n",
    "print(\"Meta shape:\", meta_raw.shape)\n",
    "\n",
    "# --- Identify expected columns robustly ---\n",
    "ts_col   = require_col(traffic_raw, [\"Timestamp\", \"timestamp\", \"Time\", \"Datetime\"], \"Timestamp\")\n",
    "st_col   = require_col(traffic_raw, [\"Station\", \"station\", \"ID\"], \"Station ID\")\n",
    "flow_col = require_col(traffic_raw, [\"Total Flow\", \"total_flow\", \"Flow\", \"total flow\"], \"Total Flow\")\n",
    "spd_col  = require_col(traffic_raw, [\"Avg Speed\", \"avg_speed\", \"Speed\", \"Avg speed\"], \"Avg Speed\")\n",
    "\n",
    "lane_col = require_col(traffic_raw, [\"Lane Type\", \"lane_type\", \"LaneType\"], \"Lane Type\")\n",
    "dir_col  = require_col(traffic_raw, [\"Direction of Travel\", \"direction\", \"Dir\"], \"Direction\")\n",
    "dist_col = require_col(traffic_raw, [\"District\", \"district\"], \"District\")\n",
    "\n",
    "# --- Standardize traffic ---\n",
    "traffic = traffic_raw.rename(columns={\n",
    "    ts_col: \"timestamp\",\n",
    "    st_col: \"station\",\n",
    "    flow_col: \"total_flow\",\n",
    "    spd_col: \"avg_speed\",\n",
    "    lane_col: \"lane_type\",\n",
    "    dir_col: \"direction\",\n",
    "    dist_col: \"district\",\n",
    "}).copy()\n",
    "\n",
    "traffic[\"timestamp\"] = to_datetime_safe(traffic[\"timestamp\"])\n",
    "traffic[\"station\"] = pd.to_numeric(traffic[\"station\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "traffic = traffic.dropna(subset=[\"timestamp\", \"station\"]).copy()\n",
    "traffic[\"station\"] = traffic[\"station\"].astype(int)\n",
    "\n",
    "print(\"After basic parsing:\", traffic.shape)\n",
    "print(\"Timestamp range:\", traffic[\"timestamp\"].min(), \"→\", traffic[\"timestamp\"].max())\n",
    "\n",
    "# --- Standardize metadata ---\n",
    "# station id in metadata usually is 'ID'\n",
    "meta_id_col = require_col(meta_raw, [\"ID\", \"station\", \"Station\"], \"Meta Station ID\")\n",
    "meta = meta_raw.rename(columns={meta_id_col: \"station\"}).copy()\n",
    "meta[\"station\"] = pd.to_numeric(meta[\"station\"], errors=\"coerce\").astype(\"Int64\")\n",
    "meta = meta.dropna(subset=[\"station\"]).copy()\n",
    "meta[\"station\"] = meta[\"station\"].astype(int)\n",
    "\n",
    "print(\"Meta columns (peek):\", list(meta.columns)[:20])\n",
    "\n",
    "# Merge metadata (inner ensures we only keep stations that have metadata)\n",
    "df = traffic.merge(meta, on=\"station\", how=\"inner\", validate=\"m:1\")\n",
    "print(\"Merged df shape:\", df.shape)\n",
    "print(\"Unique stations:\", df[\"station\"].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f00d72-390f-4127-8fee-892c70b33868",
   "metadata": {},
   "source": [
    "## Sanity checks\n",
    "\n",
    "We check:\n",
    "- Duplicate rows per (timestamp, station)\n",
    "- Time frequency (hourly vs not)\n",
    "- Missingness rates\n",
    "These checks prevent silent data problems that can invalidate results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ccee1e2-e6e9-4140-835d-2d16cf1beabd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T20:33:17.465616Z",
     "iopub.status.busy": "2026-02-07T20:33:17.465333Z",
     "iopub.status.idle": "2026-02-07T20:33:17.896142Z",
     "shell.execute_reply": "2026-02-07T20:33:17.895372Z",
     "shell.execute_reply.started": "2026-02-07T20:33:17.465593Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate (timestamp, station) rows: 0\n",
      "Most common timestamp deltas:\n",
      " 0 days 01:00:00    2207\n",
      "Name: count, dtype: int64\n",
      "Missing total_flow (%): 7.243150334150209\n",
      "Missing avg_speed (%): 37.88496011843161\n"
     ]
    }
   ],
   "source": [
    "# 1) Duplicates by (timestamp, station)\n",
    "dup_count = df.duplicated(subset=[\"timestamp\", \"station\"]).sum()\n",
    "print(\"Duplicate (timestamp, station) rows:\", int(dup_count))\n",
    "\n",
    "if dup_count > 0:\n",
    "    # Resolve duplicates safely: flow sums, speed averages\n",
    "    df = (df.groupby([\"timestamp\", \"station\"], as_index=False)\n",
    "            .agg({\n",
    "                \"total_flow\": \"sum\",\n",
    "                \"avg_speed\": \"mean\",\n",
    "                \"lane_type\": \"first\",\n",
    "                \"direction\": \"first\",\n",
    "                \"district\": \"first\",\n",
    "                # keep metadata columns by first\n",
    "                **{c: \"first\" for c in meta.columns if c != \"station\"}\n",
    "            }))\n",
    "    print(\"After de-duplication:\", df.shape)\n",
    "\n",
    "# 2) Check time deltas\n",
    "times = pd.DatetimeIndex(sorted(df[\"timestamp\"].unique()))\n",
    "deltas = pd.Series(times[1:] - times[:-1]).value_counts().head(5)\n",
    "print(\"Most common timestamp deltas:\\n\", deltas)\n",
    "\n",
    "# 3) Missingness\n",
    "print(\"Missing total_flow (%):\", pct_missing(df[\"total_flow\"]))\n",
    "print(\"Missing avg_speed (%):\", pct_missing(df[\"avg_speed\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78bf0b1-3cbf-4a41-85bc-b09e0ecd29d9",
   "metadata": {},
   "source": [
    "## Build station-time matrices\n",
    "\n",
    "Graph models require a clean tensor format.\n",
    "We create two matrices:\n",
    "- Flow:  (T timestamps × N stations)\n",
    "- Speed: (T timestamps × N stations)\n",
    "\n",
    "We also select a stable station set using a coverage threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c1913df-2f73-4406-99cc-2cc7019f4f19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T20:37:39.730439Z",
     "iopub.status.busy": "2026-02-07T20:37:39.730115Z",
     "iopub.status.idle": "2026-02-07T20:37:47.228785Z",
     "shell.execute_reply": "2026-02-07T20:37:47.228035Z",
     "shell.execute_reply.started": "2026-02-07T20:37:39.730415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamps (T) = 2208\n",
      "Stations kept (N) = 1821  (coverage threshold=0.98)\n",
      "Flow matrix: (2208, 1821) Speed matrix: (2208, 1821)\n",
      "Flow missing fraction: 0.0712893656137335\n",
      "Speed missing fraction: 0.3772684720928937\n"
     ]
    }
   ],
   "source": [
    "# Full timestamp index\n",
    "all_times = pd.DatetimeIndex(sorted(df[\"timestamp\"].unique()))\n",
    "T = len(all_times)\n",
    "\n",
    "# Station coverage\n",
    "counts = df.groupby(\"station\")[\"timestamp\"].nunique()\n",
    "coverage = counts / T\n",
    "\n",
    "keep_stations = coverage[coverage >= COVERAGE_THRESHOLD].index\n",
    "df2 = df[df[\"station\"].isin(keep_stations)].copy()\n",
    "\n",
    "stations = np.array(sorted(df2[\"station\"].unique()), dtype=int)\n",
    "N = len(stations)\n",
    "\n",
    "print(f\"Timestamps (T) = {T}\")\n",
    "print(f\"Stations kept (N) = {N}  (coverage threshold={COVERAGE_THRESHOLD})\")\n",
    "\n",
    "# Build matrices\n",
    "flow = (df2.pivot(index=\"timestamp\", columns=\"station\", values=\"total_flow\")\n",
    "          .reindex(index=all_times, columns=stations)\n",
    "          .sort_index())\n",
    "\n",
    "speed = (df2.pivot(index=\"timestamp\", columns=\"station\", values=\"avg_speed\")\n",
    "           .reindex(index=all_times, columns=stations)\n",
    "           .sort_index())\n",
    "\n",
    "print(\"Flow matrix:\", flow.shape, \"Speed matrix:\", speed.shape)\n",
    "print(\"Flow missing fraction:\", float(np.isnan(flow.to_numpy()).mean()))\n",
    "print(\"Speed missing fraction:\", float(np.isnan(speed.to_numpy()).mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdefb51-4046-4fe5-8ae7-83654b1afbc3",
   "metadata": {},
   "source": [
    "## Leakage-safe imputation\n",
    "\n",
    "We must not use validation/test information when estimating fill values.\n",
    "\n",
    "Strategy:\n",
    "- Forward-fill across time (realistic streaming behavior).\n",
    "- Remaining NaNs filled using TRAIN statistics only.\n",
    "\n",
    "Flow:\n",
    "- ffill → fill with per-station TRAIN mean → fill with global TRAIN mean\n",
    "\n",
    "Speed:\n",
    "- ffill → fill using a TRAIN-only group lookup (lane_type, meta type, hour, fwy, district)\n",
    "- then per-station TRAIN mean → global TRAIN mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc0cc46e-f59e-436a-9538-76628782eeb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T20:38:52.379831Z",
     "iopub.status.busy": "2026-02-07T20:38:52.379494Z",
     "iopub.status.idle": "2026-02-07T20:39:31.394012Z",
     "shell.execute_reply": "2026-02-07T20:39:31.392919Z",
     "shell.execute_reply.started": "2026-02-07T20:38:52.379793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After imputation:\n",
      "Flow missing fraction: 0.0\n",
      "Speed missing fraction: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Identify metadata columns we need for speed lookup\n",
    "meta_type_col = None\n",
    "for cand in [\"Type\", \"type\", \"Station Type\"]:\n",
    "    if cand in df2.columns:\n",
    "        meta_type_col = cand\n",
    "        break\n",
    "\n",
    "fwy_col = None\n",
    "for cand in [\"Fwy\", \"FWY\", \"fwy\", \"Freeway\"]:\n",
    "    if cand in df2.columns:\n",
    "        fwy_col = cand\n",
    "        break\n",
    "\n",
    "if meta_type_col is None or fwy_col is None:\n",
    "    raise KeyError(f\"Missing metadata columns for speed lookup. Found meta_type={meta_type_col}, fwy={fwy_col}\")\n",
    "\n",
    "train_time_mask = flow.index <= TRAIN_END\n",
    "\n",
    "# -------------------------\n",
    "# Flow imputation\n",
    "# -------------------------\n",
    "flow_ff = flow.ffill()\n",
    "\n",
    "flow_train_mean_station = flow_ff.loc[train_time_mask].mean(axis=0)\n",
    "flow_train_mean_global = flow_ff.loc[train_time_mask].stack().mean()\n",
    "\n",
    "flow_imp = flow_ff.fillna(flow_train_mean_station).fillna(flow_train_mean_global)\n",
    "\n",
    "# -------------------------\n",
    "# Speed lookup (TRAIN only)\n",
    "# -------------------------\n",
    "train_rows = df2[df2[\"timestamp\"] <= TRAIN_END].copy()\n",
    "train_rows[\"hour\"] = train_rows[\"timestamp\"].dt.hour\n",
    "\n",
    "speed_grp_cols = [\"lane_type\", meta_type_col, \"hour\", fwy_col, \"district\"]\n",
    "speed_lookup = train_rows.groupby(speed_grp_cols)[\"avg_speed\"].mean()\n",
    "\n",
    "global_speed_train_mean = train_rows[\"avg_speed\"].mean()\n",
    "\n",
    "# Station-level \"mode\" descriptors used when applying the lookup\n",
    "station_info = (df2.groupby(\"station\")\n",
    "                  .agg(\n",
    "                      lane_type=(\"lane_type\", lambda x: x.mode().iloc[0] if len(x.mode()) else x.iloc[0]),\n",
    "                      meta_type=(meta_type_col, lambda x: x.mode().iloc[0] if len(x.mode()) else x.iloc[0]),\n",
    "                      fwy=(fwy_col, lambda x: x.mode().iloc[0] if len(x.mode()) else x.iloc[0]),\n",
    "                      district=(\"district\", lambda x: x.mode().iloc[0] if len(x.mode()) else x.iloc[0]),\n",
    "                  )\n",
    "                  .reindex(stations))\n",
    "\n",
    "speed_ff = speed.ffill()\n",
    "speed_np = speed_ff.to_numpy(dtype=np.float32)\n",
    "miss = np.isnan(speed_np)\n",
    "hours = speed_ff.index.hour.values\n",
    "\n",
    "# Fill with lookup\n",
    "for j, st in enumerate(stations):\n",
    "    if not miss[:, j].any():\n",
    "        continue\n",
    "    info = station_info.loc[st]\n",
    "    lane_type = info[\"lane_type\"]\n",
    "    meta_type = info[\"meta_type\"]\n",
    "    fwy = info[\"fwy\"]\n",
    "    district = info[\"district\"]\n",
    "\n",
    "    idxs = np.where(miss[:, j])[0]\n",
    "    fill_vals = []\n",
    "    for t_idx in idxs:\n",
    "        h = int(hours[t_idx])\n",
    "        key = (lane_type, meta_type, h, fwy, district)\n",
    "        fill_vals.append(speed_lookup.get(key, np.nan))\n",
    "    speed_np[idxs, j] = np.array(fill_vals, dtype=np.float32)\n",
    "\n",
    "speed_imp = pd.DataFrame(speed_np, index=speed_ff.index, columns=speed_ff.columns)\n",
    "\n",
    "# Remaining NaNs → station TRAIN mean → global TRAIN mean\n",
    "speed_train_mean_station = speed_imp.loc[train_time_mask].mean(axis=0)\n",
    "speed_imp = speed_imp.fillna(speed_train_mean_station).fillna(global_speed_train_mean)\n",
    "\n",
    "print(\"After imputation:\")\n",
    "print(\"Flow missing fraction:\", float(np.isnan(flow_imp.to_numpy()).mean()))\n",
    "print(\"Speed missing fraction:\", float(np.isnan(speed_imp.to_numpy()).mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4ef022-a962-4b61-8216-b9f447935673",
   "metadata": {},
   "source": [
    "## Build graph-ready tensors\n",
    "\n",
    "We create:\n",
    "- X: (T, N, F)\n",
    "  Features include:\n",
    "  - flow (1)\n",
    "  - speed (1)\n",
    "  - time encodings: hour_sin, hour_cos, dow_sin, dow_cos (4)\n",
    "  Total F = 6\n",
    "\n",
    "- Y: (T, N)\n",
    "  Target is flow at each station.\n",
    "\n",
    "Later, each training sample is a sliding window:\n",
    "- Input:  X[t : t+IN_LEN]\n",
    "- Output: Y[t+IN_LEN : t+IN_LEN+OUT_LEN]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "979fd9ed-9d39-4ad5-8941-d65abd464ec5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T20:39:31.395785Z",
     "iopub.status.busy": "2026-02-07T20:39:31.395279Z",
     "iopub.status.idle": "2026-02-07T20:39:31.523145Z",
     "shell.execute_reply": "2026-02-07T20:39:31.522242Z",
     "shell.execute_reply.started": "2026-02-07T20:39:31.395760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (2208, 1821, 6)  (T,N,F)\n",
      "Y shape: (2208, 1821)  (T,N)\n"
     ]
    }
   ],
   "source": [
    "def make_time_features(timestamps: pd.DatetimeIndex) -> np.ndarray:\n",
    "    hours = timestamps.hour.values\n",
    "    dow   = timestamps.dayofweek.values\n",
    "    hour_sin = np.sin(2*np.pi*hours/24.0)\n",
    "    hour_cos = np.cos(2*np.pi*hours/24.0)\n",
    "    dow_sin  = np.sin(2*np.pi*dow/7.0)\n",
    "    dow_cos  = np.cos(2*np.pi*dow/7.0)\n",
    "    return np.stack([hour_sin, hour_cos, dow_sin, dow_cos], axis=1).astype(np.float32)  # (T,4)\n",
    "\n",
    "time_feats = make_time_features(flow_imp.index)  # (T,4)\n",
    "time_feats_b = np.repeat(time_feats[:, None, :], repeats=N, axis=1)  # (T,N,4)\n",
    "\n",
    "flow_arr  = flow_imp.to_numpy(dtype=np.float32)[:, :, None]   # (T,N,1)\n",
    "speed_arr = speed_imp.to_numpy(dtype=np.float32)[:, :, None]  # (T,N,1)\n",
    "\n",
    "X = np.concatenate([flow_arr, speed_arr, time_feats_b], axis=2)  # (T,N,6)\n",
    "Y = flow_arr.squeeze(-1).astype(np.float32)                      # (T,N)\n",
    "\n",
    "print(\"X shape:\", X.shape, \" (T,N,F)\")\n",
    "print(\"Y shape:\", Y.shape, \" (T,N)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316168d6-874e-4fb3-a6f0-9de8d05c5830",
   "metadata": {},
   "source": [
    "## Build adjacency matrix A (static graph baseline)\n",
    "\n",
    "We build a physical-neighborhood adjacency using metadata:\n",
    "- Sort stations by (freeway, absolute postmile)\n",
    "- Connect K neighbors upstream + downstream\n",
    "- Weight edges using a Gaussian kernel of distance\n",
    "- Add self-loops\n",
    "\n",
    "Note:\n",
    "GraphWaveNet can also learn an adaptive adjacency; this static graph is a strong baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "def5e213-a0b5-43ea-b332-25a80dfaa72b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T20:40:39.110976Z",
     "iopub.status.busy": "2026-02-07T20:40:39.110692Z",
     "iopub.status.idle": "2026-02-07T20:40:39.239413Z",
     "shell.execute_reply": "2026-02-07T20:40:39.238547Z",
     "shell.execute_reply.started": "2026-02-07T20:40:39.110955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A shape: (1821, 1821)\n",
      "Adjacency density (A>0): 0.0023693916932872663\n"
     ]
    }
   ],
   "source": [
    "def build_adjacency_from_metadata(meta_df: pd.DataFrame, stations: np.ndarray, k_neighbors: int = 2) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build adjacency within each freeway chain using Abs PM order.\n",
    "    Edge weights = exp(-(dist^2 / sigma^2)), sigma = median neighbor distance.\n",
    "    \"\"\"\n",
    "    # Find needed meta columns\n",
    "    id_col = \"station\"\n",
    "    abs_pm_col = None\n",
    "    for cand in [\"Abs PM\", \"abs_pm\", \"AbsPM\", \"Postmile\", \"PM\"]:\n",
    "        if cand in meta_df.columns:\n",
    "            abs_pm_col = cand\n",
    "            break\n",
    "    fwy_col2 = None\n",
    "    for cand in [\"Fwy\", \"FWY\", \"fwy\", \"Freeway\"]:\n",
    "        if cand in meta_df.columns:\n",
    "            fwy_col2 = cand\n",
    "            break\n",
    "\n",
    "    if abs_pm_col is None or fwy_col2 is None:\n",
    "        raise KeyError(f\"Metadata missing Abs PM or Fwy columns. Found AbsPM={abs_pm_col}, Fwy={fwy_col2}\")\n",
    "\n",
    "    meta_sub = meta_df[meta_df[id_col].isin(stations)].copy()\n",
    "    meta_sub[\"abs_pm\"] = pd.to_numeric(meta_sub[abs_pm_col], errors=\"coerce\")\n",
    "    meta_sub[\"fwy\"] = meta_sub[fwy_col2].astype(str)\n",
    "\n",
    "    # station index map\n",
    "    station_to_idx = {s: i for i, s in enumerate(stations)}\n",
    "    N = len(stations)\n",
    "    A = np.zeros((N, N), dtype=np.float32)\n",
    "\n",
    "    # estimate sigma from typical neighbor distances\n",
    "    all_dists = []\n",
    "    for fwy, grp in meta_sub.sort_values([\"fwy\", \"abs_pm\"]).groupby(\"fwy\"):\n",
    "        pm = grp[\"abs_pm\"].dropna().values\n",
    "        if len(pm) < 2:\n",
    "            continue\n",
    "        d = np.diff(np.sort(pm))\n",
    "        d = d[d > 0]\n",
    "        all_dists.extend(d.tolist())\n",
    "\n",
    "    sigma = float(np.median(all_dists)) if len(all_dists) else 0.5\n",
    "    sigma = max(sigma, 1e-3)\n",
    "\n",
    "    def w(dist):  # gaussian weight\n",
    "        return float(np.exp(- (dist**2) / (sigma**2)))\n",
    "\n",
    "    # connect neighbors\n",
    "    for fwy, grp in meta_sub.sort_values([\"fwy\", \"abs_pm\"]).groupby(\"fwy\"):\n",
    "        grp = grp.dropna(subset=[\"abs_pm\"]).sort_values(\"abs_pm\")\n",
    "        ids = grp[id_col].astype(int).tolist()\n",
    "        pms = grp[\"abs_pm\"].astype(float).tolist()\n",
    "\n",
    "        for i, sid in enumerate(ids):\n",
    "            ii = station_to_idx[sid]\n",
    "            for step in range(1, k_neighbors + 1):\n",
    "                if i - step >= 0:\n",
    "                    sj = ids[i - step]; jj = station_to_idx[sj]\n",
    "                    A[ii, jj] = w(abs(pms[i] - pms[i-step]))\n",
    "                if i + step < len(ids):\n",
    "                    sj = ids[i + step]; jj = station_to_idx[sj]\n",
    "                    A[ii, jj] = w(abs(pms[i] - pms[i+step]))\n",
    "\n",
    "    # self loops + symmetrize\n",
    "    np.fill_diagonal(A, 1.0)\n",
    "    A = np.maximum(A, A.T)\n",
    "    return A\n",
    "\n",
    "# metadata table for adjacency should be meta with standardized station column\n",
    "meta_for_adj = meta.copy()\n",
    "meta_for_adj[\"station\"] = meta_for_adj[\"station\"].astype(int)\n",
    "\n",
    "A = build_adjacency_from_metadata(meta_for_adj, stations=stations, k_neighbors=K_NEIGHBORS)\n",
    "print(\"A shape:\", A.shape)\n",
    "print(\"Adjacency density (A>0):\", float((A > 0).mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf7f0ba-39f7-4563-a319-8809315ec010",
   "metadata": {},
   "source": [
    "## Sliding windows + splits\n",
    "\n",
    "Each sample uses:\n",
    "- Input window:  X[t : t+IN_LEN]\n",
    "- Output window: Y[t+IN_LEN : t+IN_LEN+OUT_LEN]\n",
    "\n",
    "We split by the **time of the first predicted hour** (t + IN_LEN):\n",
    "- Train if output_start_time ≤ TRAIN_END\n",
    "- Val   if TRAIN_END < output_start_time ≤ VAL_END\n",
    "- Test  if output_start_time > VAL_END\n",
    "\n",
    "Then we save everything to a single `.npz` artifact so every model reads the exact same dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ae1abeb-d866-4754-84c2-8e9e7c9353ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T20:41:15.441072Z",
     "iopub.status.busy": "2026-02-07T20:41:15.440758Z",
     "iopub.status.idle": "2026-02-07T20:41:19.936565Z",
     "shell.execute_reply": "2026-02-07T20:41:19.935601Z",
     "shell.execute_reply.started": "2026-02-07T20:41:15.441048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window starts: train=1080, val=360, test=673\n",
      "Saved: artifacts/pems_graph_dataset.npz\n"
     ]
    }
   ],
   "source": [
    "# Sliding window starts\n",
    "T_total = X.shape[0]\n",
    "max_t = T_total - (IN_LEN + OUT_LEN) + 1\n",
    "starts = np.arange(max_t, dtype=np.int32)\n",
    "\n",
    "timestamps = pd.DatetimeIndex(flow_imp.index)\n",
    "out_start_times = timestamps[starts + IN_LEN]\n",
    "\n",
    "train_starts = starts[out_start_times <= TRAIN_END]\n",
    "val_starts   = starts[(out_start_times > TRAIN_END) & (out_start_times <= VAL_END)]\n",
    "test_starts  = starts[out_start_times > VAL_END]\n",
    "\n",
    "print(f\"Window starts: train={len(train_starts)}, val={len(val_starts)}, test={len(test_starts)}\")\n",
    "\n",
    "# Train-only scalers (per node) for flow and speed (channels 0 and 1)\n",
    "train_time_mask = timestamps <= TRAIN_END\n",
    "\n",
    "flow_mean = X[train_time_mask, :, 0].mean(axis=0).astype(np.float32)\n",
    "flow_std  = (X[train_time_mask, :, 0].std(axis=0) + 1e-6).astype(np.float32)\n",
    "\n",
    "speed_mean = X[train_time_mask, :, 1].mean(axis=0).astype(np.float32)\n",
    "speed_std  = (X[train_time_mask, :, 1].std(axis=0) + 1e-6).astype(np.float32)\n",
    "\n",
    "np.savez_compressed(\n",
    "    DATASET_NPZ,\n",
    "    X=X.astype(np.float32),\n",
    "    Y=Y.astype(np.float32),\n",
    "    A=A.astype(np.float32),\n",
    "    stations=stations.astype(np.int32),\n",
    "    timestamps=np.array(timestamps.astype(\"datetime64[ns]\")),\n",
    "    train_starts=train_starts,\n",
    "    val_starts=val_starts,\n",
    "    test_starts=test_starts,\n",
    "    in_len=np.array([IN_LEN], dtype=np.int32),\n",
    "    out_len=np.array([OUT_LEN], dtype=np.int32),\n",
    "    flow_mean=flow_mean, flow_std=flow_std,\n",
    "    speed_mean=speed_mean, speed_std=speed_std,\n",
    "    seed=np.array([SEED], dtype=np.int32),\n",
    ")\n",
    "\n",
    "print(\"Saved:\", DATASET_NPZ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604d7ebe-5650-4bac-b5be-468fe9cfb24d",
   "metadata": {},
   "source": [
    "## Fix window split leakage (strict horizon containment)\n",
    "\n",
    "A window starting at time t uses:\n",
    "- Input:  X[t : t+IN_LEN]\n",
    "- Output: Y[t+IN_LEN : t+IN_LEN+OUT_LEN]\n",
    "\n",
    "To prevent label leakage across train/val/test boundaries, we require:\n",
    "- Train: output_end_time ≤ TRAIN_END\n",
    "- Val:   output_start_time > TRAIN_END AND output_end_time ≤ VAL_END\n",
    "- Test:  output_start_time > VAL_END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26396172-e9bd-4411-b484-0b5dd724bd1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T21:10:34.519135Z",
     "iopub.status.busy": "2026-02-07T21:10:34.518819Z",
     "iopub.status.idle": "2026-02-07T21:10:39.611410Z",
     "shell.execute_reply": "2026-02-07T21:10:39.607849Z",
     "shell.execute_reply.started": "2026-02-07T21:10:34.519115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRICT window starts:\n",
      "train: 1009\n",
      "val:   289\n",
      "test:  673\n",
      "Saved strict dataset to: artifacts/pems_graph_dataset_strict.npz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATASET_NPZ = Path(\"artifacts/pems_graph_dataset.npz\")\n",
    "DATASET_NPZ_STRICT = Path(\"artifacts/pems_graph_dataset_strict.npz\")\n",
    "\n",
    "data = np.load(DATASET_NPZ, allow_pickle=True)\n",
    "\n",
    "X = data[\"X\"]\n",
    "Y = data[\"Y\"]\n",
    "A = data[\"A\"]\n",
    "stations = data[\"stations\"]\n",
    "timestamps = pd.to_datetime(data[\"timestamps\"])\n",
    "\n",
    "IN_LEN = int(data[\"in_len\"][0])\n",
    "OUT_LEN = int(data[\"out_len\"][0])\n",
    "\n",
    "flow_mean = data[\"flow_mean\"]\n",
    "flow_std  = data[\"flow_std\"]\n",
    "speed_mean = data[\"speed_mean\"]\n",
    "speed_std  = data[\"speed_std\"]\n",
    "\n",
    "T_total = X.shape[0]\n",
    "max_t = T_total - (IN_LEN + OUT_LEN) + 1\n",
    "starts = np.arange(max_t, dtype=np.int32)\n",
    "\n",
    "out_start_times = timestamps[starts + IN_LEN]\n",
    "out_end_times   = timestamps[starts + IN_LEN + OUT_LEN - 1]\n",
    "\n",
    "TRAIN_END = pd.Timestamp(\"2024-11-15 23:59:59\")\n",
    "VAL_END   = pd.Timestamp(\"2024-11-30 23:59:59\")\n",
    "\n",
    "# Strict splits\n",
    "train_starts = starts[out_end_times <= TRAIN_END]\n",
    "val_starts   = starts[(out_start_times > TRAIN_END) & (out_end_times <= VAL_END)]\n",
    "test_starts  = starts[out_start_times > VAL_END]\n",
    "\n",
    "print(\"STRICT window starts:\")\n",
    "print(\"train:\", len(train_starts))\n",
    "print(\"val:  \", len(val_starts))\n",
    "print(\"test: \", len(test_starts))\n",
    "\n",
    "np.savez_compressed(\n",
    "    DATASET_NPZ_STRICT,\n",
    "    X=X.astype(np.float32),\n",
    "    Y=Y.astype(np.float32),\n",
    "    A=A.astype(np.float32),\n",
    "    stations=stations.astype(np.int32),\n",
    "    timestamps=np.array(timestamps.astype(\"datetime64[ns]\")),\n",
    "    train_starts=train_starts,\n",
    "    val_starts=val_starts,\n",
    "    test_starts=test_starts,\n",
    "    in_len=np.array([IN_LEN], dtype=np.int32),\n",
    "    out_len=np.array([OUT_LEN], dtype=np.int32),\n",
    "    flow_mean=flow_mean.astype(np.float32),\n",
    "    flow_std=flow_std.astype(np.float32),\n",
    "    speed_mean=speed_mean.astype(np.float32),\n",
    "    speed_std=speed_std.astype(np.float32),\n",
    ")\n",
    "\n",
    "print(\"Saved strict dataset to:\", DATASET_NPZ_STRICT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb6355d-15db-480a-88c0-4504c380b16a",
   "metadata": {},
   "source": [
    "## Load strict dataset artifact\n",
    "\n",
    "We will only use the strict `.npz` going forward to ensure no leakage in labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5515da5a-67a7-47dd-a481-a12fc40907e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T21:11:06.047641Z",
     "iopub.status.busy": "2026-02-07T21:11:06.047230Z",
     "iopub.status.idle": "2026-02-07T21:11:06.540671Z",
     "shell.execute_reply": "2026-02-07T21:11:06.539927Z",
     "shell.execute_reply.started": "2026-02-07T21:11:06.047617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (2208, 1821, 6) Y: (2208, 1821) A: (1821, 1821)\n",
      "starts: 1009 289 673\n",
      "IN_LEN: 24 OUT_LEN: 72\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATASET_NPZ_STRICT = Path(\"artifacts/pems_graph_dataset_strict.npz\")\n",
    "d = np.load(DATASET_NPZ_STRICT, allow_pickle=True)\n",
    "\n",
    "X = d[\"X\"]          # (T,N,F)\n",
    "Y = d[\"Y\"]          # (T,N)\n",
    "A = d[\"A\"]          # (N,N)\n",
    "stations = d[\"stations\"]\n",
    "timestamps = pd.to_datetime(d[\"timestamps\"])\n",
    "\n",
    "train_starts = d[\"train_starts\"]\n",
    "val_starts   = d[\"val_starts\"]\n",
    "test_starts  = d[\"test_starts\"]\n",
    "\n",
    "IN_LEN = int(d[\"in_len\"][0])\n",
    "OUT_LEN = int(d[\"out_len\"][0])\n",
    "\n",
    "flow_mean = d[\"flow_mean\"]  # (N,)\n",
    "flow_std  = d[\"flow_std\"]   # (N,)\n",
    "\n",
    "print(\"X:\", X.shape, \"Y:\", Y.shape, \"A:\", A.shape)\n",
    "print(\"starts:\", len(train_starts), len(val_starts), len(test_starts))\n",
    "print(\"IN_LEN:\", IN_LEN, \"OUT_LEN:\", OUT_LEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ada36a4-6634-47c0-98c5-7353202d241c",
   "metadata": {},
   "source": [
    "## Baseline evaluation\n",
    "\n",
    "We evaluate at horizons: 12, 24, 48, 72 hours ahead.\n",
    "\n",
    "Important detail:\n",
    "Our output sequence begins at +1 hour ahead of the last input time.\n",
    "So horizon `h` corresponds to output index `h-1` in the 72-step target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "737f6784-238b-4c35-a5bc-ea736646d0b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T21:12:53.206372Z",
     "iopub.status.busy": "2026-02-07T21:12:53.205790Z",
     "iopub.status.idle": "2026-02-07T21:12:53.212959Z",
     "shell.execute_reply": "2026-02-07T21:12:53.211829Z",
     "shell.execute_reply.started": "2026-02-07T21:12:53.206347Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "EVAL_HORIZONS = [12, 24, 48, 72]\n",
    "\n",
    "def init_metric_accumulators(horizons):\n",
    "    return {\n",
    "        h: {\"abs_sum\": 0.0, \"sq_sum\": 0.0, \"count\": 0}\n",
    "        for h in horizons\n",
    "    }\n",
    "\n",
    "def finalize_metrics(acc):\n",
    "    out = {}\n",
    "    for h, v in acc.items():\n",
    "        mae = v[\"abs_sum\"] / max(v[\"count\"], 1)\n",
    "        rmse = np.sqrt(v[\"sq_sum\"] / max(v[\"count\"], 1))\n",
    "        out[h] = {\"MAE\": mae, \"RMSE\": rmse}\n",
    "    return out\n",
    "\n",
    "def print_metrics(title, metrics_dict):\n",
    "    print(\"\\n\" + title)\n",
    "    for h in sorted(metrics_dict.keys()):\n",
    "        print(f\"  {h:>3}h  MAE={metrics_dict[h]['MAE']:.3f}  RMSE={metrics_dict[h]['RMSE']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd500bf-d0f5-4205-85c3-2c7a169a645d",
   "metadata": {},
   "source": [
    "### Baseline 1 — Persistence\n",
    "\n",
    "For each station:\n",
    "- Predict that all future horizons equal the **last observed flow** in the input window.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8e7a280-81bd-4fea-8134-396e459526ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T21:14:18.584055Z",
     "iopub.status.busy": "2026-02-07T21:14:18.583297Z",
     "iopub.status.idle": "2026-02-07T21:14:18.770971Z",
     "shell.execute_reply": "2026-02-07T21:14:18.769750Z",
     "shell.execute_reply.started": "2026-02-07T21:14:18.584029Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49fa4fe544b943cfaf7ccc68a472cdd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Persistence (val):   0%|          | 0/289 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8248ff923c3a46dba30cc6d3c4398268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Persistence (test):   0%|          | 0/673 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Persistence — Validation\n",
      "   12h  MAE=910.133  RMSE=1437.403\n",
      "   24h  MAE=151.755  RMSE=354.476\n",
      "   48h  MAE=203.020  RMSE=451.021\n",
      "   72h  MAE=220.150  RMSE=478.588\n",
      "\n",
      "Persistence — Test\n",
      "   12h  MAE=917.014  RMSE=1455.414\n",
      "   24h  MAE=147.896  RMSE=340.247\n",
      "   48h  MAE=200.592  RMSE=443.364\n",
      "   72h  MAE=196.856  RMSE=431.611\n"
     ]
    }
   ],
   "source": [
    "def eval_persistence(X, Y, starts, in_len, horizons, desc=\"\"):\n",
    "    acc = init_metric_accumulators(horizons)\n",
    "\n",
    "    for t in tqdm(starts, desc=desc):\n",
    "        # last observed flow at the end of input window\n",
    "        last_flow = X[t + in_len - 1, :, 0]  # (N,)\n",
    "\n",
    "        for h in horizons:\n",
    "            idx = h - 1\n",
    "            true = Y[t + in_len + idx, :]     # (N,)\n",
    "            pred = last_flow                  # (N,)\n",
    "\n",
    "            err = pred - true\n",
    "            acc[h][\"abs_sum\"] += float(np.abs(err).sum())\n",
    "            acc[h][\"sq_sum\"]  += float((err ** 2).sum())\n",
    "            acc[h][\"count\"]   += err.size\n",
    "\n",
    "    return finalize_metrics(acc)\n",
    "\n",
    "pers_val  = eval_persistence(X, Y, val_starts, IN_LEN, EVAL_HORIZONS, desc=\"Persistence (val)\")\n",
    "pers_test = eval_persistence(X, Y, test_starts, IN_LEN, EVAL_HORIZONS, desc=\"Persistence (test)\")\n",
    "\n",
    "print_metrics(\"Persistence — Validation\", pers_val)\n",
    "print_metrics(\"Persistence — Test\", pers_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61797eea-3aad-422c-ab05-4cfc275c3280",
   "metadata": {},
   "source": [
    "### Baseline 2 — Historical Average (HA-168)\n",
    "\n",
    "We compute a per-node seasonal mean using train data only:\n",
    "- slot = (day_of_week * 24 + hour) ∈ [0..167]\n",
    "- mean_flow[slot, node] = average flow in train for that slot\n",
    "\n",
    "Forecast:\n",
    "- For each horizon step, use the slot mean of that future timestamp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f1ecda2-abdd-4241-bb4e-3215a5cb224d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T21:15:48.783046Z",
     "iopub.status.busy": "2026-02-07T21:15:48.782558Z",
     "iopub.status.idle": "2026-02-07T21:15:48.972239Z",
     "shell.execute_reply": "2026-02-07T21:15:48.971202Z",
     "shell.execute_reply.started": "2026-02-07T21:15:48.783009Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab956bdc0c8415f836d0662d4fb5523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HA-168 (val):   0%|          | 0/289 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c08a3390fe74619a202d791c8b2c9dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HA-168 (test):   0%|          | 0/673 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HA-168 — Validation\n",
      "   12h  MAE=116.424  RMSE=258.349\n",
      "   24h  MAE=123.997  RMSE=277.816\n",
      "   48h  MAE=134.454  RMSE=302.695\n",
      "   72h  MAE=137.523  RMSE=306.085\n",
      "\n",
      "HA-168 — Test\n",
      "   12h  MAE=119.650  RMSE=283.458\n",
      "   24h  MAE=120.382  RMSE=284.451\n",
      "   48h  MAE=122.195  RMSE=286.807\n",
      "   72h  MAE=126.187  RMSE=293.463\n"
     ]
    }
   ],
   "source": [
    "def build_ha168_means(Y, timestamps, train_end):\n",
    "    train_mask = timestamps <= train_end\n",
    "    Y_train = Y[train_mask]  # (T_train, N)\n",
    "    ts_train = timestamps[train_mask]\n",
    "\n",
    "    slot = ts_train.dayofweek.to_numpy() * 24 + ts_train.hour.to_numpy()  # (T_train,)\n",
    "    G = 168\n",
    "    N = Y.shape[1]\n",
    "\n",
    "    means = np.zeros((G, N), dtype=np.float32)\n",
    "    counts = np.zeros((G,), dtype=np.int64)\n",
    "\n",
    "    for g in range(G):\n",
    "        m = (slot == g)\n",
    "        if m.any():\n",
    "            means[g] = Y_train[m].mean(axis=0)\n",
    "            counts[g] = int(m.sum())\n",
    "        else:\n",
    "            # fallback (should be rare)\n",
    "            means[g] = Y_train.mean(axis=0)\n",
    "            counts[g] = 0\n",
    "    return means, counts\n",
    "\n",
    "def eval_ha168(Y, timestamps, starts, in_len, horizons, means_ha168, desc=\"\"):\n",
    "    acc = init_metric_accumulators(horizons)\n",
    "\n",
    "    for t in tqdm(starts, desc=desc):\n",
    "        for h in horizons:\n",
    "            idx = h - 1\n",
    "            future_time = timestamps[t + in_len + idx]\n",
    "            g = int(future_time.dayofweek * 24 + future_time.hour)\n",
    "\n",
    "            pred = means_ha168[g, :]                 # (N,)\n",
    "            true = Y[t + in_len + idx, :]            # (N,)\n",
    "\n",
    "            err = pred - true\n",
    "            acc[h][\"abs_sum\"] += float(np.abs(err).sum())\n",
    "            acc[h][\"sq_sum\"]  += float((err ** 2).sum())\n",
    "            acc[h][\"count\"]   += err.size\n",
    "\n",
    "    return finalize_metrics(acc)\n",
    "\n",
    "TRAIN_END = pd.Timestamp(\"2024-11-15 23:59:59\")\n",
    "ha_means, ha_counts = build_ha168_means(Y, timestamps, TRAIN_END)\n",
    "\n",
    "ha_val  = eval_ha168(Y, timestamps, val_starts, IN_LEN, EVAL_HORIZONS, ha_means, desc=\"HA-168 (val)\")\n",
    "ha_test = eval_ha168(Y, timestamps, test_starts, IN_LEN, EVAL_HORIZONS, ha_means, desc=\"HA-168 (test)\")\n",
    "\n",
    "print_metrics(\"HA-168 — Validation\", ha_val)\n",
    "print_metrics(\"HA-168 — Test\", ha_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcc74e4-add9-43ab-a18c-123e4dc5142b",
   "metadata": {},
   "source": [
    "## PyTorch Dataset for sliding windows\n",
    "\n",
    "Each item returns:\n",
    "- x: (C, N, IN_LEN)   where C = number of features (6)\n",
    "- y: (OUT_LEN, N)     scaled flow targets\n",
    "\n",
    "Scaling:\n",
    "- We scale flow and speed using TRAIN-only mean/std (per node)\n",
    "- Time features remain unchanged (already bounded by sin/cos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2c00304-797e-43d5-b857-245c266f37d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T21:16:13.532982Z",
     "iopub.status.busy": "2026-02-07T21:16:13.532696Z",
     "iopub.status.idle": "2026-02-07T21:16:13.906511Z",
     "shell.execute_reply": "2026-02-07T21:16:13.905799Z",
     "shell.execute_reply.started": "2026-02-07T21:16:13.532959Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch x: torch.Size([16, 6, 1821, 24]) Batch y: torch.Size([16, 72, 1821])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PemsWindowDataset(Dataset):\n",
    "    def __init__(self, X, Y, starts, in_len, out_len, flow_mean, flow_std, speed_mean, speed_std):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.starts = starts\n",
    "        self.in_len = in_len\n",
    "        self.out_len = out_len\n",
    "\n",
    "        self.flow_mean = flow_mean.astype(np.float32)\n",
    "        self.flow_std  = flow_std.astype(np.float32)\n",
    "        self.speed_mean = speed_mean.astype(np.float32)\n",
    "        self.speed_std  = speed_std.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.starts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t = int(self.starts[idx])\n",
    "\n",
    "        x = self.X[t : t + self.in_len].copy().astype(np.float32)  # (IN_LEN, N, F)\n",
    "        y = self.Y[t + self.in_len : t + self.in_len + self.out_len].copy().astype(np.float32)  # (OUT_LEN, N)\n",
    "\n",
    "        # scale input channels: flow=0, speed=1\n",
    "        x[..., 0] = (x[..., 0] - self.flow_mean[None, :]) / self.flow_std[None, :]\n",
    "        x[..., 1] = (x[..., 1] - self.speed_mean[None, :]) / self.speed_std[None, :]\n",
    "\n",
    "        # scale targets (flow)\n",
    "        y = (y - self.flow_mean[None, :]) / self.flow_std[None, :]\n",
    "\n",
    "        # rearrange x to (C, N, IN_LEN) for conv models\n",
    "        x = np.transpose(x, (2, 1, 0))  # (F, N, IN_LEN)\n",
    "\n",
    "        return torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "# Load speed scaler too\n",
    "speed_mean = d[\"speed_mean\"]\n",
    "speed_std  = d[\"speed_std\"]\n",
    "\n",
    "train_ds = PemsWindowDataset(X, Y, train_starts, IN_LEN, OUT_LEN, flow_mean, flow_std, speed_mean, speed_std)\n",
    "val_ds   = PemsWindowDataset(X, Y, val_starts,   IN_LEN, OUT_LEN, flow_mean, flow_std, speed_mean, speed_std)\n",
    "test_ds  = PemsWindowDataset(X, Y, test_starts,  IN_LEN, OUT_LEN, flow_mean, flow_std, speed_mean, speed_std)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "xb, yb = next(iter(train_loader))\n",
    "print(\"Batch x:\", xb.shape, \"Batch y:\", yb.shape)\n",
    "# Expect: x=(B, 6, N, 24) and y=(B, 72, N)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

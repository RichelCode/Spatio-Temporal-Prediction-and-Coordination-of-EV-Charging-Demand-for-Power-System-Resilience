{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39f2ed5a-37a6-4e2a-ad00-1f590cc07f21",
   "metadata": {},
   "source": [
    "#  Spatio-Temporal Prediction and Coordination of EV Charging Demand for Power System Resilience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01198064-dd96-4a18-b994-23e9eb6bc67d",
   "metadata": {},
   "source": [
    "## Research Objectives\n",
    "\n",
    "Recent studies have explored electric vehicles (EVs) from different perspectives, ranging from estimating vehicle range based on battery capacity, model specifications, and internal components (Ahmed et al., 2022) to forecasting charging behavior using machine learning methods such as Random Forest and SVM with factors like previous payment data, weather, and traffic (Shahriar et al., 2020). In parallel, research on smart cities has focused on managing traffic flow efficiently to reduce congestion and energy consumption (Dymora, Mazurek, & Jucha, 2024).\n",
    "\n",
    "Building on these insights, this study links traffic dynamics with EV energy consumption to better predict when and where charging demand will arise. By integrating spatio-temporal traffic features with deep learning models, the goal is to anticipate EV charging needs in real time and enable coordinated charging strategies that support overall power system resilience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ca069f-f9ae-41da-af3a-f50b5a0caaba",
   "metadata": {},
   "source": [
    "## Load Required Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0fc73d5-615f-4d6d-be64-dda46bebebeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet\n",
    "from scipy import stats\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import random\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1c1bb6-b368-4a83-9dac-bdfea10497b2",
   "metadata": {},
   "source": [
    "## Load and Clean the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf82b185-a4c5-4814-9f69-8e9ef96f6b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"cleaned_traffic_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6878437b-d025-4604-b133-d01b8dc4c2c1",
   "metadata": {},
   "source": [
    "## How the data looks directly from PEMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f06245c-ff8e-438e-a6e1-4743e1a6be3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Station</th>\n",
       "      <th>District</th>\n",
       "      <th>Route</th>\n",
       "      <th>Direction of Travel</th>\n",
       "      <th>Lane Type</th>\n",
       "      <th>Station Length</th>\n",
       "      <th>Samples</th>\n",
       "      <th>% Observed</th>\n",
       "      <th>Total Flow</th>\n",
       "      <th>...</th>\n",
       "      <th>Lane 5 Avg Speed</th>\n",
       "      <th>Lane 6 Flow</th>\n",
       "      <th>Lane 6 Avg Occ</th>\n",
       "      <th>Lane 6 Avg Speed</th>\n",
       "      <th>Lane 7 Flow</th>\n",
       "      <th>Lane 7 Avg Occ</th>\n",
       "      <th>Lane 7 Avg Speed</th>\n",
       "      <th>Lane 8 Flow</th>\n",
       "      <th>Lane 8 Avg Occ</th>\n",
       "      <th>Lane 8 Avg Speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10/01/2024 00:00:00</td>\n",
       "      <td>308512</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>W</td>\n",
       "      <td>ML</td>\n",
       "      <td>3.995</td>\n",
       "      <td>197</td>\n",
       "      <td>0</td>\n",
       "      <td>497.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/01/2024 00:00:00</td>\n",
       "      <td>311831</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>S</td>\n",
       "      <td>OR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101</td>\n",
       "      <td>92</td>\n",
       "      <td>27.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10/01/2024 00:00:00</td>\n",
       "      <td>311832</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>S</td>\n",
       "      <td>FR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101</td>\n",
       "      <td>92</td>\n",
       "      <td>78.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10/01/2024 00:00:00</td>\n",
       "      <td>311844</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>N</td>\n",
       "      <td>OR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>202</td>\n",
       "      <td>92</td>\n",
       "      <td>43.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10/01/2024 00:00:00</td>\n",
       "      <td>311847</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>N</td>\n",
       "      <td>OR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>303</td>\n",
       "      <td>92</td>\n",
       "      <td>73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Timestamp  Station  District  Route Direction of Travel  \\\n",
       "0  10/01/2024 00:00:00   308512         3     50                   W   \n",
       "1  10/01/2024 00:00:00   311831         3      5                   S   \n",
       "2  10/01/2024 00:00:00   311832         3      5                   S   \n",
       "3  10/01/2024 00:00:00   311844         3      5                   N   \n",
       "4  10/01/2024 00:00:00   311847         3      5                   N   \n",
       "\n",
       "  Lane Type  Station Length  Samples  % Observed  Total Flow  ...  \\\n",
       "0        ML           3.995      197           0       497.0  ...   \n",
       "1        OR             NaN      101          92        27.0  ...   \n",
       "2        FR             NaN      101          92        78.0  ...   \n",
       "3        OR             NaN      202          92        43.0  ...   \n",
       "4        OR             NaN      303          92        73.0  ...   \n",
       "\n",
       "   Lane 5 Avg Speed  Lane 6 Flow  Lane 6 Avg Occ  Lane 6 Avg Speed  \\\n",
       "0               NaN          NaN             NaN               NaN   \n",
       "1               NaN          NaN             NaN               NaN   \n",
       "2               NaN          NaN             NaN               NaN   \n",
       "3               NaN          NaN             NaN               NaN   \n",
       "4               NaN          NaN             NaN               NaN   \n",
       "\n",
       "   Lane 7 Flow  Lane 7 Avg Occ  Lane 7 Avg Speed  Lane 8 Flow  Lane 8 Avg Occ  \\\n",
       "0          NaN             NaN               NaN          NaN             NaN   \n",
       "1          NaN             NaN               NaN          NaN             NaN   \n",
       "2          NaN             NaN               NaN          NaN             NaN   \n",
       "3          NaN             NaN               NaN          NaN             NaN   \n",
       "4          NaN             NaN               NaN          NaN             NaN   \n",
       "\n",
       "   Lane 8 Avg Speed  \n",
       "0               NaN  \n",
       "1               NaN  \n",
       "2               NaN  \n",
       "3               NaN  \n",
       "4               NaN  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8afcbac-ee79-49cb-aa8c-2223430535ed",
   "metadata": {},
   "source": [
    "### We ignore and remove features that contain only NAN values, and maintain the other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f56d4bcc-49a9-4286-9f00-8cafecf313f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the final selected columns\n",
    "selected_columns = [\n",
    "    \"Timestamp\", \"Station\", \"Route\", \"Direction of Travel\",\n",
    "    \"Total Flow\", \"Avg Speed\", \"% Observed\",\"Samples\",\"Lane Type\"\n",
    "]\n",
    "\n",
    "# Keep only the selected columns\n",
    "df1 = df1[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "826e6a46-5d77-4f38-895a-343e67165846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Station</th>\n",
       "      <th>Route</th>\n",
       "      <th>Direction of Travel</th>\n",
       "      <th>Total Flow</th>\n",
       "      <th>Avg Speed</th>\n",
       "      <th>% Observed</th>\n",
       "      <th>Samples</th>\n",
       "      <th>Lane Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10/01/2024 00:00:00</td>\n",
       "      <td>308512</td>\n",
       "      <td>50</td>\n",
       "      <td>W</td>\n",
       "      <td>497.0</td>\n",
       "      <td>64.1</td>\n",
       "      <td>0</td>\n",
       "      <td>197</td>\n",
       "      <td>ML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/01/2024 00:00:00</td>\n",
       "      <td>311831</td>\n",
       "      <td>5</td>\n",
       "      <td>S</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92</td>\n",
       "      <td>101</td>\n",
       "      <td>OR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10/01/2024 00:00:00</td>\n",
       "      <td>311832</td>\n",
       "      <td>5</td>\n",
       "      <td>S</td>\n",
       "      <td>78.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92</td>\n",
       "      <td>101</td>\n",
       "      <td>FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10/01/2024 00:00:00</td>\n",
       "      <td>311844</td>\n",
       "      <td>5</td>\n",
       "      <td>N</td>\n",
       "      <td>43.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92</td>\n",
       "      <td>202</td>\n",
       "      <td>OR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10/01/2024 00:00:00</td>\n",
       "      <td>311847</td>\n",
       "      <td>5</td>\n",
       "      <td>N</td>\n",
       "      <td>73.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92</td>\n",
       "      <td>303</td>\n",
       "      <td>OR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4114675</th>\n",
       "      <td>12/31/2024 23:00:00</td>\n",
       "      <td>3423094</td>\n",
       "      <td>99</td>\n",
       "      <td>S</td>\n",
       "      <td>68.0</td>\n",
       "      <td>64.8</td>\n",
       "      <td>96</td>\n",
       "      <td>118</td>\n",
       "      <td>ML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4114676</th>\n",
       "      <td>12/31/2024 23:00:00</td>\n",
       "      <td>3900021</td>\n",
       "      <td>50</td>\n",
       "      <td>E</td>\n",
       "      <td>803.0</td>\n",
       "      <td>66.5</td>\n",
       "      <td>67</td>\n",
       "      <td>292</td>\n",
       "      <td>ML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4114677</th>\n",
       "      <td>12/31/2024 23:00:00</td>\n",
       "      <td>3900022</td>\n",
       "      <td>50</td>\n",
       "      <td>E</td>\n",
       "      <td>509.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4114678</th>\n",
       "      <td>12/31/2024 23:00:00</td>\n",
       "      <td>3900023</td>\n",
       "      <td>50</td>\n",
       "      <td>W</td>\n",
       "      <td>881.0</td>\n",
       "      <td>67.4</td>\n",
       "      <td>67</td>\n",
       "      <td>289</td>\n",
       "      <td>ML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4114679</th>\n",
       "      <td>12/31/2024 23:00:00</td>\n",
       "      <td>3900024</td>\n",
       "      <td>50</td>\n",
       "      <td>W</td>\n",
       "      <td>509.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>HV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4114680 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Timestamp  Station  Route Direction of Travel  Total Flow  \\\n",
       "0        10/01/2024 00:00:00   308512     50                   W       497.0   \n",
       "1        10/01/2024 00:00:00   311831      5                   S        27.0   \n",
       "2        10/01/2024 00:00:00   311832      5                   S        78.0   \n",
       "3        10/01/2024 00:00:00   311844      5                   N        43.0   \n",
       "4        10/01/2024 00:00:00   311847      5                   N        73.0   \n",
       "...                      ...      ...    ...                 ...         ...   \n",
       "4114675  12/31/2024 23:00:00  3423094     99                   S        68.0   \n",
       "4114676  12/31/2024 23:00:00  3900021     50                   E       803.0   \n",
       "4114677  12/31/2024 23:00:00  3900022     50                   E       509.0   \n",
       "4114678  12/31/2024 23:00:00  3900023     50                   W       881.0   \n",
       "4114679  12/31/2024 23:00:00  3900024     50                   W       509.0   \n",
       "\n",
       "         Avg Speed  % Observed  Samples Lane Type  \n",
       "0             64.1           0      197        ML  \n",
       "1              NaN          92      101        OR  \n",
       "2              NaN          92      101        FR  \n",
       "3              NaN          92      202        OR  \n",
       "4              NaN          92      303        OR  \n",
       "...            ...         ...      ...       ...  \n",
       "4114675       64.8          96      118        ML  \n",
       "4114676       66.5          67      292        ML  \n",
       "4114677       68.0           0        0        HV  \n",
       "4114678       67.4          67      289        ML  \n",
       "4114679       68.0           0       56        HV  \n",
       "\n",
       "[4114680 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9644693",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = pd.read_excel(\"pems_output.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "056b1f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fwy</th>\n",
       "      <th>District</th>\n",
       "      <th>County</th>\n",
       "      <th>City</th>\n",
       "      <th>CA PM</th>\n",
       "      <th>Abs PM</th>\n",
       "      <th>Length</th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Lanes</th>\n",
       "      <th>Type</th>\n",
       "      <th>Sensor Type</th>\n",
       "      <th>HOV</th>\n",
       "      <th>MS ID</th>\n",
       "      <th>IRM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I5-N</td>\n",
       "      <td>3</td>\n",
       "      <td>Sacramento</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.919</td>\n",
       "      <td>497.212</td>\n",
       "      <td>4.312</td>\n",
       "      <td>3413014</td>\n",
       "      <td>5NB at Twin Cities Rd</td>\n",
       "      <td>2</td>\n",
       "      <td>Mainline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I5-N</td>\n",
       "      <td>3</td>\n",
       "      <td>Sacramento</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.026</td>\n",
       "      <td>497.319</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3413016</td>\n",
       "      <td>5NB to Twin Cities Rd</td>\n",
       "      <td>4</td>\n",
       "      <td>Off Ramp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I5-N</td>\n",
       "      <td>3</td>\n",
       "      <td>Sacramento</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.498</td>\n",
       "      <td>504.791</td>\n",
       "      <td>3.291</td>\n",
       "      <td>317802</td>\n",
       "      <td>Hood Franklin Rd</td>\n",
       "      <td>2</td>\n",
       "      <td>Mainline</td>\n",
       "      <td>radars</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I5-N</td>\n",
       "      <td>3</td>\n",
       "      <td>Sacramento</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.942</td>\n",
       "      <td>506.235</td>\n",
       "      <td>3.115</td>\n",
       "      <td>3013091</td>\n",
       "      <td>5NB at Elk Grove HOV</td>\n",
       "      <td>1</td>\n",
       "      <td>HOV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24H</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I5-N</td>\n",
       "      <td>3</td>\n",
       "      <td>Sacramento</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.08</td>\n",
       "      <td>506.373</td>\n",
       "      <td>NaN</td>\n",
       "      <td>311844</td>\n",
       "      <td>Elk Grove Blvd 5NB Slip</td>\n",
       "      <td>2</td>\n",
       "      <td>On Ramp</td>\n",
       "      <td>others</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Fwy  District      County City   CA PM   Abs PM  Length       ID  \\\n",
       "0  I5-N         3  Sacramento  NaN   1.919  497.212   4.312  3413014   \n",
       "1  I5-N         3  Sacramento  NaN   2.026  497.319     NaN  3413016   \n",
       "2  I5-N         3  Sacramento  NaN   9.498  504.791   3.291   317802   \n",
       "3  I5-N         3  Sacramento  NaN  10.942  506.235   3.115  3013091   \n",
       "4  I5-N         3  Sacramento  NaN   11.08  506.373     NaN   311844   \n",
       "\n",
       "                      Name  Lanes      Type Sensor Type  HOV  MS ID  IRM  \n",
       "0    5NB at Twin Cities Rd      2  Mainline         NaN   No      1  NaN  \n",
       "1    5NB to Twin Cities Rd      4  Off Ramp         NaN   No      1  NaN  \n",
       "2         Hood Franklin Rd      2  Mainline      radars   No      1  NaN  \n",
       "3     5NB at Elk Grove HOV      1       HOV         NaN  24H      1  NaN  \n",
       "4  Elk Grove Blvd 5NB Slip      2   On Ramp      others   No      1  NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13ed5f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nicer display for debugging\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.width\", 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fd567ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>station</th>\n",
       "      <th>route</th>\n",
       "      <th>direction_of_travel</th>\n",
       "      <th>total_flow</th>\n",
       "      <th>avg_speed</th>\n",
       "      <th>pct_observed</th>\n",
       "      <th>samples</th>\n",
       "      <th>lane_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10/01/2024 00:00:00</td>\n",
       "      <td>308512</td>\n",
       "      <td>50</td>\n",
       "      <td>W</td>\n",
       "      <td>497.0</td>\n",
       "      <td>64.1</td>\n",
       "      <td>0</td>\n",
       "      <td>197</td>\n",
       "      <td>ML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/01/2024 00:00:00</td>\n",
       "      <td>311831</td>\n",
       "      <td>5</td>\n",
       "      <td>S</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92</td>\n",
       "      <td>101</td>\n",
       "      <td>OR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10/01/2024 00:00:00</td>\n",
       "      <td>311832</td>\n",
       "      <td>5</td>\n",
       "      <td>S</td>\n",
       "      <td>78.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92</td>\n",
       "      <td>101</td>\n",
       "      <td>FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10/01/2024 00:00:00</td>\n",
       "      <td>311844</td>\n",
       "      <td>5</td>\n",
       "      <td>N</td>\n",
       "      <td>43.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92</td>\n",
       "      <td>202</td>\n",
       "      <td>OR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10/01/2024 00:00:00</td>\n",
       "      <td>311847</td>\n",
       "      <td>5</td>\n",
       "      <td>N</td>\n",
       "      <td>73.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92</td>\n",
       "      <td>303</td>\n",
       "      <td>OR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp  station  route direction_of_travel  total_flow  avg_speed  pct_observed  samples lane_type\n",
       "0  10/01/2024 00:00:00   308512     50                   W       497.0       64.1             0      197        ML\n",
       "1  10/01/2024 00:00:00   311831      5                   S        27.0        NaN            92      101        OR\n",
       "2  10/01/2024 00:00:00   311832      5                   S        78.0        NaN            92      101        FR\n",
       "3  10/01/2024 00:00:00   311844      5                   N        43.0        NaN            92      202        OR\n",
       "4  10/01/2024 00:00:00   311847      5                   N        73.0        NaN            92      303        OR"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_columns(df):\n",
    "    df = df.copy()\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.strip()\n",
    "        .str.replace(\" \", \"_\")\n",
    "        .str.replace(\"-\", \"_\")\n",
    "        .str.replace(\"/\", \"_\")\n",
    "        .str.replace(\"%\", \"pct\")\n",
    "        .str.lower()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df1 = clean_columns(df1)\n",
    "df1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bebe22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"timestamp\"] = pd.to_datetime(df1[\"timestamp\"], errors=\"coerce\")\n",
    "df1 = df1.sort_values(by=[\"station\", \"timestamp\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ee5d401",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\"id\", \"name\", \"sensor_type\", \"irm\", \"ms_id\"]\n",
    "\n",
    "df1 = df1.drop(columns=[c for c in cols_to_drop if c in df1.columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0402ddbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_metadata = [\n",
    "    \"station\", \"fwy\", \"lane_type\", \"type\", \"lanes\", \n",
    "    \"hov\", \"abs_pm\", \"direction_of_travel\", \"route\"\n",
    "]\n",
    "\n",
    "df1 = df1[[col for col in df1.columns if col not in [\"id\", \"name\", \"irm\", \"sensor_type\", \"ms_id\"]]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc729d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric\n",
    "for col in [\"fwy\", \"lanes\", \"abs_pm\", \"route\"]:\n",
    "    if col in df1.columns:\n",
    "        df1[col] = pd.to_numeric(df1[col], errors=\"coerce\")\n",
    "\n",
    "# categorical\n",
    "for col in [\"lane_type\", \"type\", \"hov\", \"direction_of_travel\"]:\n",
    "    if col in df1.columns:\n",
    "        df1[col] = df1[col].astype(\"category\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9664a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Fwy', 'District', 'County', 'City', 'CA PM', 'Abs PM', 'Length', 'ID', 'Name', 'Lanes', 'Type', 'Sensor Type', 'HOV', 'MS ID', 'IRM'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fwy</th>\n",
       "      <th>District</th>\n",
       "      <th>County</th>\n",
       "      <th>City</th>\n",
       "      <th>CA PM</th>\n",
       "      <th>Abs PM</th>\n",
       "      <th>Length</th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Lanes</th>\n",
       "      <th>Type</th>\n",
       "      <th>Sensor Type</th>\n",
       "      <th>HOV</th>\n",
       "      <th>MS ID</th>\n",
       "      <th>IRM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I5-N</td>\n",
       "      <td>3</td>\n",
       "      <td>Sacramento</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.919</td>\n",
       "      <td>497.212</td>\n",
       "      <td>4.312</td>\n",
       "      <td>3413014</td>\n",
       "      <td>5NB at Twin Cities Rd</td>\n",
       "      <td>2</td>\n",
       "      <td>Mainline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I5-N</td>\n",
       "      <td>3</td>\n",
       "      <td>Sacramento</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.026</td>\n",
       "      <td>497.319</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3413016</td>\n",
       "      <td>5NB to Twin Cities Rd</td>\n",
       "      <td>4</td>\n",
       "      <td>Off Ramp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I5-N</td>\n",
       "      <td>3</td>\n",
       "      <td>Sacramento</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.498</td>\n",
       "      <td>504.791</td>\n",
       "      <td>3.291</td>\n",
       "      <td>317802</td>\n",
       "      <td>Hood Franklin Rd</td>\n",
       "      <td>2</td>\n",
       "      <td>Mainline</td>\n",
       "      <td>radars</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I5-N</td>\n",
       "      <td>3</td>\n",
       "      <td>Sacramento</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.942</td>\n",
       "      <td>506.235</td>\n",
       "      <td>3.115</td>\n",
       "      <td>3013091</td>\n",
       "      <td>5NB at Elk Grove HOV</td>\n",
       "      <td>1</td>\n",
       "      <td>HOV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24H</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I5-N</td>\n",
       "      <td>3</td>\n",
       "      <td>Sacramento</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.08</td>\n",
       "      <td>506.373</td>\n",
       "      <td>NaN</td>\n",
       "      <td>311844</td>\n",
       "      <td>Elk Grove Blvd 5NB Slip</td>\n",
       "      <td>2</td>\n",
       "      <td>On Ramp</td>\n",
       "      <td>others</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Fwy  District      County City   CA PM   Abs PM  Length       ID                     Name  Lanes      Type Sensor Type  HOV  MS ID  IRM\n",
       "0  I5-N         3  Sacramento  NaN   1.919  497.212   4.312  3413014    5NB at Twin Cities Rd      2  Mainline         NaN   No      1  NaN\n",
       "1  I5-N         3  Sacramento  NaN   2.026  497.319     NaN  3413016    5NB to Twin Cities Rd      4  Off Ramp         NaN   No      1  NaN\n",
       "2  I5-N         3  Sacramento  NaN   9.498  504.791   3.291   317802         Hood Franklin Rd      2  Mainline      radars   No      1  NaN\n",
       "3  I5-N         3  Sacramento  NaN  10.942  506.235   3.115  3013091     5NB at Elk Grove HOV      1       HOV         NaN  24H      1  NaN\n",
       "4  I5-N         3  Sacramento  NaN   11.08  506.373     NaN   311844  Elk Grove Blvd 5NB Slip      2   On Ramp      others   No      1  NaN"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(meta_data.columns)\n",
    "meta_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b06d4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>station</th>\n",
       "      <th>route</th>\n",
       "      <th>direction_of_travel</th>\n",
       "      <th>total_flow</th>\n",
       "      <th>avg_speed</th>\n",
       "      <th>pct_observed</th>\n",
       "      <th>samples</th>\n",
       "      <th>lane_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-10-01 01:00:00</td>\n",
       "      <td>308511</td>\n",
       "      <td>50</td>\n",
       "      <td>E</td>\n",
       "      <td>12.0</td>\n",
       "      <td>67.5</td>\n",
       "      <td>100</td>\n",
       "      <td>202</td>\n",
       "      <td>ML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-01 02:00:00</td>\n",
       "      <td>308511</td>\n",
       "      <td>50</td>\n",
       "      <td>E</td>\n",
       "      <td>12.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>100</td>\n",
       "      <td>197</td>\n",
       "      <td>ML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-10-01 03:00:00</td>\n",
       "      <td>308511</td>\n",
       "      <td>50</td>\n",
       "      <td>E</td>\n",
       "      <td>20.0</td>\n",
       "      <td>66.3</td>\n",
       "      <td>92</td>\n",
       "      <td>197</td>\n",
       "      <td>ML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-10-01 04:00:00</td>\n",
       "      <td>308511</td>\n",
       "      <td>50</td>\n",
       "      <td>E</td>\n",
       "      <td>55.0</td>\n",
       "      <td>67.4</td>\n",
       "      <td>100</td>\n",
       "      <td>197</td>\n",
       "      <td>ML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-10-01 05:00:00</td>\n",
       "      <td>308511</td>\n",
       "      <td>50</td>\n",
       "      <td>E</td>\n",
       "      <td>228.0</td>\n",
       "      <td>66.1</td>\n",
       "      <td>83</td>\n",
       "      <td>168</td>\n",
       "      <td>ML</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp  station  route direction_of_travel  total_flow  avg_speed  pct_observed  samples lane_type\n",
       "0 2024-10-01 01:00:00   308511     50                   E        12.0       67.5           100      202        ML\n",
       "1 2024-10-01 02:00:00   308511     50                   E        12.0       67.0           100      197        ML\n",
       "2 2024-10-01 03:00:00   308511     50                   E        20.0       66.3            92      197        ML\n",
       "3 2024-10-01 04:00:00   308511     50                   E        55.0       67.4           100      197        ML\n",
       "4 2024-10-01 05:00:00   308511     50                   E       228.0       66.1            83      168        ML"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "930c581b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1896 unique station IDs in traffic\n",
      "1861 unique IDs in metadata\n",
      "Shared IDs: 1861\n"
     ]
    }
   ],
   "source": [
    "print(df1['station'].nunique(), \"unique station IDs in traffic\")\n",
    "print(meta_data['ID'].nunique(), \"unique IDs in metadata\")\n",
    "\n",
    "shared = set(df1['station']).intersection(set(meta_data['ID']))\n",
    "print(\"Shared IDs:\", len(shared))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc5b5853",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['name', 'sensor_type', 'irm', 'ms_id', 'city', 'ca_pm', 'length']\n",
    "meta_data = meta_data.drop(columns=drop_cols, errors=\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d945be93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df1.merge(\n",
    "    meta_data,\n",
    "    left_on='station',\n",
    "    right_on='ID',\n",
    "    how='left',\n",
    "    validate='m:1'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f552a2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>station</th>\n",
       "      <th>route</th>\n",
       "      <th>direction_of_travel</th>\n",
       "      <th>total_flow</th>\n",
       "      <th>avg_speed</th>\n",
       "      <th>pct_observed</th>\n",
       "      <th>samples</th>\n",
       "      <th>lane_type</th>\n",
       "      <th>fwy</th>\n",
       "      <th>district</th>\n",
       "      <th>county</th>\n",
       "      <th>city</th>\n",
       "      <th>ca_pm</th>\n",
       "      <th>abs_pm</th>\n",
       "      <th>length</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>lanes</th>\n",
       "      <th>type</th>\n",
       "      <th>sensor_type</th>\n",
       "      <th>hov</th>\n",
       "      <th>ms_id</th>\n",
       "      <th>irm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-10-01 01:00:00</td>\n",
       "      <td>308511</td>\n",
       "      <td>50</td>\n",
       "      <td>E</td>\n",
       "      <td>12.0</td>\n",
       "      <td>67.5</td>\n",
       "      <td>100</td>\n",
       "      <td>202</td>\n",
       "      <td>ML</td>\n",
       "      <td>US50-E</td>\n",
       "      <td>3.0</td>\n",
       "      <td>El Dorado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.627</td>\n",
       "      <td>60.162</td>\n",
       "      <td>3.134</td>\n",
       "      <td>308511.0</td>\n",
       "      <td>Sly Park Rd</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Mainline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-01 02:00:00</td>\n",
       "      <td>308511</td>\n",
       "      <td>50</td>\n",
       "      <td>E</td>\n",
       "      <td>12.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>100</td>\n",
       "      <td>197</td>\n",
       "      <td>ML</td>\n",
       "      <td>US50-E</td>\n",
       "      <td>3.0</td>\n",
       "      <td>El Dorado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.627</td>\n",
       "      <td>60.162</td>\n",
       "      <td>3.134</td>\n",
       "      <td>308511.0</td>\n",
       "      <td>Sly Park Rd</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Mainline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-10-01 03:00:00</td>\n",
       "      <td>308511</td>\n",
       "      <td>50</td>\n",
       "      <td>E</td>\n",
       "      <td>20.0</td>\n",
       "      <td>66.3</td>\n",
       "      <td>92</td>\n",
       "      <td>197</td>\n",
       "      <td>ML</td>\n",
       "      <td>US50-E</td>\n",
       "      <td>3.0</td>\n",
       "      <td>El Dorado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.627</td>\n",
       "      <td>60.162</td>\n",
       "      <td>3.134</td>\n",
       "      <td>308511.0</td>\n",
       "      <td>Sly Park Rd</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Mainline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-10-01 04:00:00</td>\n",
       "      <td>308511</td>\n",
       "      <td>50</td>\n",
       "      <td>E</td>\n",
       "      <td>55.0</td>\n",
       "      <td>67.4</td>\n",
       "      <td>100</td>\n",
       "      <td>197</td>\n",
       "      <td>ML</td>\n",
       "      <td>US50-E</td>\n",
       "      <td>3.0</td>\n",
       "      <td>El Dorado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.627</td>\n",
       "      <td>60.162</td>\n",
       "      <td>3.134</td>\n",
       "      <td>308511.0</td>\n",
       "      <td>Sly Park Rd</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Mainline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-10-01 05:00:00</td>\n",
       "      <td>308511</td>\n",
       "      <td>50</td>\n",
       "      <td>E</td>\n",
       "      <td>228.0</td>\n",
       "      <td>66.1</td>\n",
       "      <td>83</td>\n",
       "      <td>168</td>\n",
       "      <td>ML</td>\n",
       "      <td>US50-E</td>\n",
       "      <td>3.0</td>\n",
       "      <td>El Dorado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.627</td>\n",
       "      <td>60.162</td>\n",
       "      <td>3.134</td>\n",
       "      <td>308511.0</td>\n",
       "      <td>Sly Park Rd</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Mainline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp  station  route direction_of_travel  total_flow  avg_speed  pct_observed  samples lane_type     fwy  district     county city   ca_pm  abs_pm  length        id         name  \\\n",
       "0 2024-10-01 01:00:00   308511     50                   E        12.0       67.5           100      202        ML  US50-E       3.0  El Dorado  NaN  31.627  60.162   3.134  308511.0  Sly Park Rd   \n",
       "1 2024-10-01 02:00:00   308511     50                   E        12.0       67.0           100      197        ML  US50-E       3.0  El Dorado  NaN  31.627  60.162   3.134  308511.0  Sly Park Rd   \n",
       "2 2024-10-01 03:00:00   308511     50                   E        20.0       66.3            92      197        ML  US50-E       3.0  El Dorado  NaN  31.627  60.162   3.134  308511.0  Sly Park Rd   \n",
       "3 2024-10-01 04:00:00   308511     50                   E        55.0       67.4           100      197        ML  US50-E       3.0  El Dorado  NaN  31.627  60.162   3.134  308511.0  Sly Park Rd   \n",
       "4 2024-10-01 05:00:00   308511     50                   E       228.0       66.1            83      168        ML  US50-E       3.0  El Dorado  NaN  31.627  60.162   3.134  308511.0  Sly Park Rd   \n",
       "\n",
       "   lanes      type sensor_type hov  ms_id  irm  \n",
       "0    2.0  Mainline         NaN  No    1.0  NaN  \n",
       "1    2.0  Mainline         NaN  No    1.0  NaN  \n",
       "2    2.0  Mainline         NaN  No    1.0  NaN  \n",
       "3    2.0  Mainline         NaN  No    1.0  NaN  \n",
       "4    2.0  Mainline         NaN  No    1.0  NaN  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_columns(df):\n",
    "    df = df.copy()\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.strip()\n",
    "        .str.replace(\" \", \"_\")\n",
    "        .str.replace(\"-\", \"_\")\n",
    "        .str.replace(\"/\", \"_\")\n",
    "        .str.replace(\"%\", \"pct\")\n",
    "        .str.lower()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df = clean_columns(df)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11a4ce72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>station</th>\n",
       "      <th>route</th>\n",
       "      <th>direction_of_travel</th>\n",
       "      <th>total_flow</th>\n",
       "      <th>avg_speed</th>\n",
       "      <th>pct_observed</th>\n",
       "      <th>samples</th>\n",
       "      <th>lane_type</th>\n",
       "      <th>fwy</th>\n",
       "      <th>district</th>\n",
       "      <th>county</th>\n",
       "      <th>city</th>\n",
       "      <th>ca_pm</th>\n",
       "      <th>abs_pm</th>\n",
       "      <th>length</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>lanes</th>\n",
       "      <th>type</th>\n",
       "      <th>sensor_type</th>\n",
       "      <th>hov</th>\n",
       "      <th>ms_id</th>\n",
       "      <th>irm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-10-01 01:00:00</td>\n",
       "      <td>308511</td>\n",
       "      <td>50</td>\n",
       "      <td>E</td>\n",
       "      <td>12.0</td>\n",
       "      <td>67.5</td>\n",
       "      <td>100</td>\n",
       "      <td>202</td>\n",
       "      <td>ML</td>\n",
       "      <td>US50-E</td>\n",
       "      <td>3.0</td>\n",
       "      <td>El Dorado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.627</td>\n",
       "      <td>60.162</td>\n",
       "      <td>3.134</td>\n",
       "      <td>308511.0</td>\n",
       "      <td>Sly Park Rd</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Mainline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-01 02:00:00</td>\n",
       "      <td>308511</td>\n",
       "      <td>50</td>\n",
       "      <td>E</td>\n",
       "      <td>12.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>100</td>\n",
       "      <td>197</td>\n",
       "      <td>ML</td>\n",
       "      <td>US50-E</td>\n",
       "      <td>3.0</td>\n",
       "      <td>El Dorado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.627</td>\n",
       "      <td>60.162</td>\n",
       "      <td>3.134</td>\n",
       "      <td>308511.0</td>\n",
       "      <td>Sly Park Rd</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Mainline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-10-01 03:00:00</td>\n",
       "      <td>308511</td>\n",
       "      <td>50</td>\n",
       "      <td>E</td>\n",
       "      <td>20.0</td>\n",
       "      <td>66.3</td>\n",
       "      <td>92</td>\n",
       "      <td>197</td>\n",
       "      <td>ML</td>\n",
       "      <td>US50-E</td>\n",
       "      <td>3.0</td>\n",
       "      <td>El Dorado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.627</td>\n",
       "      <td>60.162</td>\n",
       "      <td>3.134</td>\n",
       "      <td>308511.0</td>\n",
       "      <td>Sly Park Rd</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Mainline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-10-01 04:00:00</td>\n",
       "      <td>308511</td>\n",
       "      <td>50</td>\n",
       "      <td>E</td>\n",
       "      <td>55.0</td>\n",
       "      <td>67.4</td>\n",
       "      <td>100</td>\n",
       "      <td>197</td>\n",
       "      <td>ML</td>\n",
       "      <td>US50-E</td>\n",
       "      <td>3.0</td>\n",
       "      <td>El Dorado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.627</td>\n",
       "      <td>60.162</td>\n",
       "      <td>3.134</td>\n",
       "      <td>308511.0</td>\n",
       "      <td>Sly Park Rd</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Mainline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-10-01 05:00:00</td>\n",
       "      <td>308511</td>\n",
       "      <td>50</td>\n",
       "      <td>E</td>\n",
       "      <td>228.0</td>\n",
       "      <td>66.1</td>\n",
       "      <td>83</td>\n",
       "      <td>168</td>\n",
       "      <td>ML</td>\n",
       "      <td>US50-E</td>\n",
       "      <td>3.0</td>\n",
       "      <td>El Dorado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.627</td>\n",
       "      <td>60.162</td>\n",
       "      <td>3.134</td>\n",
       "      <td>308511.0</td>\n",
       "      <td>Sly Park Rd</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Mainline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp  station  route direction_of_travel  total_flow  avg_speed  pct_observed  samples lane_type     fwy  district     county city   ca_pm  abs_pm  length        id         name  \\\n",
       "0 2024-10-01 01:00:00   308511     50                   E        12.0       67.5           100      202        ML  US50-E       3.0  El Dorado  NaN  31.627  60.162   3.134  308511.0  Sly Park Rd   \n",
       "1 2024-10-01 02:00:00   308511     50                   E        12.0       67.0           100      197        ML  US50-E       3.0  El Dorado  NaN  31.627  60.162   3.134  308511.0  Sly Park Rd   \n",
       "2 2024-10-01 03:00:00   308511     50                   E        20.0       66.3            92      197        ML  US50-E       3.0  El Dorado  NaN  31.627  60.162   3.134  308511.0  Sly Park Rd   \n",
       "3 2024-10-01 04:00:00   308511     50                   E        55.0       67.4           100      197        ML  US50-E       3.0  El Dorado  NaN  31.627  60.162   3.134  308511.0  Sly Park Rd   \n",
       "4 2024-10-01 05:00:00   308511     50                   E       228.0       66.1            83      168        ML  US50-E       3.0  El Dorado  NaN  31.627  60.162   3.134  308511.0  Sly Park Rd   \n",
       "\n",
       "   lanes      type sensor_type hov  ms_id  irm  \n",
       "0    2.0  Mainline         NaN  No    1.0  NaN  \n",
       "1    2.0  Mainline         NaN  No    1.0  NaN  \n",
       "2    2.0  Mainline         NaN  No    1.0  NaN  \n",
       "3    2.0  Mainline         NaN  No    1.0  NaN  \n",
       "4    2.0  Mainline         NaN  No    1.0  NaN  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c311e159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fwy         1.532537\n",
       "district    1.532537\n",
       "county      1.532537\n",
       "abs_pm      1.532537\n",
       "lanes       1.532537\n",
       "type        1.532537\n",
       "hov         1.532537\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_cols = ['fwy', 'district', 'county', 'abs_pm', 'lanes', 'type', 'hov']\n",
    "df[metadata_cols].isna().mean() * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60b65f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "required = ['fwy', 'abs_pm', 'lanes', 'type']\n",
    "df = df.dropna(subset=required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b81099c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>station</th>\n",
       "      <th>route</th>\n",
       "      <th>direction_of_travel</th>\n",
       "      <th>total_flow</th>\n",
       "      <th>avg_speed</th>\n",
       "      <th>pct_observed</th>\n",
       "      <th>samples</th>\n",
       "      <th>lane_type</th>\n",
       "      <th>fwy</th>\n",
       "      <th>district</th>\n",
       "      <th>county</th>\n",
       "      <th>city</th>\n",
       "      <th>ca_pm</th>\n",
       "      <th>abs_pm</th>\n",
       "      <th>length</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>lanes</th>\n",
       "      <th>type</th>\n",
       "      <th>sensor_type</th>\n",
       "      <th>hov</th>\n",
       "      <th>ms_id</th>\n",
       "      <th>irm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-10-01 01:00:00</td>\n",
       "      <td>308511</td>\n",
       "      <td>50</td>\n",
       "      <td>E</td>\n",
       "      <td>12.0</td>\n",
       "      <td>67.5</td>\n",
       "      <td>100</td>\n",
       "      <td>202</td>\n",
       "      <td>ML</td>\n",
       "      <td>US50-E</td>\n",
       "      <td>3.0</td>\n",
       "      <td>El Dorado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.627</td>\n",
       "      <td>60.162</td>\n",
       "      <td>3.134</td>\n",
       "      <td>308511.0</td>\n",
       "      <td>Sly Park Rd</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Mainline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-01 02:00:00</td>\n",
       "      <td>308511</td>\n",
       "      <td>50</td>\n",
       "      <td>E</td>\n",
       "      <td>12.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>100</td>\n",
       "      <td>197</td>\n",
       "      <td>ML</td>\n",
       "      <td>US50-E</td>\n",
       "      <td>3.0</td>\n",
       "      <td>El Dorado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.627</td>\n",
       "      <td>60.162</td>\n",
       "      <td>3.134</td>\n",
       "      <td>308511.0</td>\n",
       "      <td>Sly Park Rd</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Mainline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-10-01 03:00:00</td>\n",
       "      <td>308511</td>\n",
       "      <td>50</td>\n",
       "      <td>E</td>\n",
       "      <td>20.0</td>\n",
       "      <td>66.3</td>\n",
       "      <td>92</td>\n",
       "      <td>197</td>\n",
       "      <td>ML</td>\n",
       "      <td>US50-E</td>\n",
       "      <td>3.0</td>\n",
       "      <td>El Dorado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.627</td>\n",
       "      <td>60.162</td>\n",
       "      <td>3.134</td>\n",
       "      <td>308511.0</td>\n",
       "      <td>Sly Park Rd</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Mainline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-10-01 04:00:00</td>\n",
       "      <td>308511</td>\n",
       "      <td>50</td>\n",
       "      <td>E</td>\n",
       "      <td>55.0</td>\n",
       "      <td>67.4</td>\n",
       "      <td>100</td>\n",
       "      <td>197</td>\n",
       "      <td>ML</td>\n",
       "      <td>US50-E</td>\n",
       "      <td>3.0</td>\n",
       "      <td>El Dorado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.627</td>\n",
       "      <td>60.162</td>\n",
       "      <td>3.134</td>\n",
       "      <td>308511.0</td>\n",
       "      <td>Sly Park Rd</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Mainline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-10-01 05:00:00</td>\n",
       "      <td>308511</td>\n",
       "      <td>50</td>\n",
       "      <td>E</td>\n",
       "      <td>228.0</td>\n",
       "      <td>66.1</td>\n",
       "      <td>83</td>\n",
       "      <td>168</td>\n",
       "      <td>ML</td>\n",
       "      <td>US50-E</td>\n",
       "      <td>3.0</td>\n",
       "      <td>El Dorado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.627</td>\n",
       "      <td>60.162</td>\n",
       "      <td>3.134</td>\n",
       "      <td>308511.0</td>\n",
       "      <td>Sly Park Rd</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Mainline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp  station  route direction_of_travel  total_flow  avg_speed  pct_observed  samples lane_type     fwy  district     county city   ca_pm  abs_pm  length        id         name  \\\n",
       "0 2024-10-01 01:00:00   308511     50                   E        12.0       67.5           100      202        ML  US50-E       3.0  El Dorado  NaN  31.627  60.162   3.134  308511.0  Sly Park Rd   \n",
       "1 2024-10-01 02:00:00   308511     50                   E        12.0       67.0           100      197        ML  US50-E       3.0  El Dorado  NaN  31.627  60.162   3.134  308511.0  Sly Park Rd   \n",
       "2 2024-10-01 03:00:00   308511     50                   E        20.0       66.3            92      197        ML  US50-E       3.0  El Dorado  NaN  31.627  60.162   3.134  308511.0  Sly Park Rd   \n",
       "3 2024-10-01 04:00:00   308511     50                   E        55.0       67.4           100      197        ML  US50-E       3.0  El Dorado  NaN  31.627  60.162   3.134  308511.0  Sly Park Rd   \n",
       "4 2024-10-01 05:00:00   308511     50                   E       228.0       66.1            83      168        ML  US50-E       3.0  El Dorado  NaN  31.627  60.162   3.134  308511.0  Sly Park Rd   \n",
       "\n",
       "   lanes      type sensor_type hov  ms_id  irm  \n",
       "0    2.0  Mainline         NaN  No    1.0  NaN  \n",
       "1    2.0  Mainline         NaN  No    1.0  NaN  \n",
       "2    2.0  Mainline         NaN  No    1.0  NaN  \n",
       "3    2.0  Mainline         NaN  No    1.0  NaN  \n",
       "4    2.0  Mainline         NaN  No    1.0  NaN  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e277dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['total_flow']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a769bc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['hour'] = df['timestamp'].dt.hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "478f1499",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_end = pd.Timestamp(\"2024-11-15 23:59:59\")\n",
    "val_end = pd.Timestamp(\"2024-11-30 23:59:59\")\n",
    "\n",
    "df_train = df[df['timestamp'] <= train_end].copy()\n",
    "df_val   = df[(df['timestamp'] > train_end) & (df['timestamp'] <= val_end)].copy()\n",
    "df_test  = df[df['timestamp'] > val_end].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da95ab66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1876911, 25) (615305, 25) (1265940, 25)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape, df_val.shape, df_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "acae9114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1772"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['station'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dce82213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_22372\\2823676815.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  .groupby(['lane_type', 'type', 'hour', 'fwy', 'district'])['avg_speed']\n"
     ]
    }
   ],
   "source": [
    "speed_lookup = (\n",
    "    df_train\n",
    "    .dropna(subset=['avg_speed'])\n",
    "    .groupby(['lane_type', 'type', 'hour', 'fwy', 'district'])['avg_speed']\n",
    "    .mean()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60482c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_speed_mean = df_train['avg_speed'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7aa946fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_speed_no_leakage(df, lookup, global_mean):\n",
    "    df = df.sort_values(['station', 'timestamp']).copy()\n",
    "    \n",
    "    # 1. forward fill within each station\n",
    "    df['avg_speed'] = df.groupby('station')['avg_speed'].ffill()\n",
    "    \n",
    "    # 2. group-average fill\n",
    "    missing_mask = df['avg_speed'].isna()\n",
    "    if missing_mask.any():\n",
    "        df.loc[missing_mask, 'avg_speed'] = df[missing_mask].apply(\n",
    "            lambda row: lookup.get(\n",
    "                (row['lane_type'], row['type'], row['hour'], row['fwy'], row['district']),\n",
    "                np.nan\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    # final fallback: global mean\n",
    "    df['avg_speed'] = df['avg_speed'].fillna(global_mean)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "73d9f342",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = impute_speed_no_leakage(df_train, speed_lookup, global_speed_mean)\n",
    "df_val   = impute_speed_no_leakage(df_val, speed_lookup, global_speed_mean)\n",
    "df_test  = impute_speed_no_leakage(df_test, speed_lookup, global_speed_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8fbf801c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp                    0\n",
       "station                      0\n",
       "route                        0\n",
       "direction_of_travel          0\n",
       "total_flow                   0\n",
       "avg_speed                    0\n",
       "pct_observed                 0\n",
       "samples                      0\n",
       "lane_type                    0\n",
       "fwy                          0\n",
       "district                     0\n",
       "county                       0\n",
       "city                   1087940\n",
       "ca_pm                        0\n",
       "abs_pm                       0\n",
       "length                  613284\n",
       "id                           0\n",
       "name                         0\n",
       "lanes                        0\n",
       "type                         0\n",
       "sensor_type            1060791\n",
       "hov                          0\n",
       "ms_id                        0\n",
       "irm                    1876911\n",
       "hour                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "149a5ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp                    0\n",
       "station                      0\n",
       "route                        0\n",
       "direction_of_travel          0\n",
       "total_flow                   0\n",
       "avg_speed                    0\n",
       "pct_observed                 0\n",
       "samples                      0\n",
       "lane_type                    0\n",
       "fwy                          0\n",
       "district                     0\n",
       "county                       0\n",
       "city                    732485\n",
       "ca_pm                        0\n",
       "abs_pm                       0\n",
       "length                  421501\n",
       "id                           0\n",
       "name                         0\n",
       "lanes                        0\n",
       "type                         0\n",
       "sensor_type             709952\n",
       "hov                          0\n",
       "ms_id                        0\n",
       "irm                    1265940\n",
       "hour                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4330ddcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp                   0\n",
       "station                     0\n",
       "route                       0\n",
       "direction_of_travel         0\n",
       "total_flow                  0\n",
       "avg_speed                   0\n",
       "pct_observed                0\n",
       "samples                     0\n",
       "lane_type                   0\n",
       "fwy                         0\n",
       "district                    0\n",
       "county                      0\n",
       "city                   356413\n",
       "ca_pm                       0\n",
       "abs_pm                      0\n",
       "length                 206705\n",
       "id                          0\n",
       "name                        0\n",
       "lanes                       0\n",
       "type                        0\n",
       "sensor_type            344101\n",
       "hov                         0\n",
       "ms_id                       0\n",
       "irm                    615305\n",
       "hour                        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "16caeb72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>station</th>\n",
       "      <th>route</th>\n",
       "      <th>direction_of_travel</th>\n",
       "      <th>total_flow</th>\n",
       "      <th>avg_speed</th>\n",
       "      <th>pct_observed</th>\n",
       "      <th>samples</th>\n",
       "      <th>lane_type</th>\n",
       "      <th>fwy</th>\n",
       "      <th>district</th>\n",
       "      <th>county</th>\n",
       "      <th>city</th>\n",
       "      <th>ca_pm</th>\n",
       "      <th>abs_pm</th>\n",
       "      <th>length</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>lanes</th>\n",
       "      <th>type</th>\n",
       "      <th>sensor_type</th>\n",
       "      <th>hov</th>\n",
       "      <th>ms_id</th>\n",
       "      <th>irm</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-10-01 01:00:00</td>\n",
       "      <td>308511</td>\n",
       "      <td>50</td>\n",
       "      <td>E</td>\n",
       "      <td>12.0</td>\n",
       "      <td>67.5</td>\n",
       "      <td>100</td>\n",
       "      <td>202</td>\n",
       "      <td>ML</td>\n",
       "      <td>US50-E</td>\n",
       "      <td>3.0</td>\n",
       "      <td>El Dorado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.627</td>\n",
       "      <td>60.162</td>\n",
       "      <td>3.134</td>\n",
       "      <td>308511.0</td>\n",
       "      <td>Sly Park Rd</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Mainline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-01 02:00:00</td>\n",
       "      <td>308511</td>\n",
       "      <td>50</td>\n",
       "      <td>E</td>\n",
       "      <td>12.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>100</td>\n",
       "      <td>197</td>\n",
       "      <td>ML</td>\n",
       "      <td>US50-E</td>\n",
       "      <td>3.0</td>\n",
       "      <td>El Dorado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.627</td>\n",
       "      <td>60.162</td>\n",
       "      <td>3.134</td>\n",
       "      <td>308511.0</td>\n",
       "      <td>Sly Park Rd</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Mainline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-10-01 03:00:00</td>\n",
       "      <td>308511</td>\n",
       "      <td>50</td>\n",
       "      <td>E</td>\n",
       "      <td>20.0</td>\n",
       "      <td>66.3</td>\n",
       "      <td>92</td>\n",
       "      <td>197</td>\n",
       "      <td>ML</td>\n",
       "      <td>US50-E</td>\n",
       "      <td>3.0</td>\n",
       "      <td>El Dorado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.627</td>\n",
       "      <td>60.162</td>\n",
       "      <td>3.134</td>\n",
       "      <td>308511.0</td>\n",
       "      <td>Sly Park Rd</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Mainline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-10-01 04:00:00</td>\n",
       "      <td>308511</td>\n",
       "      <td>50</td>\n",
       "      <td>E</td>\n",
       "      <td>55.0</td>\n",
       "      <td>67.4</td>\n",
       "      <td>100</td>\n",
       "      <td>197</td>\n",
       "      <td>ML</td>\n",
       "      <td>US50-E</td>\n",
       "      <td>3.0</td>\n",
       "      <td>El Dorado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.627</td>\n",
       "      <td>60.162</td>\n",
       "      <td>3.134</td>\n",
       "      <td>308511.0</td>\n",
       "      <td>Sly Park Rd</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Mainline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-10-01 05:00:00</td>\n",
       "      <td>308511</td>\n",
       "      <td>50</td>\n",
       "      <td>E</td>\n",
       "      <td>228.0</td>\n",
       "      <td>66.1</td>\n",
       "      <td>83</td>\n",
       "      <td>168</td>\n",
       "      <td>ML</td>\n",
       "      <td>US50-E</td>\n",
       "      <td>3.0</td>\n",
       "      <td>El Dorado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.627</td>\n",
       "      <td>60.162</td>\n",
       "      <td>3.134</td>\n",
       "      <td>308511.0</td>\n",
       "      <td>Sly Park Rd</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Mainline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp  station  route direction_of_travel  total_flow  avg_speed  pct_observed  samples lane_type     fwy  district     county city   ca_pm  abs_pm  length        id         name  \\\n",
       "0 2024-10-01 01:00:00   308511     50                   E        12.0       67.5           100      202        ML  US50-E       3.0  El Dorado  NaN  31.627  60.162   3.134  308511.0  Sly Park Rd   \n",
       "1 2024-10-01 02:00:00   308511     50                   E        12.0       67.0           100      197        ML  US50-E       3.0  El Dorado  NaN  31.627  60.162   3.134  308511.0  Sly Park Rd   \n",
       "2 2024-10-01 03:00:00   308511     50                   E        20.0       66.3            92      197        ML  US50-E       3.0  El Dorado  NaN  31.627  60.162   3.134  308511.0  Sly Park Rd   \n",
       "3 2024-10-01 04:00:00   308511     50                   E        55.0       67.4           100      197        ML  US50-E       3.0  El Dorado  NaN  31.627  60.162   3.134  308511.0  Sly Park Rd   \n",
       "4 2024-10-01 05:00:00   308511     50                   E       228.0       66.1            83      168        ML  US50-E       3.0  El Dorado  NaN  31.627  60.162   3.134  308511.0  Sly Park Rd   \n",
       "\n",
       "   lanes      type sensor_type hov  ms_id  irm  hour  \n",
       "0    2.0  Mainline         NaN  No    1.0  NaN     1  \n",
       "1    2.0  Mainline         NaN  No    1.0  NaN     2  \n",
       "2    2.0  Mainline         NaN  No    1.0  NaN     3  \n",
       "3    2.0  Mainline         NaN  No    1.0  NaN     4  \n",
       "4    2.0  Mainline         NaN  No    1.0  NaN     5  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_columns(df):\n",
    "    df = df.copy()\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.strip()\n",
    "        .str.replace(\" \", \"_\")\n",
    "        .str.replace(\"-\", \"_\")\n",
    "        .str.replace(\"/\", \"_\")\n",
    "        .str.replace(\"%\", \"pct\")\n",
    "        .str.lower()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df = clean_columns(df)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80ba4c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fwy          0.0\n",
       "abs_pm       0.0\n",
       "lanes        0.0\n",
       "type         0.0\n",
       "lane_type    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_cols = [\"fwy\", \"abs_pm\", \"lanes\", \"type\", \"lane_type\"]\n",
    "df[meta_cols].isna().mean() * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113409cc",
   "metadata": {},
   "source": [
    "## Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bea42c",
   "metadata": {},
   "source": [
    "## Temporal Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "06df12c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lags(df, target_col, lags):\n",
    "    df = df.sort_values(['station', 'timestamp']).copy()\n",
    "    \n",
    "    for lag in lags:\n",
    "        df[f'{target_col}_lag_{lag}'] = (\n",
    "            df.groupby('station')[target_col].shift(lag)\n",
    "        )\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dd0867c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = [1, 2, 3, 6, 12, 24]\n",
    "\n",
    "df_train = create_lags(df_train, 'total_flow', lags)\n",
    "df_val   = create_lags(df_val,   'total_flow', lags)\n",
    "df_test  = create_lags(df_test,  'total_flow', lags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f086ac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.dropna(subset=[f'total_flow_lag_{l}' for l in lags])\n",
    "df_val   = df_val.dropna(subset=[f'total_flow_lag_{l}' for l in lags])\n",
    "df_test  = df_test.dropna(subset=[f'total_flow_lag_{l}' for l in lags])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23e45226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rolling_features(df):\n",
    "    df = df.sort_values(['station', 'timestamp']).copy()\n",
    "\n",
    "    # rolling over 24 hours = 24 time steps (hourly data)\n",
    "    window = 24\n",
    "    \n",
    "    df['rolling_mean_24h'] = (\n",
    "        df.groupby('station')['total_flow']\n",
    "          .transform(lambda x: x.rolling(window, min_periods=12).mean())\n",
    "    )\n",
    "    \n",
    "    df['rolling_std_24h'] = (\n",
    "        df.groupby('station')['total_flow']\n",
    "          .transform(lambda x: x.rolling(window, min_periods=12).std())\n",
    "    )\n",
    "\n",
    "    df['rolling_min_24h'] = (\n",
    "        df.groupby('station')['total_flow']\n",
    "          .transform(lambda x: x.rolling(window, min_periods=12).min())\n",
    "    )\n",
    "    \n",
    "    df['rolling_max_24h'] = (\n",
    "        df.groupby('station')['total_flow']\n",
    "          .transform(lambda x: x.rolling(window, min_periods=12).max())\n",
    "    )\n",
    "\n",
    "    # coefficient of variation\n",
    "    df['rolling_cv_24h'] = df['rolling_std_24h'] / (df['rolling_mean_24h'] + 1e-4)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1fef2796",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = create_rolling_features(df_train)\n",
    "df_val   = create_rolling_features(df_val)\n",
    "df_test  = create_rolling_features(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b1c76a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_cols = [\n",
    "    'rolling_mean_24h', 'rolling_std_24h', 'rolling_min_24h',\n",
    "    'rolling_max_24h', 'rolling_cv_24h'\n",
    "]\n",
    "\n",
    "df_train = df_train.dropna(subset=rolling_cols)\n",
    "df_val   = df_val.dropna(subset=rolling_cols)\n",
    "df_test  = df_test.dropna(subset=rolling_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6cdc82eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['dayofweek'] = df['timestamp'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "40b472f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = add_time_features(df_train)\n",
    "df_val   = add_time_features(df_val)\n",
    "df_test  = add_time_features(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7604415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cyclical_time(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    \n",
    "    df['dow_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)\n",
    "    df['dow_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b8df388",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = add_cyclical_time(df_train)\n",
    "df_val   = add_cyclical_time(df_val)\n",
    "df_test  = add_cyclical_time(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9febe788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_peak_flags(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['is_peak_hour'] = df['hour'].isin([7, 8, 9, 16, 17, 18]).astype(int)\n",
    "    df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "535e3aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = add_peak_flags(df_train)\n",
    "df_val   = add_peak_flags(df_val)\n",
    "df_test  = add_peak_flags(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "012fcef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timestamp', 'station', 'route', 'direction_of_travel', 'total_flow', 'avg_speed', 'pct_observed', 'samples', 'lane_type', 'fwy', 'district', 'county', 'city', 'ca_pm', 'abs_pm', 'length',\n",
       "       'id', 'name', 'lanes', 'type', 'sensor_type', 'hov', 'ms_id', 'irm', 'hour', 'total_flow_lag_1', 'total_flow_lag_2', 'total_flow_lag_3', 'total_flow_lag_6', 'total_flow_lag_12',\n",
       "       'total_flow_lag_24', 'rolling_mean_24h', 'rolling_std_24h', 'rolling_min_24h', 'rolling_max_24h', 'rolling_cv_24h', 'dayofweek', 'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos', 'is_peak_hour',\n",
       "       'is_weekend'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbe9e1a",
   "metadata": {},
   "source": [
    "## Spatial Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c8c4f1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_station_order(df):\n",
    "    df = df.copy()\n",
    "    df['station_order'] = (\n",
    "        df.sort_values(['fwy', 'abs_pm'])\n",
    "          .groupby('fwy')\n",
    "          .cumcount()\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "408c30c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = add_station_order(df_train)\n",
    "df_val   = add_station_order(df_val)\n",
    "df_test  = add_station_order(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dd3d8179",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_sorted = (\n",
    "    df_train[['station', 'fwy', 'abs_pm']]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(['fwy', 'abs_pm'])\n",
    ")\n",
    "\n",
    "stations_sorted['upstream_station'] = stations_sorted.groupby('fwy')['station'].shift(1)\n",
    "stations_sorted['downstream_station'] = stations_sorted.groupby('fwy')['station'].shift(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "03cbb95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.merge(stations_sorted[['station','upstream_station','downstream_station']],\n",
    "                          on='station', how='left')\n",
    "\n",
    "df_val   = df_val.merge(stations_sorted[['station','upstream_station','downstream_station']],\n",
    "                        on='station', how='left')\n",
    "\n",
    "df_test  = df_test.merge(stations_sorted[['station','upstream_station','downstream_station']],\n",
    "                         on='station', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0a65b909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_neighbor_lags(df, neighbor_col, lag):\n",
    "    df = df.sort_values(['station', 'timestamp']).copy()\n",
    "    \n",
    "    # Map from station→lagged flow\n",
    "    mapping = (\n",
    "        df[['station', 'timestamp', f'total_flow_lag_{lag}']]\n",
    "        .rename(columns={'station': 'neighbor_station',\n",
    "                         f'total_flow_lag_{lag}': f'{neighbor_col}_flow_lag_{lag}'})\n",
    "    )\n",
    "    \n",
    "    df = df.merge(mapping, \n",
    "                  left_on=['timestamp', neighbor_col], \n",
    "                  right_on=['timestamp', 'neighbor_station'], \n",
    "                  how='left')\n",
    "    \n",
    "    df = df.drop(columns=['neighbor_station'])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6e1fd896",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = add_neighbor_lags(df_train, 'upstream_station', 1)\n",
    "df_train = add_neighbor_lags(df_train, 'downstream_station', 1)\n",
    "\n",
    "df_val   = add_neighbor_lags(df_val, 'upstream_station', 1)\n",
    "df_val   = add_neighbor_lags(df_val, 'downstream_station', 1)\n",
    "\n",
    "df_test  = add_neighbor_lags(df_test, 'upstream_station', 1)\n",
    "df_test  = add_neighbor_lags(df_test, 'downstream_station', 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b855b64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_pm(df):\n",
    "    df = df.copy()\n",
    "    df['abs_pm_norm'] = (\n",
    "        df.groupby('fwy')['abs_pm']\n",
    "          .transform(lambda x: (x - x.min()) / (x.max() - x.min() + 1e-6))\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8fec3c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = normalize_pm(df_train)\n",
    "df_val   = normalize_pm(df_val)\n",
    "df_test  = normalize_pm(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "50fec5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>station</th>\n",
       "      <th>route</th>\n",
       "      <th>direction_of_travel</th>\n",
       "      <th>total_flow</th>\n",
       "      <th>avg_speed</th>\n",
       "      <th>pct_observed</th>\n",
       "      <th>samples</th>\n",
       "      <th>lane_type</th>\n",
       "      <th>fwy</th>\n",
       "      <th>district</th>\n",
       "      <th>county</th>\n",
       "      <th>city</th>\n",
       "      <th>ca_pm</th>\n",
       "      <th>abs_pm</th>\n",
       "      <th>length</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>lanes</th>\n",
       "      <th>type</th>\n",
       "      <th>sensor_type</th>\n",
       "      <th>hov</th>\n",
       "      <th>ms_id</th>\n",
       "      <th>irm</th>\n",
       "      <th>hour</th>\n",
       "      <th>total_flow_lag_1</th>\n",
       "      <th>total_flow_lag_2</th>\n",
       "      <th>total_flow_lag_3</th>\n",
       "      <th>total_flow_lag_6</th>\n",
       "      <th>total_flow_lag_12</th>\n",
       "      <th>total_flow_lag_24</th>\n",
       "      <th>rolling_mean_24h</th>\n",
       "      <th>rolling_std_24h</th>\n",
       "      <th>rolling_min_24h</th>\n",
       "      <th>rolling_max_24h</th>\n",
       "      <th>rolling_cv_24h</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>dow_sin</th>\n",
       "      <th>dow_cos</th>\n",
       "      <th>is_peak_hour</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>station_order</th>\n",
       "      <th>upstream_station</th>\n",
       "      <th>downstream_station</th>\n",
       "      <th>upstream_station_flow_lag_1</th>\n",
       "      <th>downstream_station_flow_lag_1</th>\n",
       "      <th>abs_pm_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-10-02 12:00:00</td>\n",
       "      <td>308511</td>\n",
       "      <td>50</td>\n",
       "      <td>E</td>\n",
       "      <td>572.0</td>\n",
       "      <td>65.5</td>\n",
       "      <td>46</td>\n",
       "      <td>204</td>\n",
       "      <td>ML</td>\n",
       "      <td>US50-E</td>\n",
       "      <td>3.0</td>\n",
       "      <td>El Dorado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.627</td>\n",
       "      <td>60.162</td>\n",
       "      <td>3.134</td>\n",
       "      <td>308511.0</td>\n",
       "      <td>Sly Park Rd</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Mainline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>628.0</td>\n",
       "      <td>596.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>395.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>308.0</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>173.193008</td>\n",
       "      <td>168.0</td>\n",
       "      <td>628.0</td>\n",
       "      <td>0.434068</td>\n",
       "      <td>2</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>217131</td>\n",
       "      <td>3086071.0</td>\n",
       "      <td>3086081.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>0.554877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-02 13:00:00</td>\n",
       "      <td>308511</td>\n",
       "      <td>50</td>\n",
       "      <td>E</td>\n",
       "      <td>580.0</td>\n",
       "      <td>65.5</td>\n",
       "      <td>46</td>\n",
       "      <td>191</td>\n",
       "      <td>ML</td>\n",
       "      <td>US50-E</td>\n",
       "      <td>3.0</td>\n",
       "      <td>El Dorado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.627</td>\n",
       "      <td>60.162</td>\n",
       "      <td>3.134</td>\n",
       "      <td>308511.0</td>\n",
       "      <td>Sly Park Rd</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Mainline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>572.0</td>\n",
       "      <td>628.0</td>\n",
       "      <td>596.0</td>\n",
       "      <td>486.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>412.923077</td>\n",
       "      <td>173.251966</td>\n",
       "      <td>168.0</td>\n",
       "      <td>628.0</td>\n",
       "      <td>0.419574</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.588190e-01</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>217132</td>\n",
       "      <td>3086071.0</td>\n",
       "      <td>3086081.0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>267.0</td>\n",
       "      <td>0.554877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-10-02 14:00:00</td>\n",
       "      <td>308511</td>\n",
       "      <td>50</td>\n",
       "      <td>E</td>\n",
       "      <td>572.0</td>\n",
       "      <td>65.4</td>\n",
       "      <td>50</td>\n",
       "      <td>209</td>\n",
       "      <td>ML</td>\n",
       "      <td>US50-E</td>\n",
       "      <td>3.0</td>\n",
       "      <td>El Dorado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.627</td>\n",
       "      <td>60.162</td>\n",
       "      <td>3.134</td>\n",
       "      <td>308511.0</td>\n",
       "      <td>Sly Park Rd</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Mainline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14</td>\n",
       "      <td>580.0</td>\n",
       "      <td>572.0</td>\n",
       "      <td>628.0</td>\n",
       "      <td>461.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>424.285714</td>\n",
       "      <td>171.798822</td>\n",
       "      <td>168.0</td>\n",
       "      <td>628.0</td>\n",
       "      <td>0.404913</td>\n",
       "      <td>2</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>217133</td>\n",
       "      <td>3086071.0</td>\n",
       "      <td>3086081.0</td>\n",
       "      <td>454.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>0.554877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-10-02 15:00:00</td>\n",
       "      <td>308511</td>\n",
       "      <td>50</td>\n",
       "      <td>E</td>\n",
       "      <td>605.0</td>\n",
       "      <td>65.7</td>\n",
       "      <td>46</td>\n",
       "      <td>192</td>\n",
       "      <td>ML</td>\n",
       "      <td>US50-E</td>\n",
       "      <td>3.0</td>\n",
       "      <td>El Dorado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.627</td>\n",
       "      <td>60.162</td>\n",
       "      <td>3.134</td>\n",
       "      <td>308511.0</td>\n",
       "      <td>Sly Park Rd</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Mainline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>572.0</td>\n",
       "      <td>580.0</td>\n",
       "      <td>572.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>436.333333</td>\n",
       "      <td>171.999446</td>\n",
       "      <td>168.0</td>\n",
       "      <td>628.0</td>\n",
       "      <td>0.394193</td>\n",
       "      <td>2</td>\n",
       "      <td>-7.071068e-01</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>217134</td>\n",
       "      <td>3086071.0</td>\n",
       "      <td>3086081.0</td>\n",
       "      <td>497.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>0.554877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-10-02 16:00:00</td>\n",
       "      <td>308511</td>\n",
       "      <td>50</td>\n",
       "      <td>E</td>\n",
       "      <td>539.0</td>\n",
       "      <td>65.8</td>\n",
       "      <td>50</td>\n",
       "      <td>194</td>\n",
       "      <td>ML</td>\n",
       "      <td>US50-E</td>\n",
       "      <td>3.0</td>\n",
       "      <td>El Dorado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.627</td>\n",
       "      <td>60.162</td>\n",
       "      <td>3.134</td>\n",
       "      <td>308511.0</td>\n",
       "      <td>Sly Park Rd</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Mainline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>605.0</td>\n",
       "      <td>572.0</td>\n",
       "      <td>580.0</td>\n",
       "      <td>596.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>442.750000</td>\n",
       "      <td>168.137840</td>\n",
       "      <td>168.0</td>\n",
       "      <td>628.0</td>\n",
       "      <td>0.379758</td>\n",
       "      <td>2</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>217135</td>\n",
       "      <td>3086071.0</td>\n",
       "      <td>3086081.0</td>\n",
       "      <td>657.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>0.554877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp  station  route direction_of_travel  total_flow  avg_speed  pct_observed  samples lane_type     fwy  district     county city   ca_pm  abs_pm  length        id         name  \\\n",
       "0 2024-10-02 12:00:00   308511     50                   E       572.0       65.5            46      204        ML  US50-E       3.0  El Dorado  NaN  31.627  60.162   3.134  308511.0  Sly Park Rd   \n",
       "1 2024-10-02 13:00:00   308511     50                   E       580.0       65.5            46      191        ML  US50-E       3.0  El Dorado  NaN  31.627  60.162   3.134  308511.0  Sly Park Rd   \n",
       "2 2024-10-02 14:00:00   308511     50                   E       572.0       65.4            50      209        ML  US50-E       3.0  El Dorado  NaN  31.627  60.162   3.134  308511.0  Sly Park Rd   \n",
       "3 2024-10-02 15:00:00   308511     50                   E       605.0       65.7            46      192        ML  US50-E       3.0  El Dorado  NaN  31.627  60.162   3.134  308511.0  Sly Park Rd   \n",
       "4 2024-10-02 16:00:00   308511     50                   E       539.0       65.8            50      194        ML  US50-E       3.0  El Dorado  NaN  31.627  60.162   3.134  308511.0  Sly Park Rd   \n",
       "\n",
       "   lanes      type sensor_type hov  ms_id  irm  hour  total_flow_lag_1  total_flow_lag_2  total_flow_lag_3  total_flow_lag_6  total_flow_lag_12  total_flow_lag_24  rolling_mean_24h  rolling_std_24h  \\\n",
       "0    2.0  Mainline         NaN  No    1.0  NaN    12             628.0             596.0             531.0             395.0              183.0              308.0        399.000000       173.193008   \n",
       "1    2.0  Mainline         NaN  No    1.0  NaN    13             572.0             628.0             596.0             486.0              181.0              320.0        412.923077       173.251966   \n",
       "2    2.0  Mainline         NaN  No    1.0  NaN    14             580.0             572.0             628.0             461.0              168.0              357.0        424.285714       171.798822   \n",
       "3    2.0  Mainline         NaN  No    1.0  NaN    15             572.0             580.0             572.0             531.0              178.0              244.0        436.333333       171.999446   \n",
       "4    2.0  Mainline         NaN  No    1.0  NaN    16             605.0             572.0             580.0             596.0              229.0              196.0        442.750000       168.137840   \n",
       "\n",
       "   rolling_min_24h  rolling_max_24h  rolling_cv_24h  dayofweek      hour_sin  hour_cos   dow_sin   dow_cos  is_peak_hour  is_weekend  station_order  upstream_station  downstream_station  \\\n",
       "0            168.0            628.0        0.434068          2  1.224647e-16 -1.000000  0.974928 -0.222521             0           0         217131         3086071.0           3086081.0   \n",
       "1            168.0            628.0        0.419574          2 -2.588190e-01 -0.965926  0.974928 -0.222521             0           0         217132         3086071.0           3086081.0   \n",
       "2            168.0            628.0        0.404913          2 -5.000000e-01 -0.866025  0.974928 -0.222521             0           0         217133         3086071.0           3086081.0   \n",
       "3            168.0            628.0        0.394193          2 -7.071068e-01 -0.707107  0.974928 -0.222521             0           0         217134         3086071.0           3086081.0   \n",
       "4            168.0            628.0        0.379758          2 -8.660254e-01 -0.500000  0.974928 -0.222521             1           0         217135         3086071.0           3086081.0   \n",
       "\n",
       "   upstream_station_flow_lag_1  downstream_station_flow_lag_1  abs_pm_norm  \n",
       "0                        504.0                          334.0     0.554877  \n",
       "1                        470.0                          267.0     0.554877  \n",
       "2                        454.0                          332.0     0.554877  \n",
       "3                        497.0                          248.0     0.554877  \n",
       "4                        657.0                          251.0     0.554877  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c7851453",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"total_flow\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "08bccbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    # Temporal numeric\n",
    "    'avg_speed', 'hour', 'dayofweek',\n",
    "    'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos',\n",
    "    'is_peak_hour', 'is_weekend',\n",
    "\n",
    "    # Lags\n",
    "    'total_flow_lag_1', 'total_flow_lag_2', 'total_flow_lag_3',\n",
    "    'total_flow_lag_6', 'total_flow_lag_12', 'total_flow_lag_24',\n",
    "\n",
    "    # Rolling stats\n",
    "    'rolling_mean_24h', 'rolling_std_24h',\n",
    "    'rolling_min_24h', 'rolling_max_24h', 'rolling_cv_24h',\n",
    "\n",
    "    # Spatial features\n",
    "    'station_order', 'abs_pm_norm',\n",
    "\n",
    "    # Spatial neighbor lag features\n",
    "    'upstream_station_flow_lag_1',\n",
    "    'downstream_station_flow_lag_1',\n",
    "\n",
    "    # Metadata\n",
    "    'route', 'lanes',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "25ad3fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\n",
    "    'lane_type', 'type', 'direction_of_travel', \n",
    "    'hov', 'county', 'fwy', 'district'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "30a75afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_encoded = pd.get_dummies(df_train, columns=cat_cols, drop_first=False)\n",
    "df_val_encoded   = pd.get_dummies(df_val,   columns=cat_cols, drop_first=False)\n",
    "df_test_encoded  = pd.get_dummies(df_test,  columns=cat_cols, drop_first=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "95de1689",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_encoded = df_val_encoded.reindex(columns=df_train_encoded.columns, fill_value=0)\n",
    "df_test_encoded = df_test_encoded.reindex(columns=df_train_encoded.columns, fill_value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8b65fc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\n",
    "    'timestamp', 'station', 'id',\n",
    "    'upstream_station', 'downstream_station'\n",
    "]\n",
    "\n",
    "df_train_encoded = df_train_encoded.drop(columns=drop_cols, errors='ignore')\n",
    "df_val_encoded   = df_val_encoded.drop(columns=drop_cols, errors='ignore')\n",
    "df_test_encoded  = df_test_encoded.drop(columns=drop_cols, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d0e81b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train_encoded.drop(columns=[target_col])\n",
    "y_train = df_train_encoded[target_col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "529ce7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = df_val_encoded.drop(columns=[target_col])\n",
    "y_val = df_val_encoded[target_col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d510373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test_encoded.drop(columns=[target_col])\n",
    "y_test = df_test_encoded[target_col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9e8f5285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (1815062, 106)\n",
      "X_val: (555000, 106)\n",
      "X_test: (1205541, 106)\n",
      "y_train: (1815062,)\n",
      "y_val: (555000,)\n",
      "y_test: (1205541,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_val:\", X_val.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"y_val:\", y_val.shape)\n",
    "print(\"y_test:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4176d70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric columns used for scaling:\n",
      "Index(['route', 'avg_speed', 'pct_observed', 'samples', 'abs_pm', 'length', 'lanes', 'ms_id', 'irm', 'hour', 'total_flow_lag_1', 'total_flow_lag_2', 'total_flow_lag_3', 'total_flow_lag_6',\n",
      "       'total_flow_lag_12', 'total_flow_lag_24', 'rolling_mean_24h', 'rolling_std_24h', 'rolling_min_24h', 'rolling_max_24h', 'rolling_cv_24h', 'dayofweek', 'hour_sin', 'hour_cos', 'dow_sin',\n",
      "       'dow_cos', 'is_peak_hour', 'is_weekend', 'station_order', 'upstream_station_flow_lag_1', 'downstream_station_flow_lag_1', 'abs_pm_norm'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\attafuro\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\extmath.py:1144: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "C:\\Users\\attafuro\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\extmath.py:1149: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "C:\\Users\\attafuro\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\extmath.py:1169: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    }
   ],
   "source": [
    "# Drop the columns you don't want\n",
    "cols_to_drop = [\"ca_pm\", \"city\", \"name\"]\n",
    "X_train = X_train.drop(columns=cols_to_drop)\n",
    "X_val   = X_val.drop(columns=cols_to_drop)\n",
    "X_test  = X_test.drop(columns=cols_to_drop)\n",
    "\n",
    "# Keep only numeric columns for scaling\n",
    "numeric_cols = X_train.select_dtypes(include=[\"number\"]).columns\n",
    "\n",
    "X_train_num = X_train[numeric_cols]\n",
    "X_val_num   = X_val[numeric_cols]\n",
    "X_test_num  = X_test[numeric_cols]\n",
    "\n",
    "print(\"Numeric columns used for scaling:\")\n",
    "print(numeric_cols)\n",
    "\n",
    "#Scale only numeric features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_num)\n",
    "X_val_scaled   = scaler.transform(X_val_num)\n",
    "X_test_scaled  = scaler.transform(X_test_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc11453",
   "metadata": {},
   "source": [
    "## Linear Regression Model (Elastic Net MIMO baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5268625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "HORIZONS = [12, 24, 48, 72]\n",
    "TARGET_COLS = [f\"y_{h}\" for h in HORIZONS]\n",
    "\n",
    "def add_mimo_targets(df, target=\"total_flow\"):\n",
    "    df = df.sort_values([\"station\", \"timestamp\"]).copy()\n",
    "    for h in HORIZONS:\n",
    "        df[f\"y_{h}\"] = df.groupby(\"station\")[target].shift(-h)\n",
    "    return df\n",
    "\n",
    "df_train_m = add_mimo_targets(df_train)\n",
    "df_val_m   = add_mimo_targets(df_val)\n",
    "df_test_m  = add_mimo_targets(df_test)\n",
    "\n",
    "# Drop rows where any horizon is missing\n",
    "df_train_m = df_train_m.dropna(subset=TARGET_COLS)\n",
    "df_val_m   = df_val_m.dropna(subset=TARGET_COLS)\n",
    "df_test_m  = df_test_m.dropna(subset=TARGET_COLS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "be2fd630",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\n",
    "    \"lane_type\", \"type\", \"direction_of_travel\",\n",
    "    \"hov\", \"county\", \"fwy\", \"district\"\n",
    "]\n",
    "\n",
    "df_train_enc = pd.get_dummies(df_train_m, columns=cat_cols, drop_first=False)\n",
    "df_val_enc   = pd.get_dummies(df_val_m,   columns=cat_cols, drop_first=False)\n",
    "df_test_enc  = pd.get_dummies(df_test_m,  columns=cat_cols, drop_first=False)\n",
    "\n",
    "# Align columns: val/test → same columns as train\n",
    "df_val_enc  = df_val_enc.reindex(columns=df_train_enc.columns, fill_value=0)\n",
    "df_test_enc = df_test_enc.reindex(columns=df_train_enc.columns, fill_value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1c228999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (1688253, 99)\n",
      "X_val:   (430974, 99)\n",
      "X_test:  (1081400, 99)\n",
      "y_train: (1688253, 4)\n",
      "y_val:   (430974, 4)\n",
      "y_test:  (1081400, 4)\n"
     ]
    }
   ],
   "source": [
    "# Columns we do NOT want as features\n",
    "NON_FEATURE_COLS = [\n",
    "    \"timestamp\", \"station\", \"id\",\n",
    "    \"upstream_station\", \"downstream_station\",\n",
    "    \"total_flow\",        # current target, not a predictor\n",
    "] + TARGET_COLS         # these are our y's, not X\n",
    "\n",
    "NON_FEATURE_COLS = [c for c in NON_FEATURE_COLS if c in df_train_enc.columns]\n",
    "\n",
    "X_train = df_train_enc.drop(columns=NON_FEATURE_COLS)\n",
    "X_val   = df_val_enc.drop(columns=NON_FEATURE_COLS)\n",
    "X_test  = df_test_enc.drop(columns=NON_FEATURE_COLS)\n",
    "\n",
    "y_train = df_train_enc[TARGET_COLS].values   # shape (n_samples, 4)\n",
    "y_val   = df_val_enc[TARGET_COLS].values\n",
    "y_test  = df_test_enc[TARGET_COLS].values\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_val:  \", X_val.shape)\n",
    "print(\"X_test: \", X_test.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"y_val:  \", y_val.shape)\n",
    "print(\"y_test: \", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a78048bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with NaNs in X_train: ['upstream_station_flow_lag_1', 'downstream_station_flow_lag_1']\n",
      "Dropping from train: 151190\n",
      "Dropping from val:   31190\n",
      "Dropping from test:  87582\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nan_cols = [c for c in X_train.columns if X_train[c].isna().any()]\n",
    "print(\"Columns with NaNs in X_train:\", nan_cols)\n",
    "\n",
    "# Build masks for rows that are clean (no NaNs in those columns)\n",
    "train_mask = ~X_train[nan_cols].isna().any(axis=1)\n",
    "val_mask   = ~X_val[nan_cols].isna().any(axis=1)\n",
    "test_mask  = ~X_test[nan_cols].isna().any(axis=1)\n",
    "\n",
    "print(\"Dropping from train:\", (~train_mask).sum())\n",
    "print(\"Dropping from val:  \", (~val_mask).sum())\n",
    "print(\"Dropping from test: \", (~test_mask).sum())\n",
    "\n",
    "# Keep only rows with complete features\n",
    "X_train = X_train[train_mask]\n",
    "y_train = y_train[train_mask]\n",
    "\n",
    "X_val   = X_val[val_mask]\n",
    "y_val   = y_val[val_mask]\n",
    "\n",
    "X_test  = X_test[test_mask]\n",
    "y_test  = y_test[test_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5761d1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_val_s   = scaler.transform(X_val)\n",
    "X_test_s  = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a6465d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best config (alpha, l1_ratio): (0.01, 0.9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>l1_ratio</th>\n",
       "      <th>val_rmse_mean</th>\n",
       "      <th>val_mae_12</th>\n",
       "      <th>val_mae_24</th>\n",
       "      <th>val_mae_48</th>\n",
       "      <th>val_mae_72</th>\n",
       "      <th>val_rmse_12</th>\n",
       "      <th>val_rmse_24</th>\n",
       "      <th>val_rmse_48</th>\n",
       "      <th>val_rmse_72</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.9</td>\n",
       "      <td>375.962396</td>\n",
       "      <td>167.283193</td>\n",
       "      <td>196.127142</td>\n",
       "      <td>237.520304</td>\n",
       "      <td>241.767428</td>\n",
       "      <td>344.265177</td>\n",
       "      <td>353.783271</td>\n",
       "      <td>406.179786</td>\n",
       "      <td>399.621350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>376.413373</td>\n",
       "      <td>169.091868</td>\n",
       "      <td>197.100274</td>\n",
       "      <td>238.219815</td>\n",
       "      <td>242.027400</td>\n",
       "      <td>343.874304</td>\n",
       "      <td>355.537014</td>\n",
       "      <td>406.745124</td>\n",
       "      <td>399.497051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>376.938354</td>\n",
       "      <td>170.999468</td>\n",
       "      <td>198.249672</td>\n",
       "      <td>238.853579</td>\n",
       "      <td>242.390376</td>\n",
       "      <td>343.840061</td>\n",
       "      <td>357.252835</td>\n",
       "      <td>407.148724</td>\n",
       "      <td>399.511797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.9</td>\n",
       "      <td>377.087988</td>\n",
       "      <td>171.423809</td>\n",
       "      <td>198.586852</td>\n",
       "      <td>238.990496</td>\n",
       "      <td>242.450820</td>\n",
       "      <td>343.864822</td>\n",
       "      <td>357.731468</td>\n",
       "      <td>407.237263</td>\n",
       "      <td>399.518396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>381.772471</td>\n",
       "      <td>190.210511</td>\n",
       "      <td>208.482754</td>\n",
       "      <td>244.177641</td>\n",
       "      <td>246.459388</td>\n",
       "      <td>350.665178</td>\n",
       "      <td>367.482027</td>\n",
       "      <td>408.831227</td>\n",
       "      <td>400.111451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>387.078708</td>\n",
       "      <td>205.757763</td>\n",
       "      <td>216.929586</td>\n",
       "      <td>249.170637</td>\n",
       "      <td>250.857471</td>\n",
       "      <td>361.155021</td>\n",
       "      <td>373.984071</td>\n",
       "      <td>411.049947</td>\n",
       "      <td>402.125792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.9</td>\n",
       "      <td>388.422510</td>\n",
       "      <td>208.985002</td>\n",
       "      <td>218.812639</td>\n",
       "      <td>250.008252</td>\n",
       "      <td>251.593134</td>\n",
       "      <td>364.004852</td>\n",
       "      <td>375.538217</td>\n",
       "      <td>411.563087</td>\n",
       "      <td>402.583886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>439.791949</td>\n",
       "      <td>294.045399</td>\n",
       "      <td>265.142417</td>\n",
       "      <td>283.645773</td>\n",
       "      <td>282.382751</td>\n",
       "      <td>460.442616</td>\n",
       "      <td>421.312660</td>\n",
       "      <td>443.956291</td>\n",
       "      <td>433.456228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>479.377674</td>\n",
       "      <td>340.575086</td>\n",
       "      <td>291.373391</td>\n",
       "      <td>305.599596</td>\n",
       "      <td>302.784001</td>\n",
       "      <td>526.132005</td>\n",
       "      <td>455.949125</td>\n",
       "      <td>473.524885</td>\n",
       "      <td>461.904682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alpha  l1_ratio  val_rmse_mean  val_mae_12  val_mae_24  val_mae_48  val_mae_72  val_rmse_12  val_rmse_24  val_rmse_48  val_rmse_72\n",
       "2   0.01       0.9     375.962396  167.283193  196.127142  237.520304  241.767428   344.265177   353.783271   406.179786   399.621350\n",
       "1   0.01       0.5     376.413373  169.091868  197.100274  238.219815  242.027400   343.874304   355.537014   406.745124   399.497051\n",
       "0   0.01       0.1     376.938354  170.999468  198.249672  238.853579  242.390376   343.840061   357.252835   407.148724   399.511797\n",
       "5   0.10       0.9     377.087988  171.423809  198.586852  238.990496  242.450820   343.864822   357.731468   407.237263   399.518396\n",
       "4   0.10       0.5     381.772471  190.210511  208.482754  244.177641  246.459388   350.665178   367.482027   408.831227   400.111451\n",
       "3   0.10       0.1     387.078708  205.757763  216.929586  249.170637  250.857471   361.155021   373.984071   411.049947   402.125792\n",
       "8   1.00       0.9     388.422510  208.985002  218.812639  250.008252  251.593134   364.004852   375.538217   411.563087   402.583886\n",
       "7   1.00       0.5     439.791949  294.045399  265.142417  283.645773  282.382751   460.442616   421.312660   443.956291   433.456228\n",
       "6   1.00       0.1     479.377674  340.575086  291.373391  305.599596  302.784001   526.132005   455.949125   473.524885   461.904682"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import MultiTaskElasticNet\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "alphas = [0.01, 0.1, 1.0]       # small but reasonable grid\n",
    "l1_ratios = [0.1, 0.5, 0.9]\n",
    "\n",
    "best_cfg = None\n",
    "best_val_rmse_mean = np.inf\n",
    "results = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    for l1 in l1_ratios:\n",
    "        model = MultiTaskElasticNet(\n",
    "            alpha=alpha,\n",
    "            l1_ratio=l1,\n",
    "            max_iter=3000,   # keep reasonable to avoid days of training\n",
    "            random_state=42\n",
    "        )\n",
    "        model.fit(X_train_s, y_train)\n",
    "\n",
    "        y_val_pred = model.predict(X_val_s)\n",
    "\n",
    "        mae_list = []\n",
    "        rmse_list = []\n",
    "        for i, h in enumerate(HORIZONS):\n",
    "            mae = mean_absolute_error(y_val[:, i], y_val_pred[:, i])\n",
    "            rmse = mean_squared_error(y_val[:, i], y_val_pred[:, i], squared=False)\n",
    "            mae_list.append(mae)\n",
    "            rmse_list.append(rmse)\n",
    "\n",
    "        rmse_mean = np.mean(rmse_list)\n",
    "\n",
    "        results.append({\n",
    "            \"alpha\": alpha,\n",
    "            \"l1_ratio\": l1,\n",
    "            \"val_rmse_mean\": rmse_mean,\n",
    "            \"val_mae_12\": mae_list[0],\n",
    "            \"val_mae_24\": mae_list[1],\n",
    "            \"val_mae_48\": mae_list[2],\n",
    "            \"val_mae_72\": mae_list[3],\n",
    "            \"val_rmse_12\": rmse_list[0],\n",
    "            \"val_rmse_24\": rmse_list[1],\n",
    "            \"val_rmse_48\": rmse_list[2],\n",
    "            \"val_rmse_72\": rmse_list[3],\n",
    "        })\n",
    "\n",
    "        if rmse_mean < best_val_rmse_mean:\n",
    "            best_val_rmse_mean = rmse_mean\n",
    "            best_cfg = (alpha, l1)\n",
    "\n",
    "print(\"Best config (alpha, l1_ratio):\", best_cfg)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df.sort_values(\"val_rmse_mean\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2ac01bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Validation performance ===\n",
      "Horizon 12h:  MAE=167.283,  RMSE=344.265\n",
      "Horizon 24h:  MAE=196.127,  RMSE=353.783\n",
      "Horizon 48h:  MAE=237.520,  RMSE=406.180\n",
      "Horizon 72h:  MAE=241.767,  RMSE=399.621\n",
      "\n",
      "=== Test performance ===\n",
      "Horizon 12h:  MAE=167.259,  RMSE=323.810\n",
      "Horizon 24h:  MAE=194.969,  RMSE=345.779\n",
      "Horizon 48h:  MAE=226.868,  RMSE=386.246\n",
      "Horizon 72h:  MAE=226.044,  RMSE=380.819\n"
     ]
    }
   ],
   "source": [
    "best_alpha, best_l1 = best_cfg\n",
    "\n",
    "best_model = MultiTaskElasticNet(\n",
    "    alpha=best_alpha,\n",
    "    l1_ratio=best_l1,\n",
    "    max_iter=3000,\n",
    "    random_state=42\n",
    ")\n",
    "best_model.fit(X_train_s, y_train)\n",
    "\n",
    "y_val_pred  = best_model.predict(X_val_s)\n",
    "y_test_pred = best_model.predict(X_test_s)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def evaluate_multioutput(y_true, y_pred, split_name):\n",
    "    print(f\"\\n=== {split_name} performance ===\")\n",
    "    for i, h in enumerate(HORIZONS):\n",
    "        mae = mean_absolute_error(y_true[:, i], y_pred[:, i])\n",
    "        rmse = mean_squared_error(y_true[:, i], y_pred[:, i], squared=False)\n",
    "        print(f\"Horizon {h:>2}h:  MAE={mae:.3f},  RMSE={rmse:.3f}\")\n",
    "\n",
    "evaluate_multioutput(y_val,  y_val_pred,  \"Validation\")\n",
    "evaluate_multioutput(y_test, y_test_pred, \"Test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bda368",
   "metadata": {},
   "source": [
    "## Random Forest (Hyperparamter Tuning,MIMO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c481cf25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any NaNs in X_train? False\n",
      "Any NaNs in X_val?   False\n",
      "Any NaNs in X_test?  False\n",
      "Any NaNs in y_train? False\n",
      "Any NaNs in y_val?   False\n",
      "Any NaNs in y_test?  False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Any NaNs in X_train?\", X_train.isna().any().any())\n",
    "print(\"Any NaNs in X_val?  \", X_val.isna().any().any())\n",
    "print(\"Any NaNs in X_test? \", X_test.isna().any().any())\n",
    "\n",
    "print(\"Any NaNs in y_train?\", np.isnan(y_train).any())\n",
    "print(\"Any NaNs in y_val?  \", np.isnan(y_val).any())\n",
    "print(\"Any NaNs in y_test? \", np.isnan(y_test).any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6702a32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning on 300000 samples.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "max_tune_samples = 300_000  # adjust if memory/time is tight\n",
    "\n",
    "if len(X_train) > max_tune_samples:\n",
    "    rng = np.random.default_rng(42)\n",
    "    tune_idx = rng.choice(len(X_train), size=max_tune_samples, replace=False)\n",
    "    \n",
    "    X_train_tune = X_train.iloc[tune_idx]\n",
    "    y_train_tune = y_train[tune_idx]\n",
    "else:\n",
    "    X_train_tune = X_train\n",
    "    y_train_tune = y_train\n",
    "\n",
    "print(\"Tuning on\", X_train_tune.shape[0], \"samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "55cc5db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RF config: (100, None, 2, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>val_rmse_mean</th>\n",
       "      <th>val_mae_12</th>\n",
       "      <th>val_mae_24</th>\n",
       "      <th>val_mae_48</th>\n",
       "      <th>val_mae_72</th>\n",
       "      <th>val_rmse_12</th>\n",
       "      <th>val_rmse_24</th>\n",
       "      <th>val_rmse_48</th>\n",
       "      <th>val_rmse_72</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>282.983614</td>\n",
       "      <td>117.338824</td>\n",
       "      <td>127.960574</td>\n",
       "      <td>153.048205</td>\n",
       "      <td>160.171105</td>\n",
       "      <td>246.066365</td>\n",
       "      <td>266.758007</td>\n",
       "      <td>306.538847</td>\n",
       "      <td>312.571236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>282.983614</td>\n",
       "      <td>117.338824</td>\n",
       "      <td>127.960574</td>\n",
       "      <td>153.048205</td>\n",
       "      <td>160.171105</td>\n",
       "      <td>246.066365</td>\n",
       "      <td>266.758007</td>\n",
       "      <td>306.538847</td>\n",
       "      <td>312.571236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>283.039988</td>\n",
       "      <td>116.047731</td>\n",
       "      <td>127.669979</td>\n",
       "      <td>153.037767</td>\n",
       "      <td>160.507952</td>\n",
       "      <td>244.737546</td>\n",
       "      <td>266.925250</td>\n",
       "      <td>306.962873</td>\n",
       "      <td>313.534282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>100</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>283.157123</td>\n",
       "      <td>118.211423</td>\n",
       "      <td>128.482174</td>\n",
       "      <td>153.455056</td>\n",
       "      <td>160.531777</td>\n",
       "      <td>246.378616</td>\n",
       "      <td>266.868051</td>\n",
       "      <td>306.685382</td>\n",
       "      <td>312.696442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>100</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>283.157123</td>\n",
       "      <td>118.211423</td>\n",
       "      <td>128.482174</td>\n",
       "      <td>153.455056</td>\n",
       "      <td>160.531777</td>\n",
       "      <td>246.378616</td>\n",
       "      <td>266.868051</td>\n",
       "      <td>306.685382</td>\n",
       "      <td>312.696442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>283.251061</td>\n",
       "      <td>116.950120</td>\n",
       "      <td>127.989081</td>\n",
       "      <td>153.357308</td>\n",
       "      <td>160.837637</td>\n",
       "      <td>245.660602</td>\n",
       "      <td>266.900261</td>\n",
       "      <td>306.945856</td>\n",
       "      <td>313.497525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>100</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>283.270967</td>\n",
       "      <td>117.621892</td>\n",
       "      <td>128.444136</td>\n",
       "      <td>153.642853</td>\n",
       "      <td>160.914510</td>\n",
       "      <td>245.311320</td>\n",
       "      <td>267.039325</td>\n",
       "      <td>307.125569</td>\n",
       "      <td>313.607655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>283.335284</td>\n",
       "      <td>117.813566</td>\n",
       "      <td>128.448546</td>\n",
       "      <td>153.332248</td>\n",
       "      <td>160.634840</td>\n",
       "      <td>246.429410</td>\n",
       "      <td>267.266171</td>\n",
       "      <td>306.727751</td>\n",
       "      <td>312.917805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>283.335284</td>\n",
       "      <td>117.813566</td>\n",
       "      <td>128.448546</td>\n",
       "      <td>153.332248</td>\n",
       "      <td>160.634840</td>\n",
       "      <td>246.429410</td>\n",
       "      <td>267.266171</td>\n",
       "      <td>306.727751</td>\n",
       "      <td>312.917805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>100</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>283.474345</td>\n",
       "      <td>118.288078</td>\n",
       "      <td>128.720355</td>\n",
       "      <td>153.866312</td>\n",
       "      <td>161.162918</td>\n",
       "      <td>246.147202</td>\n",
       "      <td>267.113721</td>\n",
       "      <td>307.066050</td>\n",
       "      <td>313.570408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_estimators  max_depth  min_samples_split  min_samples_leaf  val_rmse_mean  val_mae_12  val_mae_24  val_mae_48  val_mae_72  val_rmse_12  val_rmse_24  val_rmse_48  val_rmse_72\n",
       "23           100        NaN                  5                 3     282.983614  117.338824  127.960574  153.048205  160.171105   246.066365   266.758007   306.538847   312.571236\n",
       "21           100        NaN                  2                 3     282.983614  117.338824  127.960574  153.048205  160.171105   246.066365   266.758007   306.538847   312.571236\n",
       "20           100        NaN                  2                 1     283.039988  116.047731  127.669979  153.037767  160.507952   244.737546   266.925250   306.962873   313.534282\n",
       "19           100       20.0                  5                 3     283.157123  118.211423  128.482174  153.455056  160.531777   246.378616   266.868051   306.685382   312.696442\n",
       "17           100       20.0                  2                 3     283.157123  118.211423  128.482174  153.455056  160.531777   246.378616   266.868051   306.685382   312.696442\n",
       "22           100        NaN                  5                 1     283.251061  116.950120  127.989081  153.357308  160.837637   245.660602   266.900261   306.945856   313.497525\n",
       "16           100       20.0                  2                 1     283.270967  117.621892  128.444136  153.642853  160.914510   245.311320   267.039325   307.125569   313.607655\n",
       "9             50        NaN                  2                 3     283.335284  117.813566  128.448546  153.332248  160.634840   246.429410   267.266171   306.727751   312.917805\n",
       "11            50        NaN                  5                 3     283.335284  117.813566  128.448546  153.332248  160.634840   246.429410   267.266171   306.727751   312.917805\n",
       "18           100       20.0                  5                 1     283.474345  118.288078  128.720355  153.866312  161.162918   246.147202   267.113721   307.066050   313.570408"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "HORIZONS = [12, 24, 48, 72]\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100],\n",
    "    \"max_depth\": [10, 20, None],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 3],\n",
    "}\n",
    "\n",
    "best_cfg = None\n",
    "best_val_rmse_mean = np.inf\n",
    "results = []\n",
    "\n",
    "for n_est in param_grid[\"n_estimators\"]:\n",
    "    for depth in param_grid[\"max_depth\"]:\n",
    "        for min_split in param_grid[\"min_samples_split\"]:\n",
    "            for min_leaf in param_grid[\"min_samples_leaf\"]:\n",
    "                \n",
    "                rf = RandomForestRegressor(\n",
    "                    n_estimators=n_est,\n",
    "                    max_depth=depth,\n",
    "                    min_samples_split=min_split,\n",
    "                    min_samples_leaf=min_leaf,\n",
    "                    n_jobs=-1,\n",
    "                    random_state=42,\n",
    "                )\n",
    "                \n",
    "                rf.fit(X_train_tune, y_train_tune)   # MIMO training\n",
    "                \n",
    "                # Predict on validation (full val, not subsample)\n",
    "                y_val_pred = rf.predict(X_val)\n",
    "                \n",
    "                mae_list = []\n",
    "                rmse_list = []\n",
    "                for i, h in enumerate(HORIZONS):\n",
    "                    mae = mean_absolute_error(y_val[:, i], y_val_pred[:, i])\n",
    "                    rmse = mean_squared_error(y_val[:, i], y_val_pred[:, i], squared=False)\n",
    "                    mae_list.append(mae)\n",
    "                    rmse_list.append(rmse)\n",
    "                \n",
    "                rmse_mean = np.mean(rmse_list)\n",
    "                \n",
    "                results.append({\n",
    "                    \"n_estimators\": n_est,\n",
    "                    \"max_depth\": depth,\n",
    "                    \"min_samples_split\": min_split,\n",
    "                    \"min_samples_leaf\": min_leaf,\n",
    "                    \"val_rmse_mean\": rmse_mean,\n",
    "                    \"val_mae_12\": mae_list[0],\n",
    "                    \"val_mae_24\": mae_list[1],\n",
    "                    \"val_mae_48\": mae_list[2],\n",
    "                    \"val_mae_72\": mae_list[3],\n",
    "                    \"val_rmse_12\": rmse_list[0],\n",
    "                    \"val_rmse_24\": rmse_list[1],\n",
    "                    \"val_rmse_48\": rmse_list[2],\n",
    "                    \"val_rmse_72\": rmse_list[3],\n",
    "                })\n",
    "                \n",
    "                if rmse_mean < best_val_rmse_mean:\n",
    "                    best_val_rmse_mean = rmse_mean\n",
    "                    best_cfg = (n_est, depth, min_split, min_leaf)\n",
    "\n",
    "print(\"Best RF config:\", best_cfg)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df.sort_values(\"val_rmse_mean\").head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7005d538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(min_samples_leaf=3, n_jobs=-1, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(min_samples_leaf=3, n_jobs=-1, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(min_samples_leaf=3, n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_n_est, best_depth, best_min_split, best_min_leaf = best_cfg\n",
    "\n",
    "rf_best = RandomForestRegressor(\n",
    "    n_estimators=best_n_est,\n",
    "    max_depth=best_depth,\n",
    "    min_samples_split=best_min_split,\n",
    "    min_samples_leaf=best_min_leaf,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "rf_best.fit(X_train, y_train)   # full train, multi-output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "da8675f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Validation performance (Random Forest MIMO) ===\n",
      "Horizon 12h:  MAE=110.796,  RMSE=238.475\n",
      "Horizon 24h:  MAE=122.136,  RMSE=261.175\n",
      "Horizon 48h:  MAE=148.032,  RMSE=303.520\n",
      "Horizon 72h:  MAE=156.794,  RMSE=312.984\n",
      "\n",
      "=== Test performance (Random Forest MIMO) ===\n",
      "Horizon 12h:  MAE=108.037,  RMSE=230.633\n",
      "Horizon 24h:  MAE=109.674,  RMSE=238.836\n",
      "Horizon 48h:  MAE=123.461,  RMSE=267.246\n",
      "Horizon 72h:  MAE=131.515,  RMSE=279.490\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "y_val_pred  = rf_best.predict(X_val)\n",
    "y_test_pred = rf_best.predict(X_test)\n",
    "\n",
    "def evaluate_multioutput(y_true, y_pred, split_name):\n",
    "    print(f\"\\n=== {split_name} performance (Random Forest MIMO) ===\")\n",
    "    for i, h in enumerate(HORIZONS):\n",
    "        mae  = mean_absolute_error(y_true[:, i], y_pred[:, i])\n",
    "        rmse = mean_squared_error(y_true[:, i], y_pred[:, i], squared=False)\n",
    "        print(f\"Horizon {h:>2}h:  MAE={mae:.3f},  RMSE={rmse:.3f}\")\n",
    "\n",
    "evaluate_multioutput(y_val,  y_val_pred,  \"Validation\")\n",
    "evaluate_multioutput(y_test, y_test_pred, \"Test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3012d5e9",
   "metadata": {},
   "source": [
    "## LSTM MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "dd5b176a-d035-40e4-81f8-e2b85c2c3870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Train rows (LSTM): 1688253\n",
      "Val rows   (LSTM): 430974\n",
      "Test rows  (LSTM): 1081400\n",
      "Number of LSTM numeric features: 36\n",
      "Example features: ['route', 'total_flow', 'avg_speed', 'pct_observed', 'samples', 'district', 'abs_pm', 'length', 'lanes', 'ms_id', 'irm', 'hour', 'total_flow_lag_1', 'total_flow_lag_2', 'total_flow_lag_3', 'total_flow_lag_6', 'total_flow_lag_12', 'total_flow_lag_24', 'rolling_mean_24h', 'rolling_std_24h']\n",
      "Dropping all-NaN features (train): ['irm'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-21 23:01:16,772] A new study created in memory with name: no-name-57a985a3-25d9-4a32-8cde-e15780b21769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE=888286.5131 | Val avg MAE=204.9874\n",
      "Epoch 02 | Train MSE=81608.0009 | Val avg MAE=165.1434\n",
      "Epoch 03 | Train MSE=61305.1449 | Val avg MAE=156.8264\n",
      "Epoch 04 | Train MSE=47235.2947 | Val avg MAE=148.6420\n",
      "Epoch 05 | Train MSE=41653.5490 | Val avg MAE=147.5267\n",
      "Epoch 06 | Train MSE=39218.9999 | Val avg MAE=148.6104\n",
      "Epoch 07 | Train MSE=36999.8900 | Val avg MAE=150.5392\n",
      "Epoch 08 | Train MSE=34598.4405 | Val avg MAE=143.5492\n",
      "Epoch 09 | Train MSE=32771.6470 | Val avg MAE=139.6030\n",
      "Epoch 10 | Train MSE=31579.9796 | Val avg MAE=140.5489\n",
      "Epoch 11 | Train MSE=30753.5867 | Val avg MAE=146.5336\n",
      "Epoch 12 | Train MSE=29984.3764 | Val avg MAE=141.3441\n",
      "Epoch 13 | Train MSE=29366.8188 | Val avg MAE=141.5092\n",
      "Epoch 14 | Train MSE=28852.4461 | Val avg MAE=143.9686\n",
      "Early stopping after 14 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 01:19:04,001] Trial 0 finished with value: 139.60297012329102 and parameters: {'seq_len': 48, 'hidden_size': 128, 'num_layers': 1, 'dropout': 0.18279657183045964, 'lr': 0.00017243169962096453, 'weight_decay': 0.00034854923494569655, 'batch_size': 256, 'use_fc_head': True, 'fc_hidden': 256}. Best is trial 0 with value: 139.60297012329102.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE=2621705.7906 | Val avg MAE=789.4817\n",
      "Epoch 02 | Train MSE=2151011.8758 | Val avg MAE=690.6048\n",
      "Epoch 03 | Train MSE=1796077.1652 | Val avg MAE=622.9527\n",
      "Epoch 04 | Train MSE=1510598.8991 | Val avg MAE=573.8016\n",
      "Epoch 05 | Train MSE=1279173.4918 | Val avg MAE=532.5050\n",
      "Epoch 06 | Train MSE=1092885.3186 | Val avg MAE=493.6100\n",
      "Epoch 07 | Train MSE=937913.8016 | Val avg MAE=454.2796\n",
      "Epoch 08 | Train MSE=805808.4096 | Val avg MAE=416.3296\n",
      "Epoch 09 | Train MSE=690995.0705 | Val avg MAE=378.0546\n",
      "Epoch 10 | Train MSE=590861.3039 | Val avg MAE=344.8239\n",
      "Epoch 11 | Train MSE=504595.3047 | Val avg MAE=314.2296\n",
      "Epoch 12 | Train MSE=431245.8452 | Val avg MAE=292.6281\n",
      "Epoch 13 | Train MSE=369205.5969 | Val avg MAE=268.7428\n",
      "Epoch 14 | Train MSE=317047.7433 | Val avg MAE=252.8503\n",
      "Epoch 15 | Train MSE=273347.0962 | Val avg MAE=239.3214\n",
      "Epoch 16 | Train MSE=236618.8761 | Val avg MAE=225.5005\n",
      "Epoch 17 | Train MSE=205944.4810 | Val avg MAE=211.6714\n",
      "Epoch 18 | Train MSE=180184.7148 | Val avg MAE=202.9552\n",
      "Epoch 19 | Train MSE=158614.8294 | Val avg MAE=194.9935\n",
      "Epoch 20 | Train MSE=140640.2246 | Val avg MAE=189.5177\n",
      "Epoch 21 | Train MSE=125692.5666 | Val avg MAE=184.0170\n",
      "Epoch 22 | Train MSE=113363.2301 | Val avg MAE=176.6951\n",
      "Epoch 23 | Train MSE=103239.5558 | Val avg MAE=172.5187\n",
      "Epoch 24 | Train MSE=95015.8113 | Val avg MAE=170.8204\n",
      "Epoch 25 | Train MSE=88318.0573 | Val avg MAE=167.4623\n",
      "Epoch 26 | Train MSE=82855.4641 | Val avg MAE=165.0475\n",
      "Epoch 27 | Train MSE=78331.2530 | Val avg MAE=162.8670\n",
      "Epoch 28 | Train MSE=74465.6919 | Val avg MAE=161.7203\n",
      "Epoch 29 | Train MSE=71063.7156 | Val avg MAE=160.3799\n",
      "Epoch 30 | Train MSE=67922.9965 | Val avg MAE=160.6644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 02:55:49,144] Trial 1 finished with value: 160.37986755371094 and parameters: {'seq_len': 24, 'hidden_size': 64, 'num_layers': 1, 'dropout': 0.11877656731480904, 'lr': 0.0003776461966532008, 'weight_decay': 7.863453923578373e-06, 'batch_size': 128, 'use_fc_head': False}. Best is trial 0 with value: 139.60297012329102.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE=284033.4902 | Val avg MAE=157.1277\n",
      "Epoch 02 | Train MSE=46351.6253 | Val avg MAE=145.4572\n",
      "Epoch 03 | Train MSE=33912.8262 | Val avg MAE=141.9714\n",
      "Epoch 04 | Train MSE=30040.7267 | Val avg MAE=135.2995\n",
      "Epoch 05 | Train MSE=27664.3493 | Val avg MAE=137.1155\n",
      "Epoch 06 | Train MSE=25949.5624 | Val avg MAE=138.3983\n",
      "Epoch 07 | Train MSE=24658.5912 | Val avg MAE=137.2494\n",
      "Epoch 08 | Train MSE=23609.4686 | Val avg MAE=138.7376\n",
      "Epoch 09 | Train MSE=22743.8419 | Val avg MAE=136.8513\n",
      "Early stopping after 9 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 05:43:41,105] Trial 2 finished with value: 135.29945182800293 and parameters: {'seq_len': 24, 'hidden_size': 128, 'num_layers': 3, 'dropout': 0.1398739486757031, 'lr': 0.00047412555827233585, 'weight_decay': 3.008165097293078e-06, 'batch_size': 128, 'use_fc_head': True, 'fc_hidden': 128}. Best is trial 2 with value: 135.29945182800293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE=2294259.2660 | Val avg MAE=651.5102\n",
      "Epoch 02 | Train MSE=1482749.0246 | Val avg MAE=530.2490\n",
      "Epoch 03 | Train MSE=1012463.3921 | Val avg MAE=433.4208\n",
      "Epoch 04 | Train MSE=703793.5889 | Val avg MAE=354.5431\n",
      "Epoch 05 | Train MSE=493297.5230 | Val avg MAE=297.7001\n",
      "Epoch 06 | Train MSE=350895.0014 | Val avg MAE=254.7436\n",
      "Epoch 07 | Train MSE=252490.8693 | Val avg MAE=221.1709\n",
      "Epoch 08 | Train MSE=184623.7303 | Val avg MAE=199.9077\n",
      "Epoch 09 | Train MSE=138032.2261 | Val avg MAE=184.2266\n",
      "Epoch 10 | Train MSE=106814.7962 | Val avg MAE=176.2647\n",
      "Epoch 11 | Train MSE=86542.7910 | Val avg MAE=168.1982\n",
      "Epoch 12 | Train MSE=72576.1783 | Val avg MAE=163.6987\n",
      "Epoch 13 | Train MSE=61934.2868 | Val avg MAE=157.8394\n",
      "Epoch 14 | Train MSE=53730.3491 | Val avg MAE=156.1455\n",
      "Epoch 15 | Train MSE=47711.7626 | Val avg MAE=153.4488\n",
      "Epoch 16 | Train MSE=43304.0829 | Val avg MAE=148.5932\n",
      "Epoch 17 | Train MSE=39939.0743 | Val avg MAE=151.4136\n",
      "Epoch 18 | Train MSE=37324.0479 | Val avg MAE=148.4808\n",
      "Epoch 19 | Train MSE=35088.5825 | Val avg MAE=147.1698\n",
      "Epoch 20 | Train MSE=33235.7524 | Val avg MAE=148.3777\n",
      "Epoch 21 | Train MSE=31682.5868 | Val avg MAE=145.0995\n",
      "Epoch 22 | Train MSE=30356.1201 | Val avg MAE=147.3832\n",
      "Epoch 23 | Train MSE=29209.2844 | Val avg MAE=143.6507\n",
      "Epoch 24 | Train MSE=28248.4649 | Val avg MAE=143.8497\n",
      "Epoch 25 | Train MSE=27476.6638 | Val avg MAE=144.3288\n",
      "Epoch 26 | Train MSE=26773.0391 | Val avg MAE=141.8480\n",
      "Epoch 27 | Train MSE=26139.6864 | Val avg MAE=144.1714\n",
      "Epoch 28 | Train MSE=25623.7780 | Val avg MAE=141.0975\n",
      "Epoch 29 | Train MSE=25129.2140 | Val avg MAE=143.0883\n",
      "Epoch 30 | Train MSE=24707.8586 | Val avg MAE=142.6394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 13:26:11,537] Trial 3 finished with value: 141.0974521636963 and parameters: {'seq_len': 48, 'hidden_size': 96, 'num_layers': 2, 'dropout': 0.2835048252827458, 'lr': 0.0006307009205788019, 'weight_decay': 1.2062676008709296e-06, 'batch_size': 128, 'use_fc_head': False}. Best is trial 2 with value: 135.29945182800293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE=260988.3778 | Val avg MAE=146.9226\n",
      "Epoch 02 | Train MSE=39085.6565 | Val avg MAE=141.6802\n",
      "Epoch 03 | Train MSE=31941.2732 | Val avg MAE=141.9841\n",
      "Epoch 04 | Train MSE=29010.5577 | Val avg MAE=137.9284\n",
      "Epoch 05 | Train MSE=27166.7309 | Val avg MAE=139.7583\n",
      "Epoch 06 | Train MSE=25804.5396 | Val avg MAE=140.0269\n",
      "Epoch 07 | Train MSE=24747.7265 | Val avg MAE=139.8873\n",
      "Epoch 08 | Train MSE=23823.6942 | Val avg MAE=141.8532\n",
      "Epoch 09 | Train MSE=23084.4415 | Val avg MAE=140.1224\n",
      "Early stopping after 9 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 16:06:52,993] Trial 4 finished with value: 137.92837142944336 and parameters: {'seq_len': 48, 'hidden_size': 192, 'num_layers': 1, 'dropout': 0.04667802755387429, 'lr': 0.0005998145323909479, 'weight_decay': 0.00010353206835601686, 'batch_size': 128, 'use_fc_head': True, 'fc_hidden': 64}. Best is trial 2 with value: 135.29945182800293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE=2684883.3318 | Val avg MAE=820.3597\n",
      "Epoch 02 | Train MSE=2297647.8178 | Val avg MAE=730.7983\n",
      "Epoch 03 | Train MSE=1989562.4904 | Val avg MAE=666.6228\n",
      "Epoch 04 | Train MSE=1734265.0883 | Val avg MAE=617.4141\n",
      "Epoch 05 | Train MSE=1517627.4097 | Val avg MAE=577.8007\n",
      "Epoch 06 | Train MSE=1333585.8886 | Val avg MAE=543.6396\n",
      "Epoch 07 | Train MSE=1176849.7188 | Val avg MAE=507.5430\n",
      "Epoch 08 | Train MSE=1041609.0367 | Val avg MAE=469.8899\n",
      "Epoch 09 | Train MSE=921016.3394 | Val avg MAE=433.5579\n",
      "Epoch 10 | Train MSE=813651.9175 | Val avg MAE=401.2734\n",
      "Epoch 11 | Train MSE=718357.5126 | Val avg MAE=374.8584\n",
      "Epoch 12 | Train MSE=633577.5967 | Val avg MAE=347.9872\n",
      "Epoch 13 | Train MSE=558116.8781 | Val avg MAE=324.4159\n",
      "Epoch 14 | Train MSE=491738.7573 | Val avg MAE=305.0246\n",
      "Epoch 15 | Train MSE=433454.3251 | Val avg MAE=286.3428\n",
      "Epoch 16 | Train MSE=382561.2933 | Val avg MAE=269.3656\n",
      "Epoch 17 | Train MSE=338141.8355 | Val avg MAE=253.6048\n",
      "Epoch 18 | Train MSE=299469.5738 | Val avg MAE=242.7223\n",
      "Epoch 19 | Train MSE=265808.9889 | Val avg MAE=231.9024\n",
      "Epoch 20 | Train MSE=236561.6045 | Val avg MAE=221.8752\n",
      "Epoch 21 | Train MSE=211338.5230 | Val avg MAE=212.7442\n",
      "Epoch 22 | Train MSE=189531.0732 | Val avg MAE=205.4183\n",
      "Epoch 23 | Train MSE=170715.9897 | Val avg MAE=199.0198\n",
      "Epoch 24 | Train MSE=154515.0588 | Val avg MAE=193.2048\n",
      "Epoch 25 | Train MSE=140461.3871 | Val avg MAE=188.4958\n",
      "Epoch 26 | Train MSE=128342.6424 | Val avg MAE=183.9964\n",
      "Epoch 27 | Train MSE=117927.9255 | Val avg MAE=179.4067\n",
      "Epoch 28 | Train MSE=108970.4793 | Val avg MAE=178.4774\n",
      "Epoch 29 | Train MSE=101275.1148 | Val avg MAE=175.3826\n",
      "Epoch 30 | Train MSE=94697.4205 | Val avg MAE=174.0200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 19:02:10,569] Trial 5 finished with value: 174.01996231079102 and parameters: {'seq_len': 24, 'hidden_size': 64, 'num_layers': 2, 'dropout': 0.058076191292492446, 'lr': 0.00028913167587516473, 'weight_decay': 0.00010666718837173085, 'batch_size': 128, 'use_fc_head': False}. Best is trial 2 with value: 135.29945182800293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE=2803203.4691 | Val avg MAE=885.6512\n",
      "Epoch 02 | Train MSE=2597472.8882 | Val avg MAE=827.5401\n",
      "Epoch 03 | Train MSE=2415081.3623 | Val avg MAE=780.4769\n",
      "Epoch 04 | Train MSE=2252531.4674 | Val avg MAE=741.2516\n",
      "Epoch 05 | Train MSE=2106742.6438 | Val avg MAE=708.3741\n",
      "Epoch 06 | Train MSE=1973935.8167 | Val avg MAE=679.7475\n",
      "Epoch 07 | Train MSE=1851217.5591 | Val avg MAE=654.9623\n",
      "Epoch 08 | Train MSE=1737670.7555 | Val avg MAE=631.0606\n",
      "Epoch 09 | Train MSE=1632197.1376 | Val avg MAE=611.5437\n",
      "Epoch 10 | Train MSE=1534143.9806 | Val avg MAE=593.4034\n",
      "Epoch 11 | Train MSE=1443355.0732 | Val avg MAE=575.1620\n",
      "Epoch 12 | Train MSE=1359080.5994 | Val avg MAE=560.7695\n",
      "Epoch 13 | Train MSE=1280730.9828 | Val avg MAE=546.3141\n",
      "Epoch 14 | Train MSE=1208282.6694 | Val avg MAE=530.8681\n",
      "Epoch 15 | Train MSE=1140660.3700 | Val avg MAE=514.5329\n",
      "Epoch 16 | Train MSE=1077289.4858 | Val avg MAE=498.8444\n",
      "Epoch 17 | Train MSE=1017515.0709 | Val avg MAE=482.4942\n",
      "Epoch 18 | Train MSE=960690.1810 | Val avg MAE=465.4574\n",
      "Epoch 19 | Train MSE=906955.0956 | Val avg MAE=450.3168\n",
      "Epoch 20 | Train MSE=855915.5181 | Val avg MAE=435.3197\n",
      "Epoch 21 | Train MSE=807633.9801 | Val avg MAE=419.0098\n",
      "Epoch 22 | Train MSE=762243.2550 | Val avg MAE=406.1541\n",
      "Epoch 23 | Train MSE=719233.6056 | Val avg MAE=391.9480\n",
      "Epoch 24 | Train MSE=678544.2192 | Val avg MAE=380.7392\n",
      "Epoch 25 | Train MSE=639957.2478 | Val avg MAE=366.7866\n",
      "Epoch 26 | Train MSE=603332.9734 | Val avg MAE=353.2302\n",
      "Epoch 27 | Train MSE=568770.9450 | Val avg MAE=343.9203\n",
      "Epoch 28 | Train MSE=536177.3527 | Val avg MAE=332.6523\n",
      "Epoch 29 | Train MSE=505460.9657 | Val avg MAE=322.9227\n",
      "Epoch 30 | Train MSE=476532.7574 | Val avg MAE=313.8608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-22 22:08:42,113] Trial 6 finished with value: 313.86082458496094 and parameters: {'seq_len': 24, 'hidden_size': 128, 'num_layers': 1, 'dropout': 0.011094582668396136, 'lr': 0.00013331706318015402, 'weight_decay': 0.00028683881838577097, 'batch_size': 256, 'use_fc_head': False}. Best is trial 2 with value: 135.29945182800293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE=464606.9194 | Val avg MAE=161.4172\n",
      "Epoch 02 | Train MSE=52458.0181 | Val avg MAE=148.2775\n",
      "Epoch 03 | Train MSE=38908.0276 | Val avg MAE=141.7835\n",
      "Epoch 04 | Train MSE=33739.9144 | Val avg MAE=140.8395\n",
      "Epoch 05 | Train MSE=31561.8034 | Val avg MAE=140.1644\n",
      "Epoch 06 | Train MSE=30080.7688 | Val avg MAE=139.9337\n",
      "Epoch 07 | Train MSE=28933.3383 | Val avg MAE=139.4897\n",
      "Epoch 08 | Train MSE=28011.5741 | Val avg MAE=141.6566\n",
      "Epoch 09 | Train MSE=27241.5938 | Val avg MAE=140.7020\n",
      "Epoch 10 | Train MSE=26602.1427 | Val avg MAE=141.4180\n",
      "Epoch 11 | Train MSE=26053.1234 | Val avg MAE=142.7401\n",
      "Epoch 12 | Train MSE=25518.8418 | Val avg MAE=140.0545\n",
      "Early stopping after 12 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-23 00:27:40,224] Trial 7 finished with value: 139.48967170715332 and parameters: {'seq_len': 48, 'hidden_size': 128, 'num_layers': 1, 'dropout': 0.13973381907150748, 'lr': 0.0005124113628062034, 'weight_decay': 8.556461033525865e-05, 'batch_size': 256, 'use_fc_head': True, 'fc_hidden': 128}. Best is trial 2 with value: 135.29945182800293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE=2739757.4359 | Val avg MAE=842.4169\n",
      "Epoch 02 | Train MSE=2430941.3489 | Val avg MAE=766.3767\n",
      "Epoch 03 | Train MSE=2178379.2796 | Val avg MAE=709.2198\n",
      "Epoch 04 | Train MSE=1962990.7260 | Val avg MAE=664.0283\n",
      "Epoch 05 | Train MSE=1772116.7047 | Val avg MAE=625.9785\n",
      "Epoch 06 | Train MSE=1602332.4344 | Val avg MAE=595.4917\n",
      "Epoch 07 | Train MSE=1451851.6273 | Val avg MAE=566.2780\n",
      "Epoch 08 | Train MSE=1318711.5664 | Val avg MAE=543.5745\n",
      "Epoch 09 | Train MSE=1200897.7543 | Val avg MAE=520.8731\n",
      "Epoch 10 | Train MSE=1095901.5154 | Val avg MAE=494.5957\n",
      "Epoch 11 | Train MSE=1001241.7233 | Val avg MAE=472.1403\n",
      "Epoch 12 | Train MSE=914976.8914 | Val avg MAE=449.4621\n",
      "Epoch 13 | Train MSE=835879.4852 | Val avg MAE=424.9232\n",
      "Epoch 14 | Train MSE=763744.4531 | Val avg MAE=406.2322\n",
      "Epoch 15 | Train MSE=697896.0798 | Val avg MAE=385.3488\n",
      "Epoch 16 | Train MSE=637302.3783 | Val avg MAE=365.5727\n",
      "Epoch 17 | Train MSE=582057.7048 | Val avg MAE=347.9329\n",
      "Epoch 18 | Train MSE=531820.7073 | Val avg MAE=333.1479\n",
      "Epoch 19 | Train MSE=486270.9472 | Val avg MAE=316.6976\n",
      "Epoch 20 | Train MSE=444943.9340 | Val avg MAE=305.4074\n",
      "Epoch 21 | Train MSE=407460.9077 | Val avg MAE=289.0171\n",
      "Epoch 22 | Train MSE=373241.9051 | Val avg MAE=279.8793\n",
      "Epoch 23 | Train MSE=342079.0973 | Val avg MAE=266.6041\n",
      "Epoch 24 | Train MSE=314047.1936 | Val avg MAE=257.1157\n",
      "Epoch 25 | Train MSE=288611.5194 | Val avg MAE=250.9406\n",
      "Epoch 26 | Train MSE=265460.6830 | Val avg MAE=241.6198\n",
      "Epoch 27 | Train MSE=244420.4585 | Val avg MAE=233.6582\n",
      "Epoch 28 | Train MSE=225331.5786 | Val avg MAE=225.8290\n",
      "Epoch 29 | Train MSE=207998.4382 | Val avg MAE=217.9503\n",
      "Epoch 30 | Train MSE=192333.3883 | Val avg MAE=214.0431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-23 05:33:05,293] Trial 8 finished with value: 214.0431022644043 and parameters: {'seq_len': 48, 'hidden_size': 128, 'num_layers': 1, 'dropout': 0.251561045511314, 'lr': 0.00011225019871639282, 'weight_decay': 1.002654251729281e-06, 'batch_size': 128, 'use_fc_head': False}. Best is trial 2 with value: 135.29945182800293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE=758409.2972 | Val avg MAE=187.0823\n",
      "Epoch 02 | Train MSE=73882.6501 | Val avg MAE=166.8026\n",
      "Epoch 03 | Train MSE=64293.4651 | Val avg MAE=161.7550\n",
      "Epoch 04 | Train MSE=50628.8792 | Val avg MAE=155.2564\n",
      "Epoch 05 | Train MSE=41021.0099 | Val avg MAE=149.9497\n",
      "Epoch 06 | Train MSE=38002.9858 | Val avg MAE=149.3010\n",
      "Epoch 07 | Train MSE=34827.1180 | Val avg MAE=146.0250\n",
      "Epoch 08 | Train MSE=31161.5938 | Val avg MAE=145.7017\n",
      "Epoch 09 | Train MSE=29095.9076 | Val avg MAE=143.9759\n",
      "Epoch 10 | Train MSE=27754.4067 | Val avg MAE=141.9228\n",
      "Epoch 11 | Train MSE=26750.7786 | Val avg MAE=140.9322\n",
      "Epoch 12 | Train MSE=25827.0713 | Val avg MAE=141.7650\n",
      "Epoch 13 | Train MSE=25069.2377 | Val avg MAE=140.3488\n",
      "Epoch 14 | Train MSE=24373.7806 | Val avg MAE=143.4617\n",
      "Epoch 15 | Train MSE=23795.6431 | Val avg MAE=141.8383\n",
      "Epoch 16 | Train MSE=23260.0325 | Val avg MAE=141.6295\n",
      "Epoch 17 | Train MSE=22701.6178 | Val avg MAE=139.3617\n",
      "Epoch 18 | Train MSE=22274.4746 | Val avg MAE=140.5797\n",
      "Epoch 19 | Train MSE=21846.7328 | Val avg MAE=141.9268\n",
      "Epoch 20 | Train MSE=21431.2526 | Val avg MAE=141.2170\n",
      "Epoch 21 | Train MSE=21056.4389 | Val avg MAE=141.8851\n",
      "Epoch 22 | Train MSE=20733.3831 | Val avg MAE=141.3458\n",
      "Early stopping after 22 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-23 18:19:37,689] Trial 9 finished with value: 139.36168479919434 and parameters: {'seq_len': 48, 'hidden_size': 128, 'num_layers': 3, 'dropout': 0.22776324392350739, 'lr': 0.00028133278163109516, 'weight_decay': 0.0004397701596305855, 'batch_size': 256, 'use_fc_head': True, 'fc_hidden': 128}. Best is trial 2 with value: 135.29945182800293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE=304693.6676 | Val avg MAE=156.4496\n",
      "Epoch 02 | Train MSE=50421.2956 | Val avg MAE=143.4345\n",
      "Epoch 03 | Train MSE=34770.9535 | Val avg MAE=138.8524\n",
      "Epoch 04 | Train MSE=29497.9097 | Val avg MAE=140.2761\n",
      "Epoch 05 | Train MSE=26964.8626 | Val avg MAE=136.9650\n",
      "Epoch 06 | Train MSE=25075.2465 | Val avg MAE=135.8765\n",
      "Epoch 07 | Train MSE=23512.3296 | Val avg MAE=139.8687\n",
      "Epoch 08 | Train MSE=22287.8167 | Val avg MAE=138.0817\n",
      "Epoch 09 | Train MSE=21257.7699 | Val avg MAE=138.4098\n",
      "Epoch 10 | Train MSE=20392.3774 | Val avg MAE=138.2908\n",
      "Epoch 11 | Train MSE=19608.9211 | Val avg MAE=139.0440\n",
      "Early stopping after 11 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 00:53:56,764] Trial 10 finished with value: 135.87645149230957 and parameters: {'seq_len': 24, 'hidden_size': 192, 'num_layers': 3, 'dropout': 0.101225242494378, 'lr': 0.00038378821501749413, 'weight_decay': 6.077813966908005e-06, 'batch_size': 128, 'use_fc_head': True, 'fc_hidden': 128}. Best is trial 2 with value: 135.29945182800293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE=279284.6351 | Val avg MAE=148.8024\n",
      "Epoch 02 | Train MSE=39305.7303 | Val avg MAE=139.0697\n",
      "Epoch 03 | Train MSE=29949.2799 | Val avg MAE=137.3662\n",
      "Epoch 04 | Train MSE=26363.5798 | Val avg MAE=137.1802\n",
      "Epoch 05 | Train MSE=24059.5869 | Val avg MAE=136.3881\n",
      "Epoch 06 | Train MSE=22358.3973 | Val avg MAE=133.9233\n",
      "Epoch 07 | Train MSE=21034.0140 | Val avg MAE=137.5202\n",
      "Epoch 08 | Train MSE=19943.1887 | Val avg MAE=137.0705\n",
      "Epoch 09 | Train MSE=19026.7000 | Val avg MAE=135.3519\n",
      "Epoch 10 | Train MSE=18190.5133 | Val avg MAE=136.2853\n",
      "Epoch 11 | Train MSE=17502.0238 | Val avg MAE=136.8981\n",
      "Early stopping after 11 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 07:14:03,337] Trial 11 finished with value: 133.9233112335205 and parameters: {'seq_len': 24, 'hidden_size': 192, 'num_layers': 3, 'dropout': 0.08964163752606763, 'lr': 0.0004045030802118628, 'weight_decay': 5.901669321983368e-06, 'batch_size': 128, 'use_fc_head': True, 'fc_hidden': 128}. Best is trial 11 with value: 133.9233112335205.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE=186442.6692 | Val avg MAE=144.2151\n",
      "Epoch 02 | Train MSE=35226.5526 | Val avg MAE=140.0301\n",
      "Epoch 03 | Train MSE=28850.5453 | Val avg MAE=139.8354\n",
      "Epoch 04 | Train MSE=25861.3349 | Val avg MAE=139.2626\n",
      "Epoch 05 | Train MSE=23940.6118 | Val avg MAE=139.0280\n",
      "Epoch 06 | Train MSE=22458.3859 | Val avg MAE=138.1199\n",
      "Epoch 07 | Train MSE=21372.4467 | Val avg MAE=136.0789\n",
      "Epoch 08 | Train MSE=20372.7731 | Val avg MAE=136.7900\n",
      "Epoch 09 | Train MSE=19586.8926 | Val avg MAE=135.9178\n",
      "Epoch 10 | Train MSE=18894.0419 | Val avg MAE=137.3139\n",
      "Epoch 11 | Train MSE=18282.1834 | Val avg MAE=136.8845\n",
      "Epoch 12 | Train MSE=17786.9971 | Val avg MAE=136.7448\n",
      "Epoch 13 | Train MSE=17364.2711 | Val avg MAE=137.2697\n",
      "Epoch 14 | Train MSE=17015.1243 | Val avg MAE=136.7741\n",
      "Early stopping after 14 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 15:27:42,262] Trial 12 finished with value: 135.9177646636963 and parameters: {'seq_len': 24, 'hidden_size': 192, 'num_layers': 3, 'dropout': 0.17000210469070173, 'lr': 0.0007949108157047812, 'weight_decay': 4.8654761136106204e-06, 'batch_size': 128, 'use_fc_head': True, 'fc_hidden': 128}. Best is trial 11 with value: 133.9233112335205.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE=348580.6089 | Val avg MAE=161.3604\n",
      "Epoch 02 | Train MSE=62921.8425 | Val avg MAE=151.6389\n",
      "Epoch 03 | Train MSE=42839.8754 | Val avg MAE=143.6594\n",
      "Epoch 04 | Train MSE=36142.1205 | Val avg MAE=142.2110\n",
      "Epoch 05 | Train MSE=31330.6735 | Val avg MAE=139.4617\n",
      "Epoch 06 | Train MSE=29414.4711 | Val avg MAE=141.2255\n",
      "Epoch 07 | Train MSE=27999.7005 | Val avg MAE=139.2467\n",
      "Epoch 08 | Train MSE=26850.4758 | Val avg MAE=142.2234\n",
      "Epoch 09 | Train MSE=25915.7651 | Val avg MAE=137.8974\n",
      "Epoch 10 | Train MSE=25149.9979 | Val avg MAE=135.3493\n",
      "Epoch 11 | Train MSE=24426.9738 | Val avg MAE=138.8775\n",
      "Epoch 12 | Train MSE=23839.6450 | Val avg MAE=137.6000\n",
      "Epoch 13 | Train MSE=23366.2449 | Val avg MAE=137.7680\n",
      "Epoch 14 | Train MSE=22867.5905 | Val avg MAE=135.6014\n",
      "Epoch 15 | Train MSE=22490.5167 | Val avg MAE=138.9943\n",
      "Early stopping after 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 18:46:38,947] Trial 13 finished with value: 135.34930229187012 and parameters: {'seq_len': 24, 'hidden_size': 96, 'num_layers': 3, 'dropout': 0.07117707811692936, 'lr': 0.0004077622308173462, 'weight_decay': 1.682006029819981e-05, 'batch_size': 128, 'use_fc_head': True, 'fc_hidden': 128}. Best is trial 11 with value: 133.9233112335205.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE=516855.7652 | Val avg MAE=157.1177\n",
      "Epoch 02 | Train MSE=61125.7373 | Val avg MAE=148.3840\n",
      "Epoch 03 | Train MSE=41558.7641 | Val avg MAE=146.4435\n",
      "Epoch 04 | Train MSE=36031.8955 | Val avg MAE=138.0881\n",
      "Epoch 05 | Train MSE=30696.7399 | Val avg MAE=138.0244\n",
      "Epoch 06 | Train MSE=28023.6283 | Val avg MAE=139.5586\n",
      "Epoch 07 | Train MSE=26336.7248 | Val avg MAE=139.1256\n",
      "Epoch 08 | Train MSE=25017.1608 | Val avg MAE=139.4393\n",
      "Epoch 09 | Train MSE=23997.8156 | Val avg MAE=139.4142\n",
      "Epoch 10 | Train MSE=23054.9509 | Val avg MAE=139.0334\n",
      "Early stopping after 10 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 22:30:07,389] Trial 14 finished with value: 138.02438163757324 and parameters: {'seq_len': 24, 'hidden_size': 192, 'num_layers': 2, 'dropout': 0.19673780715529038, 'lr': 0.0002585468986854149, 'weight_decay': 2.4972119495076417e-06, 'batch_size': 128, 'use_fc_head': True, 'fc_hidden': 64}. Best is trial 11 with value: 133.9233112335205.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE=336089.8868 | Val avg MAE=161.4560\n",
      "Epoch 02 | Train MSE=57717.9074 | Val avg MAE=148.8668\n",
      "Epoch 03 | Train MSE=39382.8713 | Val avg MAE=143.1752\n",
      "Epoch 04 | Train MSE=31968.2203 | Val avg MAE=140.5323\n",
      "Epoch 05 | Train MSE=28922.5236 | Val avg MAE=139.0856\n",
      "Epoch 06 | Train MSE=26898.1297 | Val avg MAE=139.7355\n",
      "Epoch 07 | Train MSE=25307.5352 | Val avg MAE=139.2145\n",
      "Epoch 08 | Train MSE=24059.5566 | Val avg MAE=137.2347\n",
      "Epoch 09 | Train MSE=22980.1167 | Val avg MAE=139.2110\n",
      "Epoch 10 | Train MSE=22076.6139 | Val avg MAE=138.1128\n",
      "Epoch 11 | Train MSE=21298.4518 | Val avg MAE=137.7496\n",
      "Epoch 12 | Train MSE=20586.6966 | Val avg MAE=136.9756\n",
      "Epoch 13 | Train MSE=19969.2824 | Val avg MAE=139.1564\n",
      "Epoch 14 | Train MSE=19396.8252 | Val avg MAE=137.8435\n",
      "Epoch 15 | Train MSE=18874.0063 | Val avg MAE=137.9800\n",
      "Epoch 16 | Train MSE=18393.2891 | Val avg MAE=138.7812\n",
      "Epoch 17 | Train MSE=17934.1955 | Val avg MAE=138.3741\n",
      "Early stopping after 17 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-25 08:05:44,895] Trial 15 finished with value: 136.97562789916992 and parameters: {'seq_len': 24, 'hidden_size': 192, 'num_layers': 3, 'dropout': 0.09729382516478127, 'lr': 0.0002179641614253948, 'weight_decay': 1.5509688548975008e-05, 'batch_size': 128, 'use_fc_head': True, 'fc_hidden': 256}. Best is trial 11 with value: 133.9233112335205.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE=389296.3747 | Val avg MAE=164.8741\n",
      "Epoch 02 | Train MSE=65039.1047 | Val avg MAE=156.8596\n",
      "Epoch 03 | Train MSE=46461.1763 | Val avg MAE=149.2650\n",
      "Epoch 04 | Train MSE=41781.6123 | Val avg MAE=146.3369\n",
      "Epoch 05 | Train MSE=36582.8605 | Val avg MAE=141.0688\n",
      "Epoch 06 | Train MSE=34372.4619 | Val avg MAE=140.2212\n",
      "Epoch 07 | Train MSE=33015.5121 | Val avg MAE=141.8196\n",
      "Epoch 08 | Train MSE=31958.0506 | Val avg MAE=141.8921\n",
      "Epoch 09 | Train MSE=31121.2252 | Val avg MAE=141.6164\n",
      "Epoch 10 | Train MSE=30404.2920 | Val avg MAE=139.7038\n",
      "Epoch 11 | Train MSE=29802.3856 | Val avg MAE=139.9544\n",
      "Epoch 12 | Train MSE=29252.8847 | Val avg MAE=139.9010\n",
      "Epoch 13 | Train MSE=28792.2317 | Val avg MAE=140.5877\n",
      "Epoch 14 | Train MSE=28355.5815 | Val avg MAE=139.1037\n",
      "Epoch 15 | Train MSE=27981.0564 | Val avg MAE=140.3638\n",
      "Epoch 16 | Train MSE=27616.4142 | Val avg MAE=138.3142\n",
      "Epoch 17 | Train MSE=27320.7520 | Val avg MAE=140.0763\n",
      "Epoch 18 | Train MSE=27034.3575 | Val avg MAE=138.0690\n",
      "Epoch 19 | Train MSE=26731.7126 | Val avg MAE=138.6768\n",
      "Epoch 20 | Train MSE=26499.8905 | Val avg MAE=138.6510\n",
      "Epoch 21 | Train MSE=26268.9049 | Val avg MAE=140.0447\n",
      "Epoch 22 | Train MSE=26074.6587 | Val avg MAE=139.8949\n",
      "Epoch 23 | Train MSE=25889.1168 | Val avg MAE=139.0560\n",
      "Early stopping after 23 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-25 11:07:58,437] Trial 16 finished with value: 138.06896018981934 and parameters: {'seq_len': 24, 'hidden_size': 64, 'num_layers': 3, 'dropout': 0.1422982715750114, 'lr': 0.0004726276037662199, 'weight_decay': 2.4508033260334865e-06, 'batch_size': 128, 'use_fc_head': True, 'fc_hidden': 128}. Best is trial 11 with value: 133.9233112335205.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE=201415.0059 | Val avg MAE=145.4687\n",
      "Epoch 02 | Train MSE=37359.7056 | Val avg MAE=138.8119\n",
      "Epoch 03 | Train MSE=31420.8225 | Val avg MAE=135.3271\n",
      "Epoch 04 | Train MSE=28898.4725 | Val avg MAE=140.2875\n",
      "Epoch 05 | Train MSE=27153.0307 | Val avg MAE=134.0841\n",
      "Epoch 06 | Train MSE=25911.9836 | Val avg MAE=136.0683\n",
      "Epoch 07 | Train MSE=24890.4163 | Val avg MAE=139.0135\n",
      "Epoch 08 | Train MSE=24083.2980 | Val avg MAE=137.8493\n",
      "Epoch 09 | Train MSE=23361.0051 | Val avg MAE=137.4012\n",
      "Epoch 10 | Train MSE=22768.8035 | Val avg MAE=136.2285\n",
      "Early stopping after 10 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-25 12:32:20,555] Trial 17 finished with value: 134.0840892791748 and parameters: {'seq_len': 24, 'hidden_size': 96, 'num_layers': 2, 'dropout': 0.019991406980689852, 'lr': 0.0007928137555241749, 'weight_decay': 3.3245558192360414e-05, 'batch_size': 128, 'use_fc_head': True, 'fc_hidden': 128}. Best is trial 11 with value: 133.9233112335205.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE=269588.5983 | Val avg MAE=152.2807\n",
      "Epoch 02 | Train MSE=41498.2649 | Val avg MAE=140.0074\n",
      "Epoch 03 | Train MSE=33254.9508 | Val avg MAE=140.0364\n",
      "Epoch 04 | Train MSE=30310.2022 | Val avg MAE=139.7786\n",
      "Epoch 05 | Train MSE=28381.0174 | Val avg MAE=138.2783\n",
      "Epoch 06 | Train MSE=26891.2090 | Val avg MAE=137.7808\n",
      "Epoch 07 | Train MSE=25760.9913 | Val avg MAE=137.1574\n",
      "Epoch 08 | Train MSE=24875.4676 | Val avg MAE=139.3918\n",
      "Epoch 09 | Train MSE=24089.7699 | Val avg MAE=141.2078\n",
      "Epoch 10 | Train MSE=23428.1948 | Val avg MAE=137.1876\n",
      "Epoch 11 | Train MSE=22888.2156 | Val avg MAE=138.1601\n",
      "Epoch 12 | Train MSE=22385.0849 | Val avg MAE=135.8554\n",
      "Epoch 13 | Train MSE=21934.3143 | Val avg MAE=136.2459\n",
      "Epoch 14 | Train MSE=21535.7674 | Val avg MAE=139.0741\n",
      "Epoch 15 | Train MSE=21165.1241 | Val avg MAE=139.7768\n",
      "Epoch 16 | Train MSE=20832.8565 | Val avg MAE=139.6542\n",
      "Epoch 17 | Train MSE=20553.9377 | Val avg MAE=137.2509\n",
      "Early stopping after 17 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-25 14:45:39,997] Trial 18 finished with value: 135.85537147521973 and parameters: {'seq_len': 24, 'hidden_size': 96, 'num_layers': 2, 'dropout': 0.001137797412376719, 'lr': 0.0007487024700864935, 'weight_decay': 4.0922455834938574e-05, 'batch_size': 256, 'use_fc_head': True, 'fc_hidden': 256}. Best is trial 11 with value: 133.9233112335205.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE=249533.8382 | Val avg MAE=151.4566\n",
      "Epoch 02 | Train MSE=42042.5544 | Val avg MAE=140.6876\n",
      "Epoch 03 | Train MSE=33383.5741 | Val avg MAE=138.1679\n",
      "Epoch 04 | Train MSE=30442.9111 | Val avg MAE=139.0662\n",
      "Epoch 05 | Train MSE=28588.7579 | Val avg MAE=136.8446\n",
      "Epoch 06 | Train MSE=27214.5432 | Val avg MAE=137.1700\n",
      "Epoch 07 | Train MSE=26143.2931 | Val avg MAE=140.4338\n",
      "Epoch 08 | Train MSE=25266.6812 | Val avg MAE=137.2312\n",
      "Epoch 09 | Train MSE=24531.9731 | Val avg MAE=137.8746\n",
      "Epoch 10 | Train MSE=23888.2900 | Val avg MAE=138.1539\n",
      "Early stopping after 10 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-25 16:10:17,017] Trial 19 finished with value: 136.8446216583252 and parameters: {'seq_len': 24, 'hidden_size': 96, 'num_layers': 2, 'dropout': 0.029247727208351795, 'lr': 0.0006240517598688299, 'weight_decay': 3.4872387954872066e-05, 'batch_size': 128, 'use_fc_head': True, 'fc_hidden': 128}. Best is trial 11 with value: 133.9233112335205.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE=554683.0875 | Val avg MAE=160.7711\n",
      "Epoch 02 | Train MSE=64326.0953 | Val avg MAE=154.1983\n",
      "Epoch 03 | Train MSE=45511.7942 | Val avg MAE=145.8251\n",
      "Epoch 04 | Train MSE=39735.4932 | Val avg MAE=139.6499\n",
      "Epoch 05 | Train MSE=34901.3491 | Val avg MAE=140.1053\n",
      "Epoch 06 | Train MSE=31977.9353 | Val avg MAE=139.1503\n",
      "Epoch 07 | Train MSE=30406.6757 | Val avg MAE=136.2493\n",
      "Epoch 08 | Train MSE=29271.8935 | Val avg MAE=137.0428\n",
      "Epoch 09 | Train MSE=28326.5109 | Val avg MAE=137.7484\n",
      "Epoch 10 | Train MSE=27476.6941 | Val avg MAE=138.8310\n",
      "Epoch 11 | Train MSE=26803.2081 | Val avg MAE=138.8510\n",
      "Epoch 12 | Train MSE=26119.5104 | Val avg MAE=136.1509\n",
      "Epoch 13 | Train MSE=25562.8532 | Val avg MAE=133.7383\n",
      "Epoch 14 | Train MSE=25108.9810 | Val avg MAE=137.7418\n",
      "Epoch 15 | Train MSE=24632.9182 | Val avg MAE=137.9981\n",
      "Epoch 16 | Train MSE=24252.1654 | Val avg MAE=139.5212\n",
      "Epoch 17 | Train MSE=23850.0891 | Val avg MAE=138.8706\n",
      "Epoch 18 | Train MSE=23517.8079 | Val avg MAE=137.2863\n",
      "Early stopping after 18 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-25 18:41:04,499] Trial 20 finished with value: 133.73832321166992 and parameters: {'seq_len': 24, 'hidden_size': 96, 'num_layers': 2, 'dropout': 0.08480346545321482, 'lr': 0.000338292769713216, 'weight_decay': 1.2011095009674564e-05, 'batch_size': 128, 'use_fc_head': True, 'fc_hidden': 64}. Best is trial 20 with value: 133.73832321166992.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE=585535.0752 | Val avg MAE=166.8651\n",
      "Epoch 02 | Train MSE=65895.4729 | Val avg MAE=157.2872\n",
      "Epoch 03 | Train MSE=46761.0052 | Val avg MAE=146.9327\n",
      "Epoch 04 | Train MSE=39942.3379 | Val avg MAE=141.2341\n",
      "Epoch 05 | Train MSE=35508.7107 | Val avg MAE=140.5251\n",
      "Epoch 06 | Train MSE=32247.4221 | Val avg MAE=142.0000\n",
      "Epoch 07 | Train MSE=30543.4851 | Val avg MAE=138.2015\n",
      "Epoch 08 | Train MSE=29319.3976 | Val avg MAE=139.6683\n",
      "Epoch 09 | Train MSE=28354.8918 | Val avg MAE=138.6163\n",
      "Epoch 10 | Train MSE=27543.4809 | Val avg MAE=138.6303\n",
      "Epoch 11 | Train MSE=26798.6479 | Val avg MAE=139.5450\n",
      "Epoch 12 | Train MSE=26182.1884 | Val avg MAE=141.1644\n",
      "Early stopping after 12 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-25 20:21:04,515] Trial 21 finished with value: 138.2014617919922 and parameters: {'seq_len': 24, 'hidden_size': 96, 'num_layers': 2, 'dropout': 0.07707647921838799, 'lr': 0.00031349596209201923, 'weight_decay': 1.0666337097056402e-05, 'batch_size': 128, 'use_fc_head': True, 'fc_hidden': 64}. Best is trial 20 with value: 133.73832321166992.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE=877593.6039 | Val avg MAE=181.1512\n",
      "Epoch 02 | Train MSE=72007.5903 | Val avg MAE=159.7414\n",
      "Epoch 03 | Train MSE=58477.7291 | Val avg MAE=151.7993\n",
      "Epoch 04 | Train MSE=44392.1048 | Val avg MAE=144.7759\n",
      "Epoch 05 | Train MSE=39718.0342 | Val avg MAE=145.5488\n",
      "Epoch 06 | Train MSE=36305.6999 | Val avg MAE=141.3870\n",
      "Epoch 07 | Train MSE=33418.1315 | Val avg MAE=140.9053\n",
      "Epoch 08 | Train MSE=31482.3934 | Val avg MAE=140.0561\n",
      "Epoch 09 | Train MSE=30217.2041 | Val avg MAE=139.2312\n",
      "Epoch 10 | Train MSE=29206.4650 | Val avg MAE=140.3888\n",
      "Epoch 11 | Train MSE=28349.3728 | Val avg MAE=139.9558\n",
      "Epoch 12 | Train MSE=27613.6639 | Val avg MAE=138.5795\n",
      "Epoch 13 | Train MSE=26968.1051 | Val avg MAE=141.0058\n",
      "Epoch 14 | Train MSE=26390.7292 | Val avg MAE=138.4772\n",
      "Epoch 15 | Train MSE=25876.3125 | Val avg MAE=139.4117\n",
      "Epoch 16 | Train MSE=25399.6693 | Val avg MAE=137.7891\n",
      "Epoch 17 | Train MSE=24981.4605 | Val avg MAE=139.0874\n",
      "Epoch 18 | Train MSE=24565.8403 | Val avg MAE=138.9034\n",
      "Epoch 19 | Train MSE=24194.5789 | Val avg MAE=138.5986\n",
      "Epoch 20 | Train MSE=23849.6977 | Val avg MAE=138.9162\n",
      "Epoch 21 | Train MSE=23540.8931 | Val avg MAE=140.0784\n",
      "Early stopping after 21 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-25 23:14:56,188] Trial 22 finished with value: 137.78911018371582 and parameters: {'seq_len': 24, 'hidden_size': 96, 'num_layers': 2, 'dropout': 0.03161031920215632, 'lr': 0.00020566058345911938, 'weight_decay': 3.075360598420957e-05, 'batch_size': 128, 'use_fc_head': True, 'fc_hidden': 64}. Best is trial 20 with value: 133.73832321166992.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE=480533.7542 | Val avg MAE=159.5552\n",
      "Epoch 02 | Train MSE=63085.2855 | Val avg MAE=151.6738\n",
      "Epoch 03 | Train MSE=43859.6236 | Val avg MAE=148.2107\n",
      "Epoch 04 | Train MSE=38431.7994 | Val avg MAE=139.9894\n",
      "Epoch 05 | Train MSE=33561.1548 | Val avg MAE=139.5879\n",
      "Epoch 06 | Train MSE=30972.9872 | Val avg MAE=137.0133\n",
      "Epoch 07 | Train MSE=29493.9253 | Val avg MAE=139.8050\n",
      "Epoch 08 | Train MSE=28395.8554 | Val avg MAE=138.1059\n",
      "Epoch 09 | Train MSE=27440.9224 | Val avg MAE=137.6333\n",
      "Epoch 10 | Train MSE=26673.6503 | Val avg MAE=137.3563\n",
      "Epoch 11 | Train MSE=26003.4750 | Val avg MAE=139.2148\n",
      "Early stopping after 11 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-26 00:46:51,383] Trial 23 finished with value: 137.01329231262207 and parameters: {'seq_len': 24, 'hidden_size': 96, 'num_layers': 2, 'dropout': 0.09143701048769956, 'lr': 0.0003771556682252897, 'weight_decay': 5.380047922465392e-05, 'batch_size': 128, 'use_fc_head': True, 'fc_hidden': 64}. Best is trial 20 with value: 133.73832321166992.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE=530149.1455 | Val avg MAE=165.6089\n",
      "Epoch 02 | Train MSE=66544.2266 | Val avg MAE=156.8574\n",
      "Epoch 03 | Train MSE=48563.7879 | Val avg MAE=146.0517\n",
      "Epoch 04 | Train MSE=40273.6336 | Val avg MAE=147.2529\n",
      "Epoch 05 | Train MSE=35917.4167 | Val avg MAE=142.9268\n",
      "Epoch 06 | Train MSE=32442.4993 | Val avg MAE=140.7843\n",
      "Epoch 07 | Train MSE=30701.1622 | Val avg MAE=140.8018\n",
      "Epoch 08 | Train MSE=29449.4534 | Val avg MAE=137.6203\n",
      "Epoch 09 | Train MSE=28492.0067 | Val avg MAE=140.5315\n",
      "Epoch 10 | Train MSE=27689.7940 | Val avg MAE=139.2095\n",
      "Epoch 11 | Train MSE=26961.4682 | Val avg MAE=137.8811\n",
      "Epoch 12 | Train MSE=26394.8429 | Val avg MAE=139.6529\n",
      "Epoch 13 | Train MSE=25891.1892 | Val avg MAE=139.0613\n",
      "Early stopping after 13 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-26 02:36:05,465] Trial 24 finished with value: 137.62031173706055 and parameters: {'seq_len': 24, 'hidden_size': 96, 'num_layers': 2, 'dropout': 0.11904364849762557, 'lr': 0.0003372805336706817, 'weight_decay': 1.1922593272521357e-05, 'batch_size': 128, 'use_fc_head': True, 'fc_hidden': 64}. Best is trial 20 with value: 133.73832321166992.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE=410829.5809 | Val avg MAE=160.3398\n",
      "Epoch 02 | Train MSE=54141.0181 | Val avg MAE=144.6755\n",
      "Epoch 03 | Train MSE=38263.7346 | Val avg MAE=141.4144\n",
      "Epoch 04 | Train MSE=31548.3921 | Val avg MAE=137.0187\n",
      "Epoch 05 | Train MSE=28498.0454 | Val avg MAE=140.7984\n",
      "Epoch 06 | Train MSE=26517.2340 | Val avg MAE=138.0628\n",
      "Epoch 07 | Train MSE=24951.4485 | Val avg MAE=137.1782\n",
      "Epoch 08 | Train MSE=23746.8050 | Val avg MAE=136.7450\n",
      "Epoch 09 | Train MSE=22708.1675 | Val avg MAE=136.1671\n",
      "Epoch 10 | Train MSE=21865.8651 | Val avg MAE=139.1691\n",
      "Epoch 11 | Train MSE=21144.4620 | Val avg MAE=137.5025\n",
      "Epoch 12 | Train MSE=20463.2155 | Val avg MAE=136.8236\n",
      "Epoch 13 | Train MSE=19860.6665 | Val avg MAE=138.2087\n",
      "Epoch 14 | Train MSE=19324.4626 | Val avg MAE=138.5233\n",
      "Early stopping after 14 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-26 07:30:42,165] Trial 25 finished with value: 136.16705131530762 and parameters: {'seq_len': 24, 'hidden_size': 192, 'num_layers': 2, 'dropout': 0.04681714065798248, 'lr': 0.00023396558657020624, 'weight_decay': 2.1270764380991602e-05, 'batch_size': 128, 'use_fc_head': True, 'fc_hidden': 128}. Best is trial 20 with value: 133.73832321166992.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE=2594287.6610 | Val avg MAE=774.2896\n",
      "Epoch 02 | Train MSE=2079851.5699 | Val avg MAE=670.2895\n",
      "Epoch 03 | Train MSE=1704799.7411 | Val avg MAE=603.5558\n",
      "Epoch 04 | Train MSE=1413996.6631 | Val avg MAE=551.0280\n",
      "Epoch 05 | Train MSE=1181365.9777 | Val avg MAE=501.7033\n",
      "Epoch 06 | Train MSE=991980.3836 | Val avg MAE=447.9650\n",
      "Epoch 07 | Train MSE=829857.9109 | Val avg MAE=400.1158\n",
      "Epoch 08 | Train MSE=691710.0024 | Val avg MAE=359.4363\n",
      "Epoch 09 | Train MSE=574815.1883 | Val avg MAE=322.4950\n",
      "Epoch 10 | Train MSE=477116.8730 | Val avg MAE=293.8452\n",
      "Epoch 11 | Train MSE=396168.2216 | Val avg MAE=269.3187\n",
      "Epoch 12 | Train MSE=329430.1058 | Val avg MAE=248.3990\n",
      "Epoch 13 | Train MSE=274505.6660 | Val avg MAE=231.6960\n",
      "Epoch 14 | Train MSE=229744.6464 | Val avg MAE=219.8270\n",
      "Epoch 15 | Train MSE=193271.3296 | Val avg MAE=204.8210\n",
      "Epoch 16 | Train MSE=163829.5451 | Val avg MAE=194.3529\n",
      "Epoch 17 | Train MSE=140165.8668 | Val avg MAE=187.0908\n",
      "Epoch 18 | Train MSE=121137.1418 | Val avg MAE=180.6481\n",
      "Epoch 19 | Train MSE=106073.5663 | Val avg MAE=173.7989\n",
      "Epoch 20 | Train MSE=94075.1802 | Val avg MAE=168.1308\n",
      "Epoch 21 | Train MSE=84615.8565 | Val avg MAE=166.7446\n",
      "Epoch 22 | Train MSE=77101.7820 | Val avg MAE=162.6989\n",
      "Epoch 23 | Train MSE=70778.1882 | Val avg MAE=160.9760\n",
      "Epoch 24 | Train MSE=65229.1909 | Val avg MAE=157.3317\n",
      "Epoch 25 | Train MSE=60198.5392 | Val avg MAE=155.9180\n",
      "Epoch 26 | Train MSE=55567.0725 | Val avg MAE=155.2141\n",
      "Epoch 27 | Train MSE=51463.7484 | Val avg MAE=152.8852\n",
      "Epoch 28 | Train MSE=47925.9863 | Val avg MAE=150.9884\n",
      "Epoch 29 | Train MSE=44897.4588 | Val avg MAE=150.6253\n",
      "Epoch 30 | Train MSE=42361.5868 | Val avg MAE=149.0466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-26 11:32:01,015] Trial 26 finished with value: 149.04664611816406 and parameters: {'seq_len': 24, 'hidden_size': 96, 'num_layers': 2, 'dropout': 0.02728113964811686, 'lr': 0.0005532141303222043, 'weight_decay': 4.398509361025365e-06, 'batch_size': 256, 'use_fc_head': False}. Best is trial 20 with value: 133.73832321166992.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE=656999.0207 | Val avg MAE=169.2577\n",
      "Epoch 02 | Train MSE=70752.3051 | Val avg MAE=158.9999\n",
      "Epoch 03 | Train MSE=53323.3128 | Val avg MAE=150.8716\n",
      "Epoch 04 | Train MSE=43824.1419 | Val avg MAE=146.6006\n",
      "Epoch 05 | Train MSE=40322.8206 | Val avg MAE=145.2796\n",
      "Epoch 06 | Train MSE=36666.6281 | Val avg MAE=142.0001\n",
      "Epoch 07 | Train MSE=33883.3068 | Val avg MAE=139.8238\n",
      "Epoch 08 | Train MSE=32251.3112 | Val avg MAE=140.3006\n",
      "Epoch 09 | Train MSE=31075.0233 | Val avg MAE=141.1307\n",
      "Epoch 10 | Train MSE=30117.2084 | Val avg MAE=137.9465\n",
      "Epoch 11 | Train MSE=29271.9109 | Val avg MAE=138.4800\n",
      "Epoch 12 | Train MSE=28615.4438 | Val avg MAE=139.1407\n",
      "Epoch 13 | Train MSE=27957.5859 | Val avg MAE=135.9024\n",
      "Epoch 14 | Train MSE=27411.0437 | Val avg MAE=138.4729\n",
      "Epoch 15 | Train MSE=26895.0710 | Val avg MAE=136.8523\n",
      "Epoch 16 | Train MSE=26443.9173 | Val avg MAE=138.3530\n",
      "Epoch 17 | Train MSE=25983.7561 | Val avg MAE=141.9597\n",
      "Epoch 18 | Train MSE=25625.2739 | Val avg MAE=139.5980\n",
      "Early stopping after 18 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-26 14:01:52,079] Trial 27 finished with value: 135.90236854553223 and parameters: {'seq_len': 24, 'hidden_size': 96, 'num_layers': 2, 'dropout': 0.1128549347970683, 'lr': 0.00018854976376279906, 'weight_decay': 7.689824682478167e-06, 'batch_size': 128, 'use_fc_head': True, 'fc_hidden': 128}. Best is trial 20 with value: 133.73832321166992.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE=334100.6909 | Val avg MAE=157.3085\n",
      "Epoch 02 | Train MSE=57346.2587 | Val avg MAE=145.6731\n",
      "Epoch 03 | Train MSE=35817.6100 | Val avg MAE=136.0894\n",
      "Epoch 04 | Train MSE=28246.4396 | Val avg MAE=137.6235\n",
      "Epoch 05 | Train MSE=25230.5820 | Val avg MAE=136.0355\n",
      "Epoch 06 | Train MSE=23394.0941 | Val avg MAE=136.9545\n",
      "Epoch 07 | Train MSE=21961.6955 | Val avg MAE=136.5899\n",
      "Epoch 08 | Train MSE=20831.7578 | Val avg MAE=135.5241\n",
      "Epoch 09 | Train MSE=19906.2713 | Val avg MAE=137.6516\n",
      "Epoch 10 | Train MSE=19083.7127 | Val avg MAE=134.9472\n",
      "Epoch 11 | Train MSE=18320.5443 | Val avg MAE=138.4048\n",
      "Epoch 12 | Train MSE=17727.0660 | Val avg MAE=135.7591\n",
      "Epoch 13 | Train MSE=17167.9684 | Val avg MAE=136.4588\n",
      "Epoch 14 | Train MSE=16655.6322 | Val avg MAE=136.6309\n",
      "Epoch 15 | Train MSE=16145.2935 | Val avg MAE=136.3602\n",
      "Early stopping after 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-26 22:20:10,886] Trial 28 finished with value: 134.94724082946777 and parameters: {'seq_len': 24, 'hidden_size': 192, 'num_layers': 3, 'dropout': 0.0708301469353607, 'lr': 0.0004458716164604057, 'weight_decay': 0.00017863974102696879, 'batch_size': 128, 'use_fc_head': True, 'fc_hidden': 64}. Best is trial 20 with value: 133.73832321166992.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train MSE=1135255.8818 | Val avg MAE=256.4803\n",
      "Epoch 02 | Train MSE=121816.9749 | Val avg MAE=182.6373\n",
      "Epoch 03 | Train MSE=75831.8101 | Val avg MAE=171.6249\n",
      "Epoch 04 | Train MSE=69872.0220 | Val avg MAE=169.4399\n",
      "Epoch 05 | Train MSE=66570.7077 | Val avg MAE=169.6648\n",
      "Epoch 06 | Train MSE=63870.9313 | Val avg MAE=164.9262\n",
      "Epoch 07 | Train MSE=57273.2796 | Val avg MAE=159.7262\n",
      "Epoch 08 | Train MSE=48040.1861 | Val avg MAE=157.0184\n",
      "Epoch 09 | Train MSE=43911.2673 | Val avg MAE=153.4394\n",
      "Epoch 10 | Train MSE=41806.8226 | Val avg MAE=153.6846\n",
      "Epoch 11 | Train MSE=40409.7152 | Val avg MAE=153.9473\n",
      "Epoch 12 | Train MSE=39288.1602 | Val avg MAE=152.4635\n",
      "Epoch 13 | Train MSE=38256.3853 | Val avg MAE=152.0113\n",
      "Epoch 14 | Train MSE=37314.9183 | Val avg MAE=151.4871\n",
      "Epoch 15 | Train MSE=36217.6673 | Val avg MAE=150.6400\n",
      "Epoch 16 | Train MSE=34706.0330 | Val avg MAE=146.3842\n",
      "Epoch 17 | Train MSE=33192.8685 | Val avg MAE=149.1845\n",
      "Epoch 18 | Train MSE=32110.3212 | Val avg MAE=145.1918\n",
      "Epoch 19 | Train MSE=31403.5772 | Val avg MAE=147.6125\n",
      "Epoch 20 | Train MSE=30793.5657 | Val avg MAE=146.9173\n",
      "Epoch 21 | Train MSE=30265.2895 | Val avg MAE=142.3441\n",
      "Epoch 22 | Train MSE=29854.3512 | Val avg MAE=144.5453\n",
      "Epoch 23 | Train MSE=29406.8255 | Val avg MAE=142.7035\n",
      "Epoch 24 | Train MSE=29066.2831 | Val avg MAE=145.0647\n",
      "Epoch 25 | Train MSE=28663.1995 | Val avg MAE=142.2108\n",
      "Epoch 26 | Train MSE=28357.8539 | Val avg MAE=142.8177\n",
      "Epoch 27 | Train MSE=28048.0057 | Val avg MAE=144.8232\n",
      "Epoch 28 | Train MSE=27765.5404 | Val avg MAE=144.4884\n",
      "Epoch 29 | Train MSE=27478.1690 | Val avg MAE=145.3475\n",
      "Epoch 30 | Train MSE=27250.9949 | Val avg MAE=143.8269\n",
      "Early stopping after 30 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-27 04:43:12,984] Trial 29 finished with value: 142.2108211517334 and parameters: {'seq_len': 48, 'hidden_size': 64, 'num_layers': 3, 'dropout': 0.1706751269281414, 'lr': 0.0001651476330840796, 'weight_decay': 6.0711908845500406e-05, 'batch_size': 256, 'use_fc_head': True, 'fc_hidden': 256}. Best is trial 20 with value: 133.73832321166992.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials: 30\n",
      "Best validation avg MAE: 133.73832321166992\n",
      "Best params:\n",
      "  seq_len: 24\n",
      "  hidden_size: 96\n",
      "  num_layers: 2\n",
      "  dropout: 0.08480346545321482\n",
      "  lr: 0.000338292769713216\n",
      "  weight_decay: 1.2011095009674564e-05\n",
      "  batch_size: 128\n",
      "  use_fc_head: True\n",
      "  fc_hidden: 64\n",
      "\n",
      "Best params (for final LSTM): {'seq_len': 24, 'hidden_size': 96, 'num_layers': 2, 'dropout': 0.08480346545321482, 'lr': 0.000338292769713216, 'weight_decay': 1.2011095009674564e-05, 'batch_size': 128, 'use_fc_head': True, 'fc_hidden': 64}\n",
      "Train+Val seq shape: (2078702, 24, 35)\n",
      "Test seq shape:     (1041748, 24, 35)\n",
      "[FINAL TRAIN] Epoch 01 | Train MSE=414814.1156\n",
      "[FINAL TRAIN] Epoch 02 | Train MSE=63688.5856\n",
      "[FINAL TRAIN] Epoch 03 | Train MSE=45559.1439\n",
      "[FINAL TRAIN] Epoch 04 | Train MSE=39566.3001\n",
      "[FINAL TRAIN] Epoch 05 | Train MSE=34572.0758\n",
      "[FINAL TRAIN] Epoch 06 | Train MSE=31822.1935\n",
      "[FINAL TRAIN] Epoch 07 | Train MSE=30127.4876\n",
      "[FINAL TRAIN] Epoch 08 | Train MSE=28936.3562\n",
      "[FINAL TRAIN] Epoch 09 | Train MSE=27915.9973\n",
      "[FINAL TRAIN] Epoch 10 | Train MSE=27109.6852\n",
      "[FINAL TRAIN] Epoch 11 | Train MSE=26446.2620\n",
      "[FINAL TRAIN] Epoch 12 | Train MSE=25821.9063\n",
      "[FINAL TRAIN] Epoch 13 | Train MSE=25323.2299\n",
      "[FINAL TRAIN] Epoch 14 | Train MSE=24852.9623\n",
      "[FINAL TRAIN] Epoch 15 | Train MSE=24418.0165\n",
      "[FINAL TRAIN] Epoch 16 | Train MSE=24058.0442\n",
      "[FINAL TRAIN] Epoch 17 | Train MSE=23689.5852\n",
      "[FINAL TRAIN] Epoch 18 | Train MSE=23383.9196\n",
      "[FINAL TRAIN] Epoch 19 | Train MSE=23117.9944\n",
      "[FINAL TRAIN] Epoch 20 | Train MSE=22867.2997\n",
      "[FINAL TRAIN] Epoch 21 | Train MSE=22624.6385\n",
      "[FINAL TRAIN] Epoch 22 | Train MSE=22358.8947\n",
      "[FINAL TRAIN] Epoch 23 | Train MSE=22162.1863\n",
      "[FINAL TRAIN] Epoch 24 | Train MSE=21951.4550\n",
      "[FINAL TRAIN] Epoch 25 | Train MSE=21783.6061\n",
      "[FINAL TRAIN] Epoch 26 | Train MSE=21596.6504\n",
      "[FINAL TRAIN] Epoch 27 | Train MSE=21420.3559\n",
      "[FINAL TRAIN] Epoch 28 | Train MSE=21273.3584\n",
      "[FINAL TRAIN] Epoch 29 | Train MSE=21124.4981\n",
      "[FINAL TRAIN] Epoch 30 | Train MSE=21000.6523\n",
      "\n",
      "=== Final Test performance (LSTM MIMO, Optuna-tuned, numeric features) ===\n",
      "Horizon 12h: MAE=112.656, RMSE=237.732\n",
      "Horizon 24h: MAE=111.908, RMSE=237.104\n",
      "Horizon 48h: MAE=123.315, RMSE=267.160\n",
      "Horizon 72h: MAE=131.246, RMSE=278.011\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 1. Imports and basic setup\n",
    "# ============================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import math\n",
    "\n",
    "import optuna\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 2. Multi-horizon targets (MIMO) per station\n",
    "# ============================================\n",
    "HORIZONS = [12, 24, 48, 72]  # hours ahead\n",
    "TARGET_COLS = [f\"y_{h}\" for h in HORIZONS]\n",
    "\n",
    "def add_mimo_targets(df, target=\"total_flow\"):\n",
    "    \"\"\"\n",
    "    For each station, create future total_flow targets:\n",
    "    y_12, y_24, y_48, y_72 hours ahead.\n",
    "    \"\"\"\n",
    "    df = df.sort_values([\"station\", \"timestamp\"]).copy()\n",
    "    for h in HORIZONS:\n",
    "        df[f\"y_{h}\"] = df.groupby(\"station\")[target].shift(-h)\n",
    "    return df\n",
    "\n",
    "df_train_m = add_mimo_targets(df_train)\n",
    "df_val_m   = add_mimo_targets(df_val)\n",
    "df_test_m  = add_mimo_targets(df_test)\n",
    "\n",
    "# Drop rows that do not have all horizons available\n",
    "df_train_m = df_train_m.dropna(subset=TARGET_COLS).copy()\n",
    "df_val_m   = df_val_m.dropna(subset=TARGET_COLS).copy()\n",
    "df_test_m  = df_test_m.dropna(subset=TARGET_COLS).copy()\n",
    "\n",
    "print(\"Train rows (LSTM):\", len(df_train_m))\n",
    "print(\"Val rows   (LSTM):\", len(df_val_m))\n",
    "print(\"Test rows  (LSTM):\", len(df_test_m))\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 3. Define numeric feature set for LSTM\n",
    "# ============================================\n",
    "numeric_cols = df_train_m.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "exclude_cols = set(TARGET_COLS)\n",
    "for col in [\"station\", \"id\", \"ID\", \"MS ID\", \"MS_ID\"]:\n",
    "    if col in numeric_cols:\n",
    "        exclude_cols.add(col)\n",
    "\n",
    "SEQ_FEATURES = [c for c in numeric_cols if c not in exclude_cols]\n",
    "\n",
    "print(\"Number of LSTM numeric features:\", len(SEQ_FEATURES))\n",
    "print(\"Example features:\", SEQ_FEATURES[:20])\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 4. Handle NaNs/inf in features + Standardize (FIXED)\n",
    "# ============================================\n",
    "# 4.1 Replace inf with NaN (critical)\n",
    "for d in [df_train_m, df_val_m, df_test_m]:\n",
    "    d[SEQ_FEATURES] = d[SEQ_FEATURES].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# 4.2 Compute train means (may contain NaN if a feature is all-NaN in train)\n",
    "train_means = df_train_m[SEQ_FEATURES].mean()\n",
    "\n",
    "# 4.3 Drop features that are completely NaN in train (cannot be imputed)\n",
    "bad_all_nan = train_means.index[train_means.isna()].tolist()\n",
    "if bad_all_nan:\n",
    "    print(\"Dropping all-NaN features (train):\", bad_all_nan[:20], \"...\" if len(bad_all_nan) > 20 else \"\")\n",
    "    SEQ_FEATURES = [c for c in SEQ_FEATURES if c not in bad_all_nan]\n",
    "    train_means = df_train_m[SEQ_FEATURES].mean()\n",
    "\n",
    "# 4.4 Impute using train means\n",
    "for d in [df_train_m, df_val_m, df_test_m]:\n",
    "    d[SEQ_FEATURES] = d[SEQ_FEATURES].fillna(train_means)\n",
    "\n",
    "# 4.5 Hard safety check before scaling\n",
    "def assert_finite_df(name, d, cols):\n",
    "    x = d[cols].to_numpy()\n",
    "    if not np.isfinite(x).all():\n",
    "        bad = np.argwhere(~np.isfinite(x))\n",
    "        r, c = bad[0]\n",
    "        raise ValueError(f\"{name}: non-finite value in column '{cols[c]}' at row index {r}\")\n",
    "\n",
    "assert_finite_df(\"train features\", df_train_m, SEQ_FEATURES)\n",
    "assert_finite_df(\"val features\",   df_val_m,   SEQ_FEATURES)\n",
    "assert_finite_df(\"test features\",  df_test_m,  SEQ_FEATURES)\n",
    "\n",
    "# 4.6 Scale (fit on train only)\n",
    "scaler_seq = StandardScaler()\n",
    "scaler_seq.fit(df_train_m[SEQ_FEATURES])\n",
    "\n",
    "for d in [df_train_m, df_val_m, df_test_m]:\n",
    "    d[SEQ_FEATURES] = scaler_seq.transform(d[SEQ_FEATURES])\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 5. Sequence builder (per station, variable seq_len)\n",
    "# ============================================\n",
    "def build_sequences(df, seq_features, target_cols, seq_len=24):\n",
    "    \"\"\"\n",
    "    df must contain: 'station', 'timestamp', seq_features, target_cols\n",
    "    Returns:\n",
    "        X: (n_samples, seq_len, n_features)\n",
    "        y: (n_samples, n_targets)\n",
    "    \"\"\"\n",
    "    df = df.sort_values([\"station\", \"timestamp\"]).copy()\n",
    "\n",
    "    X_list = []\n",
    "    Y_list = []\n",
    "\n",
    "    for station_id, g in df.groupby(\"station\"):\n",
    "        g = g.reset_index(drop=True)\n",
    "\n",
    "        feat_mat = g[seq_features].values   # (T, F)\n",
    "        targ_mat = g[target_cols].values    # (T, H)\n",
    "\n",
    "        T = len(g)\n",
    "        if T < seq_len:\n",
    "            continue\n",
    "\n",
    "        for end_idx in range(seq_len - 1, T):\n",
    "            # targets should be clean, but keep guard\n",
    "            if np.isnan(targ_mat[end_idx]).any():\n",
    "                continue\n",
    "\n",
    "            start_idx = end_idx - seq_len + 1\n",
    "            X_seq = feat_mat[start_idx:end_idx + 1]   # (seq_len, F)\n",
    "            y_vec = targ_mat[end_idx]                 # (H,)\n",
    "\n",
    "            X_list.append(X_seq)\n",
    "            Y_list.append(y_vec)\n",
    "\n",
    "    if not X_list:\n",
    "        raise ValueError(\"No sequences created – adjust seq_len or check data.\")\n",
    "\n",
    "    X = np.stack(X_list, axis=0)\n",
    "    y = np.stack(Y_list, axis=0)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 6. Metrics helpers\n",
    "# ============================================\n",
    "def eval_loader(model, loader, device):\n",
    "    model.eval()\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            y_hat = model(xb)\n",
    "            all_true.append(yb.detach().cpu().numpy())\n",
    "            all_pred.append(y_hat.detach().cpu().numpy())\n",
    "    y_true = np.concatenate(all_true, axis=0)\n",
    "    y_pred = np.concatenate(all_pred, axis=0)\n",
    "    return y_true, y_pred\n",
    "\n",
    "def compute_mae_rmse_by_horizon(y_true, y_pred, horizons):\n",
    "    mae_list = []\n",
    "    rmse_list = []\n",
    "    for i, h in enumerate(horizons):\n",
    "        mae = mean_absolute_error(y_true[:, i], y_pred[:, i])\n",
    "        rmse = math.sqrt(mean_squared_error(y_true[:, i], y_pred[:, i]))\n",
    "        mae_list.append(mae)\n",
    "        rmse_list.append(rmse)\n",
    "    return mae_list, rmse_list\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 7. LSTM MIMO model (with optional FC head)\n",
    "# ============================================\n",
    "class LSTMMIMO(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size=128,\n",
    "        num_layers=2,\n",
    "        dropout=0.2,\n",
    "        n_outputs=4,\n",
    "        fc_hidden=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "\n",
    "        self.use_fc_hidden = fc_hidden is not None\n",
    "        if not self.use_fc_hidden:\n",
    "            self.fc = nn.Linear(hidden_size, n_outputs)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(hidden_size, fc_hidden)\n",
    "            self.act = nn.ReLU()\n",
    "            self.fc2 = nn.Linear(fc_hidden, n_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h_n, _) = self.lstm(x)   # h_n: (num_layers, batch, hidden)\n",
    "        last_hidden = h_n[-1]        # (batch, hidden)\n",
    "\n",
    "        if not self.use_fc_hidden:\n",
    "            return self.fc(last_hidden)\n",
    "        else:\n",
    "            z = self.fc1(last_hidden)\n",
    "            z = self.act(z)\n",
    "            return self.fc2(z)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 8. Training one config (for Optuna) (FIXED: grad clip + NaN guard)\n",
    "# ============================================\n",
    "def train_lstm_one_config(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    input_size,\n",
    "    n_outputs,\n",
    "    hidden_size,\n",
    "    num_layers,\n",
    "    dropout,\n",
    "    lr,\n",
    "    weight_decay,\n",
    "    batch_size,\n",
    "    max_epochs=30,\n",
    "    patience=5,\n",
    "    fc_hidden=None,\n",
    "):\n",
    "    X_train_t = torch.from_numpy(X_train).float()\n",
    "    y_train_t = torch.from_numpy(y_train).float()\n",
    "    X_val_t   = torch.from_numpy(X_val).float()\n",
    "    y_val_t   = torch.from_numpy(y_val).float()\n",
    "\n",
    "    train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "    val_ds   = TensorDataset(X_val_t,   y_val_t)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  drop_last=False)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    model = LSTMMIMO(\n",
    "        input_size=input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout,\n",
    "        n_outputs=n_outputs,\n",
    "        fc_hidden=fc_hidden,\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_metric = float(\"inf\")\n",
    "    best_state = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            y_hat = model(xb)\n",
    "\n",
    "            # NaN/inf guard (fail fast)\n",
    "            if torch.isnan(y_hat).any() or torch.isinf(y_hat).any():\n",
    "                raise ValueError(\"Model output became NaN/inf during training.\")\n",
    "\n",
    "            loss = criterion(y_hat, yb)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping (stabilizes LSTM training)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        train_loss = running_loss / max(n_batches, 1)\n",
    "\n",
    "        # validation\n",
    "        y_val_true, y_val_pred = eval_loader(model, val_loader, device)\n",
    "\n",
    "        # Guard before sklearn metrics\n",
    "        if (not np.isfinite(y_val_true).all()) or (not np.isfinite(y_val_pred).all()):\n",
    "            raise ValueError(\"Non-finite values in y_val_true/y_val_pred before MAE/RMSE.\")\n",
    "\n",
    "        mae_list, _ = compute_mae_rmse_by_horizon(y_val_true, y_val_pred, HORIZONS)\n",
    "        avg_val_mae = float(np.mean(mae_list))\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | Train MSE={train_loss:.4f} | Val avg MAE={avg_val_mae:.4f}\")\n",
    "\n",
    "        if avg_val_mae < best_val_metric - 1e-3:\n",
    "            best_val_metric = avg_val_mae\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping after {epoch} epochs.\")\n",
    "            break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model, best_val_metric\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 9. Training final model (no early stopping) (FIXED: grad clip + NaN guard)\n",
    "# ============================================\n",
    "def train_lstm_final(\n",
    "    X_train, y_train,\n",
    "    input_size,\n",
    "    n_outputs,\n",
    "    hidden_size,\n",
    "    num_layers,\n",
    "    dropout,\n",
    "    lr,\n",
    "    weight_decay,\n",
    "    batch_size,\n",
    "    max_epochs=30,\n",
    "    fc_hidden=None,\n",
    "):\n",
    "    X_train_t = torch.from_numpy(X_train).float()\n",
    "    y_train_t = torch.from_numpy(y_train).float()\n",
    "    train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "    model = LSTMMIMO(\n",
    "        input_size=input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout,\n",
    "        n_outputs=n_outputs,\n",
    "        fc_hidden=fc_hidden,\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            y_hat = model(xb)\n",
    "\n",
    "            if torch.isnan(y_hat).any() or torch.isinf(y_hat).any():\n",
    "                raise ValueError(\"Model output became NaN/inf during final training.\")\n",
    "\n",
    "            loss = criterion(y_hat, yb)\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        train_loss = running_loss / max(n_batches, 1)\n",
    "        print(f\"[FINAL TRAIN] Epoch {epoch:02d} | Train MSE={train_loss:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 10. Optuna objective (hyperparameter search) (FIXED: safer LR + deprecations)\n",
    "# ============================================\n",
    "def assert_finite_array(name, a):\n",
    "    a = np.asarray(a)\n",
    "    if not np.isfinite(a).all():\n",
    "        idx = np.argwhere(~np.isfinite(a))[0]\n",
    "        raise ValueError(f\"{name} has NaN/inf at index {tuple(idx)}\")\n",
    "\n",
    "def objective(trial):\n",
    "    seq_len      = trial.suggest_categorical(\"seq_len\", [24, 48])\n",
    "    hidden_size  = trial.suggest_categorical(\"hidden_size\", [64, 96, 128, 192])\n",
    "    num_layers   = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout      = trial.suggest_float(\"dropout\", 0.0, 0.3)\n",
    "\n",
    "    # safer ranges + no deprecation warnings\n",
    "    lr           = trial.suggest_float(\"lr\", 1e-4, 8e-4, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 5e-4, log=True)\n",
    "\n",
    "    batch_size   = trial.suggest_categorical(\"batch_size\", [128, 256])\n",
    "\n",
    "    use_fc_head  = trial.suggest_categorical(\"use_fc_head\", [False, True])\n",
    "    fc_hidden    = None\n",
    "    if use_fc_head:\n",
    "        fc_hidden = trial.suggest_categorical(\"fc_hidden\", [64, 128, 256])\n",
    "\n",
    "    # build sequences\n",
    "    X_train_seq, y_train_seq = build_sequences(df_train_m, SEQ_FEATURES, TARGET_COLS, seq_len=seq_len)\n",
    "    X_val_seq,   y_val_seq   = build_sequences(df_val_m,   SEQ_FEATURES, TARGET_COLS, seq_len=seq_len)\n",
    "\n",
    "    # finiteness checks\n",
    "    assert_finite_array(\"X_train_seq\", X_train_seq)\n",
    "    assert_finite_array(\"y_train_seq\", y_train_seq)\n",
    "    assert_finite_array(\"X_val_seq\",   X_val_seq)\n",
    "    assert_finite_array(\"y_val_seq\",   y_val_seq)\n",
    "\n",
    "    input_size = X_train_seq.shape[2]\n",
    "    n_outputs  = y_train_seq.shape[1]\n",
    "\n",
    "    try:\n",
    "        _, best_val_mae = train_lstm_one_config(\n",
    "            X_train_seq, y_train_seq,\n",
    "            X_val_seq,   y_val_seq,\n",
    "            input_size=input_size,\n",
    "            n_outputs=n_outputs,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay,\n",
    "            batch_size=batch_size,\n",
    "            max_epochs=30,\n",
    "            patience=5,\n",
    "            fc_hidden=fc_hidden,\n",
    "        )\n",
    "        return best_val_mae\n",
    "    except ValueError as e:\n",
    "        # prune NaN/inf configs instead of killing the whole study\n",
    "        msg = str(e).lower()\n",
    "        if (\"nan\" in msg) or (\"inf\" in msg) or (\"non-finite\" in msg):\n",
    "            raise optuna.TrialPruned(str(e))\n",
    "        raise\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 11. Run Optuna study\n",
    "# ============================================\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "print(\"Number of finished trials:\", len(study.trials))\n",
    "print(\"Best validation avg MAE:\", study.best_value)\n",
    "print(\"Best params:\")\n",
    "for k, v in study.best_trial.params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 12. Retrain best LSTM on train+val and evaluate on test\n",
    "# ============================================\n",
    "best_params = study.best_trial.params\n",
    "print(\"\\nBest params (for final LSTM):\", best_params)\n",
    "\n",
    "best_seq_len   = best_params[\"seq_len\"]\n",
    "best_hidden    = best_params[\"hidden_size\"]\n",
    "best_layers    = best_params[\"num_layers\"]\n",
    "best_dropout   = best_params[\"dropout\"]\n",
    "best_lr        = best_params[\"lr\"]\n",
    "best_wd        = best_params[\"weight_decay\"]\n",
    "best_batch     = best_params[\"batch_size\"]\n",
    "best_use_fc    = best_params[\"use_fc_head\"]\n",
    "best_fc_hidden = best_params.get(\"fc_hidden\", None) if best_use_fc else None\n",
    "\n",
    "# Combine train+val\n",
    "df_trainval_m = pd.concat([df_train_m, df_val_m], axis=0)\n",
    "\n",
    "X_trainval_seq, y_trainval_seq = build_sequences(df_trainval_m, SEQ_FEATURES, TARGET_COLS, seq_len=best_seq_len)\n",
    "X_test_seq,     y_test_seq     = build_sequences(df_test_m,     SEQ_FEATURES, TARGET_COLS, seq_len=best_seq_len)\n",
    "\n",
    "# Final sanity checks\n",
    "assert_finite_array(\"X_trainval_seq\", X_trainval_seq)\n",
    "assert_finite_array(\"y_trainval_seq\", y_trainval_seq)\n",
    "assert_finite_array(\"X_test_seq\",     X_test_seq)\n",
    "assert_finite_array(\"y_test_seq\",     y_test_seq)\n",
    "\n",
    "print(\"Train+Val seq shape:\", X_trainval_seq.shape)\n",
    "print(\"Test seq shape:    \", X_test_seq.shape)\n",
    "\n",
    "input_size = X_trainval_seq.shape[2]\n",
    "n_outputs  = y_trainval_seq.shape[1]\n",
    "\n",
    "best_lstm_model = train_lstm_final(\n",
    "    X_trainval_seq, y_trainval_seq,\n",
    "    input_size=input_size,\n",
    "    n_outputs=n_outputs,\n",
    "    hidden_size=best_hidden,\n",
    "    num_layers=best_layers,\n",
    "    dropout=best_dropout,\n",
    "    lr=best_lr,\n",
    "    weight_decay=best_wd,\n",
    "    batch_size=best_batch,\n",
    "    max_epochs=30,\n",
    "    fc_hidden=best_fc_hidden,\n",
    ")\n",
    "\n",
    "# Evaluate on test\n",
    "X_test_t = torch.from_numpy(X_test_seq).float()\n",
    "y_test_t = torch.from_numpy(y_test_seq).float()\n",
    "test_ds  = TensorDataset(X_test_t, y_test_t)\n",
    "test_loader = DataLoader(test_ds, batch_size=best_batch, shuffle=False, drop_last=False)\n",
    "\n",
    "y_test_true, y_test_pred = eval_loader(best_lstm_model, test_loader, device)\n",
    "\n",
    "if (not np.isfinite(y_test_true).all()) or (not np.isfinite(y_test_pred).all()):\n",
    "    raise ValueError(\"Non-finite values in y_test_true/y_test_pred before MAE/RMSE.\")\n",
    "\n",
    "mae_list, rmse_list = compute_mae_rmse_by_horizon(y_test_true, y_test_pred, HORIZONS)\n",
    "\n",
    "print(\"\\n=== Final Test performance (LSTM MIMO, Optuna-tuned, numeric features) ===\")\n",
    "for i, h in enumerate(HORIZONS):\n",
    "    print(f\"Horizon {h:>2}h: MAE={mae_list[i]:.3f}, RMSE={rmse_list[i]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0856de-04c3-468a-ac99-2bbf8e835b93",
   "metadata": {},
   "source": [
    "# STGCN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "061cb6b8-e66d-49d1-ad05-8b68546527b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes (stations): 1766\n",
      "Adjacency built.\n",
      "A shape: (1766, 1766) A_hat shape: (1766, 1766)\n",
      "Edges (excluding self-loops): 1723\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Build graph objects for STGCN\n",
    "# =========================\n",
    "\n",
    "# Decide where to build the graph from (use TRAIN only)\n",
    "df_graph = df_train.copy()\n",
    "\n",
    "# Sanity check: required columns\n",
    "required_cols = [\"station\", \"upstream_station\", \"downstream_station\"]\n",
    "missing = [c for c in required_cols if c not in df_graph.columns]\n",
    "if missing:\n",
    "    raise ValueError(\n",
    "        f\"Missing columns in df_train needed for adjacency: {missing}\\n\"\n",
    "        \"Make sure you already created upstream_station/downstream_station.\"\n",
    "    )\n",
    "\n",
    "# Fix dtype + drop bad station ids\n",
    "df_graph[\"station\"] = pd.to_numeric(df_graph[\"station\"], errors=\"coerce\")\n",
    "df_graph[\"upstream_station\"] = pd.to_numeric(df_graph[\"upstream_station\"], errors=\"coerce\")\n",
    "df_graph[\"downstream_station\"] = pd.to_numeric(df_graph[\"downstream_station\"], errors=\"coerce\")\n",
    "\n",
    "df_graph = df_graph.dropna(subset=[\"station\"]).copy()\n",
    "df_graph[\"station\"] = df_graph[\"station\"].astype(int)\n",
    "\n",
    "# Create a GLOBAL station list and mapping (node ordering)\n",
    "stations = np.sort(df_graph[\"station\"].unique())\n",
    "N = len(stations)\n",
    "station2idx = {s: i for i, s in enumerate(stations)}\n",
    "idx2station = {i: s for s, i in station2idx.items()}\n",
    "\n",
    "print(\"Number of nodes (stations):\", N)\n",
    "\n",
    "# Build adjacency matrix A (N x N)\n",
    "#    We'll add edges station <-> upstream and station <-> downstream (bidirectional)\n",
    "A = np.zeros((N, N), dtype=np.float32)\n",
    "\n",
    "# self-loops (optional here; STGCN often uses normalized adjacency with self-loop anyway)\n",
    "np.fill_diagonal(A, 1.0)\n",
    "\n",
    "# Get unique neighbor relationships per station to avoid repeated work\n",
    "edges_df = (\n",
    "    df_graph[[\"station\", \"upstream_station\", \"downstream_station\"]]\n",
    "    .drop_duplicates()\n",
    ")\n",
    "\n",
    "for _, row in edges_df.iterrows():\n",
    "    s = int(row[\"station\"])\n",
    "    i = station2idx.get(s, None)\n",
    "    if i is None:\n",
    "        continue\n",
    "\n",
    "    # upstream edge\n",
    "    u = row[\"upstream_station\"]\n",
    "    if not pd.isna(u):\n",
    "        u = int(u)\n",
    "        j = station2idx.get(u, None)\n",
    "        if j is not None:\n",
    "            A[i, j] = 1.0\n",
    "            A[j, i] = 1.0  # bidirectional\n",
    "\n",
    "    # downstream edge\n",
    "    d = row[\"downstream_station\"]\n",
    "    if not pd.isna(d):\n",
    "        d = int(d)\n",
    "        j = station2idx.get(d, None)\n",
    "        if j is not None:\n",
    "            A[i, j] = 1.0\n",
    "            A[j, i] = 1.0  # bidirectional\n",
    "\n",
    "# GCN-normalized adjacency (common for STGCN)\n",
    "#    A_hat = D^{-1/2} (A) D^{-1/2}\n",
    "deg = A.sum(axis=1)\n",
    "deg_inv_sqrt = np.power(deg, -0.5, where=deg>0)\n",
    "deg_inv_sqrt[~np.isfinite(deg_inv_sqrt)] = 0.0\n",
    "\n",
    "D_inv_sqrt = np.diag(deg_inv_sqrt.astype(np.float32))\n",
    "A_hat = D_inv_sqrt @ A @ D_inv_sqrt\n",
    "\n",
    "print(\"Adjacency built.\")\n",
    "print(\"A shape:\", A.shape, \"A_hat shape:\", A_hat.shape)\n",
    "print(\"Edges (excluding self-loops):\", int((A.sum() - np.trace(A)) / 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5709933b-2e59-4516-bbf0-fc7f18d34420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph nodes (station2idx): 1766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_22372\\1882305918.py:40: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df[\"timestamp\"] = df[\"timestamp\"].dt.floor(\"H\")\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_22372\\1882305918.py:40: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df[\"timestamp\"] = df[\"timestamp\"].dt.floor(\"H\")\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_22372\\1882305918.py:40: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df[\"timestamp\"] = df[\"timestamp\"].dt.floor(\"H\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw shapes (T, N, F):\n",
      "Train: (1069, 1766, 1) Val: (325, 1766, 1) Test: (709, 1766, 1)\n",
      "Windowed shapes:\n",
      "Xw_train: (1033, 1, 24, 1766) yw_train: (1033, 1766)\n",
      "Xw_val  : (289, 1, 24, 1766) yw_val  : (289, 1766)\n",
      "Xw_test : (673, 1, 24, 1766) yw_test : (673, 1766)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "#  Build STGCN tensors (X, y) from df_train/val/test\n",
    "# =========================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# -------------------------\n",
    "# Settings you control\n",
    "# -------------------------\n",
    "INPUT_LEN = 24          # past 24 hours\n",
    "HORIZON = 12            # predict t + 12 hours (your Option B base model)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "TARGET_COL = \"total_flow\"\n",
    "\n",
    "# Choose node-level features available in df_train/val/test.\n",
    "# Start simple with total_flow only (strong baseline). You can add more later.\n",
    "NODE_FEATURE_COLS = [\n",
    "    \"total_flow\",        # required\n",
    "    # \"avg_speed\",        # optional (if you want)\n",
    "    # \"pct_observed\",     # optional\n",
    "    # \"samples\",          # optional\n",
    "]\n",
    "\n",
    "# If these exist, we can also add time features (same across nodes at each timestamp)\n",
    "TIME_FEATURE_COLS = [\n",
    "    \"hour_sin\", \"hour_cos\", \"dow_sin\", \"dow_cos\", \"is_peak_hour\", \"is_weekend\"\n",
    "]\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "def _ensure_datetime(df):\n",
    "    df = df.copy()\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    # If your data is exactly hourly already, this is fine:\n",
    "    df[\"timestamp\"] = df[\"timestamp\"].dt.floor(\"H\")\n",
    "    return df\n",
    "\n",
    "def build_time_node_tensor(df, station2idx, feature_cols, timestamp_col=\"timestamp\", station_col=\"station\"):\n",
    "    \"\"\"\n",
    "    Builds a dense tensor: X_time_node_feat with shape (T, N, F)\n",
    "    Also returns timestamps used (sorted).\n",
    "    Uses ONLY stations that exist in station2idx (matches adjacency).\n",
    "    \"\"\"\n",
    "    df = _ensure_datetime(df)\n",
    "\n",
    "    # Keep only stations that are in the graph (important!)\n",
    "    df = df[df[station_col].isin(station2idx.keys())].copy()\n",
    "\n",
    "    # Sort\n",
    "    df = df.sort_values([timestamp_col, station_col])\n",
    "\n",
    "    # Build sorted time index\n",
    "    times = np.array(sorted(df[timestamp_col].unique()))\n",
    "    N = len(station2idx)\n",
    "    F = len(feature_cols)\n",
    "\n",
    "    # Initialize (T, N, F) with NaNs (we'll fill)\n",
    "    X = np.full((len(times), N, F), np.nan, dtype=np.float32)\n",
    "\n",
    "    # Fast fill by grouping per timestamp\n",
    "    time_to_row = {t: i for i, t in enumerate(times)}\n",
    "    for _, row in df.iterrows():\n",
    "        t = row[timestamp_col]\n",
    "        s = row[station_col]\n",
    "        i = time_to_row[t]\n",
    "        j = station2idx[s]\n",
    "        for k, col in enumerate(feature_cols):\n",
    "            X[i, j, k] = row[col]\n",
    "\n",
    "    return X, times\n",
    "\n",
    "def fill_missing(X, method=\"ffill\", fill_value=0.0):\n",
    "    \"\"\"\n",
    "    Fill missing values in (T, N, F).\n",
    "    method: 'ffill' (forward fill over time) then fill remaining with fill_value\n",
    "    \"\"\"\n",
    "    X_filled = X.copy()\n",
    "    T, N, F = X_filled.shape\n",
    "\n",
    "    if method == \"ffill\":\n",
    "        # Forward fill over time for each node-feature\n",
    "        for j in range(N):\n",
    "            for k in range(F):\n",
    "                col = X_filled[:, j, k]\n",
    "                # forward fill NaNs\n",
    "                mask = np.isnan(col)\n",
    "                if mask.all():\n",
    "                    continue\n",
    "                idx = np.where(~mask, np.arange(T), 0)\n",
    "                np.maximum.accumulate(idx, out=idx)\n",
    "                col_f = col[idx]\n",
    "                X_filled[:, j, k] = col_f\n",
    "\n",
    "    # Fill any remaining NaNs\n",
    "    X_filled[np.isnan(X_filled)] = fill_value\n",
    "    return X_filled\n",
    "\n",
    "def fit_standardizer(X_train):\n",
    "    \"\"\"\n",
    "    X_train: (T, N, F) float32\n",
    "    Returns mean, std per feature over ALL time+nodes.\n",
    "    \"\"\"\n",
    "    # reshape to (T*N, F)\n",
    "    flat = X_train.reshape(-1, X_train.shape[-1])\n",
    "    mean = flat.mean(axis=0)\n",
    "    std = flat.std(axis=0) + 1e-6\n",
    "    return mean.astype(np.float32), std.astype(np.float32)\n",
    "\n",
    "def apply_standardizer(X, mean, std):\n",
    "    return ((X - mean[None, None, :]) / std[None, None, :]).astype(np.float32)\n",
    "\n",
    "def make_windows(X, y_target, input_len, horizon):\n",
    "    \"\"\"\n",
    "    X: (T, N, F)\n",
    "    y_target: (T, N)  (future target comes from total_flow column)\n",
    "    Returns:\n",
    "      Xw: (num_samples, F, input_len, N)\n",
    "      yw: (num_samples, N)  predicting one horizon step ahead (t + horizon)\n",
    "    \"\"\"\n",
    "    T = X.shape[0]\n",
    "    N = X.shape[1]\n",
    "    F = X.shape[2]\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "    last_start = T - input_len - horizon\n",
    "    for t0 in range(last_start):\n",
    "        x_block = X[t0 : t0 + input_len]              # (Tin, N, F)\n",
    "        y_t = y_target[t0 + input_len + horizon - 1]  # (N,)\n",
    "        # transpose to (F, Tin, N) for STGCN-friendly shape\n",
    "        x_block = np.transpose(x_block, (2, 0, 1))    # (F, Tin, N)\n",
    "        X_list.append(x_block)\n",
    "        y_list.append(y_t)\n",
    "\n",
    "    Xw = np.stack(X_list, axis=0).astype(np.float32)\n",
    "    yw = np.stack(y_list, axis=0).astype(np.float32)\n",
    "    return Xw, yw\n",
    "\n",
    "class STGCNDataset(Dataset):\n",
    "    def __init__(self, Xw, yw):\n",
    "        self.X = torch.from_numpy(Xw)  # (B, F, Tin, N)\n",
    "        self.y = torch.from_numpy(yw)  # (B, N)\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# -------------------------\n",
    "#  Build (T, N, F) tensors from each split\n",
    "# -------------------------\n",
    "# IMPORTANT: station2idx MUST come from the adjacency build step (N=1766)\n",
    "# Example check:\n",
    "print(\"Graph nodes (station2idx):\", len(station2idx))\n",
    "\n",
    "X_train_TNF, train_times = build_time_node_tensor(df_train, station2idx, NODE_FEATURE_COLS)\n",
    "X_val_TNF,   val_times   = build_time_node_tensor(df_val,   station2idx, NODE_FEATURE_COLS)\n",
    "X_test_TNF,  test_times  = build_time_node_tensor(df_test,  station2idx, NODE_FEATURE_COLS)\n",
    "\n",
    "print(\"Raw shapes (T, N, F):\")\n",
    "print(\"Train:\", X_train_TNF.shape, \"Val:\", X_val_TNF.shape, \"Test:\", X_test_TNF.shape)\n",
    "\n",
    "# -------------------------\n",
    "# Fill missing values (because some station-timestamps may be absent)\n",
    "# -------------------------\n",
    "X_train_TNF = fill_missing(X_train_TNF, method=\"ffill\", fill_value=0.0)\n",
    "X_val_TNF   = fill_missing(X_val_TNF,   method=\"ffill\", fill_value=0.0)\n",
    "X_test_TNF  = fill_missing(X_test_TNF,  method=\"ffill\", fill_value=0.0)\n",
    "\n",
    "# -------------------------\n",
    "# Standardize features using TRAIN stats only (no leakage!)\n",
    "# -------------------------\n",
    "mean_f, std_f = fit_standardizer(X_train_TNF)\n",
    "X_train_TNF = apply_standardizer(X_train_TNF, mean_f, std_f)\n",
    "X_val_TNF   = apply_standardizer(X_val_TNF,   mean_f, std_f)\n",
    "X_test_TNF  = apply_standardizer(X_test_TNF,  mean_f, std_f)\n",
    "\n",
    "# -------------------------\n",
    "# Build targets y = future total_flow (still in original scale OR standardized)\n",
    "# We will train to predict standardized total_flow if total_flow is in NODE_FEATURE_COLS.\n",
    "# -------------------------\n",
    "# total_flow is feature index 0 if NODE_FEATURE_COLS = [\"total_flow\", ...]\n",
    "flow_idx = NODE_FEATURE_COLS.index(\"total_flow\")\n",
    "\n",
    "y_train_TN = X_train_TNF[:, :, flow_idx]  # (T, N)\n",
    "y_val_TN   = X_val_TNF[:, :, flow_idx]\n",
    "y_test_TN  = X_test_TNF[:, :, flow_idx]\n",
    "\n",
    "# -------------------------\n",
    "# Create sliding windows for 12-step horizon training\n",
    "# -------------------------\n",
    "Xw_train, yw_train = make_windows(X_train_TNF, y_train_TN, INPUT_LEN, HORIZON)\n",
    "Xw_val,   yw_val   = make_windows(X_val_TNF,   y_val_TN,   INPUT_LEN, HORIZON)\n",
    "Xw_test,  yw_test  = make_windows(X_test_TNF,  y_test_TN,  INPUT_LEN, HORIZON)\n",
    "\n",
    "print(\"Windowed shapes:\")\n",
    "print(\"Xw_train:\", Xw_train.shape, \"yw_train:\", yw_train.shape)\n",
    "print(\"Xw_val  :\", Xw_val.shape,   \"yw_val  :\", yw_val.shape)\n",
    "print(\"Xw_test :\", Xw_test.shape,  \"yw_test :\", yw_test.shape)\n",
    "\n",
    "# -------------------------\n",
    "# DataLoaders \n",
    "# -------------------------\n",
    "train_loader = DataLoader(STGCNDataset(Xw_train, yw_train), batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_loader   = DataLoader(STGCNDataset(Xw_val,   yw_val),   batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(STGCNDataset(Xw_test,  yw_test),  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "af4bd23c-6944-4a9e-aedb-25c6bf08130a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "A_hat_torch shape: (1766, 1766)\n",
      "Sample batch shape: (32, 1, 24, 1766) Target shape: (32, 1766)\n",
      "STGCN(\n",
      "  (block1): STGCNBlock(\n",
      "    (temp1): TemporalConv(\n",
      "      (conv): Conv2d(1, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (gcn): GCNLayer(\n",
      "      (theta): Linear(in_features=64, out_features=64, bias=True)\n",
      "    )\n",
      "    (temp2): TemporalConv(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (residual): Conv2d(1, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (block2): STGCNBlock(\n",
      "    (temp1): TemporalConv(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (gcn): GCNLayer(\n",
      "      (theta): Linear(in_features=64, out_features=64, bias=True)\n",
      "    )\n",
      "    (temp2): TemporalConv(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (time_pool): AdaptiveAvgPool2d(output_size=(1, 1766))\n",
      "  (head): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "Model output shape: (32, 1766)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Full STGCN Model (PyTorch) + shape checks\n",
    "# Input:  (B, F, T, N)\n",
    "# Output: (B, N)  (predict total_flow at t + HORIZON for all N stations)\n",
    "# =========================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# -------------------------\n",
    "# Convert adjacency to torch (A_hat should be numpy array)\n",
    "# -------------------------\n",
    "A_hat_torch = torch.tensor(A_hat, dtype=torch.float32, device=device)  # (N, N)\n",
    "N_graph = A_hat_torch.shape[0]\n",
    "print(\"A_hat_torch shape:\", tuple(A_hat_torch.shape))\n",
    "\n",
    "# -------------------------\n",
    "# Graph Convolution (GCN) on each time step\n",
    "# X: (B, C, T, N)\n",
    "# GCN: X' = A_hat @ X (over N) then a linear projection C->C'\n",
    "# -------------------------\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, c_in, c_out, bias=True):\n",
    "        super().__init__()\n",
    "        self.theta = nn.Linear(c_in, c_out, bias=bias)\n",
    "\n",
    "    def forward(self, x, A_hat):\n",
    "        # x: (B, C, T, N)\n",
    "        B, C, T, N = x.shape\n",
    "\n",
    "        # (B, T, N, C)\n",
    "        x_perm = x.permute(0, 2, 3, 1)\n",
    "\n",
    "        # Graph propagation: (N,N) @ (B,T,N,C) -> (B,T,N,C)\n",
    "        # We'll do einsum for clarity and speed.\n",
    "        x_g = torch.einsum(\"nm,btnc->btmc\", A_hat, x_perm)\n",
    "\n",
    "        # Linear projection over channels: (B,T,N,C_out)\n",
    "        x_out = self.theta(x_g)\n",
    "\n",
    "        # Back to (B, C_out, T, N)\n",
    "        return x_out.permute(0, 3, 1, 2)\n",
    "\n",
    "# -------------------------\n",
    "# Temporal Convolution Block\n",
    "# conv over time dimension T (kernel along T), keeping N as \"width\"\n",
    "# We'll use Conv2d with kernel (Kt, 1)\n",
    "# -------------------------\n",
    "class TemporalConv(nn.Module):\n",
    "    def __init__(self, c_in, c_out, kernel_size=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        self.conv = nn.Conv2d(c_in, c_out, kernel_size=(kernel_size, 1), padding=(padding, 0))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, T, N)\n",
    "        x = self.conv(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "# -------------------------\n",
    "# STGCN Block = TemporalConv -> GCN -> TemporalConv\n",
    "# -------------------------\n",
    "class STGCNBlock(nn.Module):\n",
    "    def __init__(self, c_in, c_hidden, c_out, Kt=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.temp1 = TemporalConv(c_in, c_hidden, kernel_size=Kt, dropout=dropout)\n",
    "        self.gcn   = GCNLayer(c_hidden, c_hidden)\n",
    "        self.temp2 = TemporalConv(c_hidden, c_out, kernel_size=Kt, dropout=dropout)\n",
    "\n",
    "        # residual (if shapes match)\n",
    "        self.residual = None\n",
    "        if c_in != c_out:\n",
    "            self.residual = nn.Conv2d(c_in, c_out, kernel_size=(1, 1))\n",
    "\n",
    "    def forward(self, x, A_hat):\n",
    "        # x: (B, C, T, N)\n",
    "        res = x\n",
    "        x = self.temp1(x)\n",
    "        x = self.gcn(x, A_hat)\n",
    "        x = F.relu(x)\n",
    "        x = self.temp2(x)\n",
    "\n",
    "        if self.residual is not None:\n",
    "            res = self.residual(res)\n",
    "        x = x + res\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "# -------------------------\n",
    "# Full STGCN Model\n",
    "# - stack 2 STGCN blocks (common)\n",
    "# - final temporal pooling + linear head to (B, N)\n",
    "# -------------------------\n",
    "class STGCN(nn.Module):\n",
    "    def __init__(self, num_nodes, in_channels, hidden_channels=64, Kt=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.block1 = STGCNBlock(in_channels, hidden_channels, hidden_channels, Kt=Kt, dropout=dropout)\n",
    "        self.block2 = STGCNBlock(hidden_channels, hidden_channels, hidden_channels, Kt=Kt, dropout=dropout)\n",
    "\n",
    "        # readout: reduce time dimension then predict per node\n",
    "        # After blocks: (B, hidden, T, N)\n",
    "        self.time_pool = nn.AdaptiveAvgPool2d((1, num_nodes))  # -> (B, hidden, 1, N)\n",
    "\n",
    "        # final mapping hidden->1 per node\n",
    "        self.head = nn.Conv2d(hidden_channels, 1, kernel_size=(1, 1))\n",
    "\n",
    "    def forward(self, x, A_hat):\n",
    "        # x: (B, F, T, N)\n",
    "        x = self.block1(x, A_hat)\n",
    "        x = self.block2(x, A_hat)\n",
    "\n",
    "        x = self.time_pool(x)         # (B, hidden, 1, N)\n",
    "        x = self.head(x)              # (B, 1, 1, N)\n",
    "        x = x.squeeze(2).squeeze(1)   # (B, N)\n",
    "        return x\n",
    "\n",
    "# -------------------------\n",
    "# Instantiate model (auto-infer in_channels from one batch)\n",
    "# -------------------------\n",
    "sample_X, sample_y = next(iter(train_loader))\n",
    "B0, F0, T0, N0 = sample_X.shape\n",
    "\n",
    "assert N0 == N_graph, f\"Mismatch: batch has N={N0} but A_hat has N={N_graph}\"\n",
    "print(\"Sample batch shape:\", tuple(sample_X.shape), \"Target shape:\", tuple(sample_y.shape))\n",
    "\n",
    "model = STGCN(num_nodes=N0, in_channels=F0, hidden_channels=64, Kt=3, dropout=0.1).to(device)\n",
    "print(model)\n",
    "\n",
    "# -------------------------\n",
    "# Quick forward pass sanity check\n",
    "# -------------------------\n",
    "with torch.no_grad():\n",
    "    out = model(sample_X.to(device), A_hat_torch)\n",
    "print(\"Model output shape:\", tuple(out.shape))  # should be (B, N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d2bb17b5-4afc-4501-84da-03abe476361e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/30 | Train MSE: 0.161837 | Val MAE: 0.2392 | Val RMSE: 0.3649 | LR: 1.00e-03\n",
      "  New best model saved (by Val MAE).\n",
      "Epoch 02/30 | Train MSE: 0.144097 | Val MAE: 0.2386 | Val RMSE: 0.3554 | LR: 1.00e-03\n",
      "  New best model saved (by Val MAE).\n",
      "Epoch 03/30 | Train MSE: 0.143275 | Val MAE: 0.2348 | Val RMSE: 0.3530 | LR: 1.00e-03\n",
      "  New best model saved (by Val MAE).\n",
      "Epoch 04/30 | Train MSE: 0.144782 | Val MAE: 0.2460 | Val RMSE: 0.3605 | LR: 1.00e-03\n",
      "Epoch 05/30 | Train MSE: 0.140495 | Val MAE: 0.2377 | Val RMSE: 0.3519 | LR: 1.00e-03\n",
      "Epoch 06/30 | Train MSE: 0.138571 | Val MAE: 0.2325 | Val RMSE: 0.3497 | LR: 1.00e-03\n",
      "  New best model saved (by Val MAE).\n",
      "Epoch 07/30 | Train MSE: 0.139226 | Val MAE: 0.2373 | Val RMSE: 0.3532 | LR: 1.00e-03\n",
      "Epoch 08/30 | Train MSE: 0.135905 | Val MAE: 0.2335 | Val RMSE: 0.3488 | LR: 1.00e-03\n",
      "Epoch 09/30 | Train MSE: 0.134823 | Val MAE: 0.2303 | Val RMSE: 0.3483 | LR: 1.00e-03\n",
      "  New best model saved (by Val MAE).\n",
      "Epoch 10/30 | Train MSE: 0.138539 | Val MAE: 0.2376 | Val RMSE: 0.3517 | LR: 1.00e-03\n",
      "Epoch 11/30 | Train MSE: 0.133688 | Val MAE: 0.2288 | Val RMSE: 0.3440 | LR: 1.00e-03\n",
      "  New best model saved (by Val MAE).\n",
      "Epoch 12/30 | Train MSE: 0.130236 | Val MAE: 0.2262 | Val RMSE: 0.3409 | LR: 1.00e-03\n",
      "  New best model saved (by Val MAE).\n",
      "Epoch 13/30 | Train MSE: 0.128825 | Val MAE: 0.2286 | Val RMSE: 0.3441 | LR: 1.00e-03\n",
      "Epoch 14/30 | Train MSE: 0.131283 | Val MAE: 0.2328 | Val RMSE: 0.3453 | LR: 1.00e-03\n",
      "Epoch 15/30 | Train MSE: 0.128075 | Val MAE: 0.2212 | Val RMSE: 0.3363 | LR: 1.00e-03\n",
      "  New best model saved (by Val MAE).\n",
      "Epoch 16/30 | Train MSE: 0.128338 | Val MAE: 0.2249 | Val RMSE: 0.3390 | LR: 1.00e-03\n",
      "Epoch 17/30 | Train MSE: 0.126581 | Val MAE: 0.2195 | Val RMSE: 0.3356 | LR: 1.00e-03\n",
      "  New best model saved (by Val MAE).\n",
      "Epoch 18/30 | Train MSE: 0.124064 | Val MAE: 0.2221 | Val RMSE: 0.3378 | LR: 1.00e-03\n",
      "Epoch 19/30 | Train MSE: 0.122167 | Val MAE: 0.2202 | Val RMSE: 0.3298 | LR: 1.00e-03\n",
      "Epoch 20/30 | Train MSE: 0.122221 | Val MAE: 0.2183 | Val RMSE: 0.3289 | LR: 1.00e-03\n",
      "  New best model saved (by Val MAE).\n",
      "Epoch 21/30 | Train MSE: 0.120355 | Val MAE: 0.2203 | Val RMSE: 0.3310 | LR: 1.00e-03\n",
      "Epoch 22/30 | Train MSE: 0.121343 | Val MAE: 0.2341 | Val RMSE: 0.3440 | LR: 1.00e-03\n",
      "Epoch 23/30 | Train MSE: 0.120985 | Val MAE: 0.2142 | Val RMSE: 0.3250 | LR: 1.00e-03\n",
      "  New best model saved (by Val MAE).\n",
      "Epoch 24/30 | Train MSE: 0.117098 | Val MAE: 0.2115 | Val RMSE: 0.3236 | LR: 1.00e-03\n",
      "  New best model saved (by Val MAE).\n",
      "Epoch 25/30 | Train MSE: 0.118152 | Val MAE: 0.2103 | Val RMSE: 0.3210 | LR: 1.00e-03\n",
      "  New best model saved (by Val MAE).\n",
      "Epoch 26/30 | Train MSE: 0.115544 | Val MAE: 0.2093 | Val RMSE: 0.3179 | LR: 1.00e-03\n",
      "  New best model saved (by Val MAE).\n",
      "Epoch 27/30 | Train MSE: 0.114527 | Val MAE: 0.2155 | Val RMSE: 0.3229 | LR: 1.00e-03\n",
      "Epoch 28/30 | Train MSE: 0.114214 | Val MAE: 0.2196 | Val RMSE: 0.3260 | LR: 1.00e-03\n",
      "Epoch 29/30 | Train MSE: 0.114772 | Val MAE: 0.2048 | Val RMSE: 0.3148 | LR: 1.00e-03\n",
      "  New best model saved (by Val MAE).\n",
      "Epoch 30/30 | Train MSE: 0.112599 | Val MAE: 0.2132 | Val RMSE: 0.3202 | LR: 1.00e-03\n",
      "\n",
      "Loaded best model with Val MAE = 0.2048\n",
      "\n",
      " TEST (12h horizon) | MAE: 0.2069 | RMSE: 0.3210\n",
      "Saved best model to: stgcn_best_12h.pth\n"
     ]
    }
   ],
   "source": [
    "###### =========================\n",
    "#Train STGCN (12h horizon) + Validation + Save Best\n",
    "# Metrics: MAE + RMSE\n",
    "# =========================\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "# ---- helpers ----\n",
    "def mae_torch(y_pred, y_true):\n",
    "    return torch.mean(torch.abs(y_pred - y_true))\n",
    "\n",
    "def rmse_torch(y_pred, y_true):\n",
    "    return torch.sqrt(torch.mean((y_pred - y_true) ** 2))\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, A_hat_torch, device):\n",
    "    model.eval()\n",
    "    total_mae = 0.0\n",
    "    total_mse = 0.0\n",
    "    total_count = 0\n",
    "\n",
    "    for X, y in loader:\n",
    "        X = X.to(device)                  # (B, F, T, N)\n",
    "        y = y.to(device)                  # (B, N)\n",
    "\n",
    "        pred = model(X, A_hat_torch)      # (B, N)\n",
    "\n",
    "        # accumulate\n",
    "        batch_size = y.shape[0]\n",
    "        total_mae += torch.sum(torch.abs(pred - y)).item()\n",
    "        total_mse += torch.sum((pred - y) ** 2).item()\n",
    "        total_count += batch_size * y.shape[1]  # B*N\n",
    "\n",
    "    mean_mae = total_mae / total_count\n",
    "    mean_rmse = sqrt(total_mse / total_count)\n",
    "    return mean_mae, mean_rmse\n",
    "\n",
    "# -------------------------\n",
    "# 0) Training config\n",
    "# -------------------------\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-5\n",
    "epochs = 30\n",
    "patience = 7  # early stop if val doesn't improve\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# Optional: stabilize training\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=3)\n",
    "\n",
    "best_val_mae = float(\"inf\")\n",
    "best_state = None\n",
    "no_improve = 0\n",
    "\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# -------------------------\n",
    "# 1) Train loop\n",
    "# -------------------------\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_count = 0\n",
    "\n",
    "    for X, y in train_loader:\n",
    "        X = X.to(device)    # (B, F, T, N)\n",
    "        y = y.to(device)    # (B, N)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X, A_hat_torch)      # (B, N)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "\n",
    "        # optional gradient clipping (helps if things explode)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=3.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * X.shape[0]\n",
    "        running_count += X.shape[0]\n",
    "\n",
    "    train_loss = running_loss / running_count\n",
    "\n",
    "    # -------------------------\n",
    "    # 2) Validation metrics\n",
    "    # -------------------------\n",
    "    val_mae, val_rmse = evaluate(model, val_loader, A_hat_torch, device)\n",
    "\n",
    "    scheduler.step(val_mae)\n",
    "\n",
    "    train_history.append(train_loss)\n",
    "    val_history.append((val_mae, val_rmse))\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d}/{epochs} | \"\n",
    "        f\"Train MSE: {train_loss:.6f} | \"\n",
    "        f\"Val MAE: {val_mae:.4f} | Val RMSE: {val_rmse:.4f} | \"\n",
    "        f\"LR: {optimizer.param_groups[0]['lr']:.2e}\"\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # 3) Save best by Val MAE\n",
    "    # -------------------------\n",
    "    if val_mae < best_val_mae - 1e-6:\n",
    "        best_val_mae = val_mae\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        no_improve = 0\n",
    "        print(\"  New best model saved (by Val MAE).\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(\"  Early stopping (no improvement).\")\n",
    "            break\n",
    "\n",
    "# -------------------------\n",
    "# 4) Load best model\n",
    "# -------------------------\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    print(f\"\\nLoaded best model with Val MAE = {best_val_mae:.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# 5) Final evaluation on TEST (12h horizon)\n",
    "# -------------------------\n",
    "test_mae, test_rmse = evaluate(model, test_loader, A_hat_torch, device)\n",
    "print(f\"\\n TEST (12h horizon) | MAE: {test_mae:.4f} | RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# 6) Save checkpoint\n",
    "# -------------------------\n",
    "save_path = \"stgcn_best_12h.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Saved best model to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e8e82b31-1a43-43f0-a727-b9afe2cc96f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Predictions generated for horizons: [12, 24, 48, 72]\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Recursive Forecasting for 12/24/48/72 using 12h-trained STGCN\n",
    "# =========================\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from math import sqrt\n",
    "\n",
    "# We will forecast in multiples of 12 because your base model predicts +12\n",
    "HORIZONS = [12, 24, 48, 72]\n",
    "STEP = 12\n",
    "\n",
    "# Confirm settings used earlier\n",
    "INPUT_LEN = 24\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()\n",
    "\n",
    "# ---- utility: compute MAE/RMSE over all nodes and samples ----\n",
    "def compute_mae_rmse(y_pred, y_true):\n",
    "    # y_pred, y_true: (num_samples, N)\n",
    "    err = y_pred - y_true\n",
    "    mae = np.mean(np.abs(err))\n",
    "    rmse = np.sqrt(np.mean(err**2))\n",
    "    return float(mae), float(rmse)\n",
    "\n",
    "@torch.no_grad()\n",
    "def recursive_predict_batch(model, A_hat_torch, X0, steps):\n",
    "    \"\"\"\n",
    "    Recursive multi-step prediction using a +12h model.\n",
    "    X0: torch tensor (B, F, T, N) initial input window\n",
    "    steps: how many +12 steps to roll forward (e.g., 2 means +24h)\n",
    "    Returns: y_hat at final horizon: (B, N)\n",
    "    \"\"\"\n",
    "    X = X0.clone()\n",
    "\n",
    "    for _ in range(steps):\n",
    "        y_hat = model(X, A_hat_torch)  # (B, N)\n",
    "\n",
    "        # We assume \"total_flow\" is feature index 0 (from Step 2)\n",
    "        # Update the last time step feature channel for total_flow with the prediction\n",
    "        # Shift the window left by 1 time step and append a new step.\n",
    "        # Because our base horizon is 12h, we shift by 12 time steps? NO:\n",
    "        # The model was trained with sequences sampled hourly, but it predicts at +12.\n",
    "        # For recursion we advance the \"virtual time\" by 12 hours, so we shift by 12 slots.\n",
    "        shift = STEP\n",
    "\n",
    "        # X: (B, F, T, N)\n",
    "        B, Fch, T, N = X.shape\n",
    "        if shift >= T:\n",
    "            raise ValueError(\"STEP must be smaller than INPUT_LEN (window length).\")\n",
    "\n",
    "        # Shift window forward by 'shift' hours\n",
    "        X[:, :, :-shift, :] = X[:, :, shift:, :]\n",
    "\n",
    "        # For the newly opened last 'shift' slots, we need to fill them.\n",
    "        # We only have predictions for flow at the end of the horizon, not each intermediate hour.\n",
    "        # So we fill all new slots with the same predicted value for total_flow.\n",
    "        # (This is a common pragmatic approximation for recursive rollouts when step > 1.)\n",
    "        # Feature idx of total_flow:\n",
    "        flow_fidx = 0\n",
    "\n",
    "        # Fill new timesteps for the flow channel with y_hat\n",
    "        for tfill in range(shift):\n",
    "            X[:, flow_fidx, T - shift + tfill, :] = y_hat\n",
    "\n",
    "        # NOTE: For other features (avg_speed, time encodings), we leave them as-is in the new slots.\n",
    "        # If you included time encodings as node features, we will handle that better in Step 5.5.\n",
    "\n",
    "    return y_hat\n",
    "\n",
    "# -------------------------\n",
    "# Build arrays of predictions for each horizon on the TEST loader\n",
    "# -------------------------\n",
    "results = {}\n",
    "\n",
    "for horizon in HORIZONS:\n",
    "    steps = horizon // STEP  # 12->1, 24->2, 48->4, 72->6\n",
    "\n",
    "    all_pred = []\n",
    "    all_true = []\n",
    "\n",
    "    for X, y in test_loader:\n",
    "        X = X.to(device)  # (B, F, T, N)\n",
    "        y = y.to(device)  # (B, N) --> this y is the +12 target, not +24 etc.\n",
    "\n",
    "        # For fair evaluation of longer horizons, we need the true target at that horizon.\n",
    "        # We'll rebuild it from the underlying time-series tensor used in Step 2.\n",
    "        # So here, we will only collect predictions, and compute true labels below using tensors.\n",
    "\n",
    "        y_hat = recursive_predict_batch(model, A_hat_torch, X, steps)  # (B, N)\n",
    "        all_pred.append(y_hat.detach().cpu().numpy())\n",
    "\n",
    "    results[horizon] = np.concatenate(all_pred, axis=0)\n",
    "\n",
    "print(\" Predictions generated for horizons:\", list(results.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "171acead-50af-44a3-88c4-6141512d224a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 (673, 1766)\n",
      "24 (661, 1766)\n",
      "48 (637, 1766)\n",
      "72 (613, 1766)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "#  Build TRUE labels for 12/24/48/72 from X_test_TNF\n",
    "# =========================\n",
    "\n",
    "# flow is feature index 0 if NODE_FEATURE_COLS started with total_flow\n",
    "flow_idx = 0\n",
    "\n",
    "# True flow series on test split: (T, N)\n",
    "y_test_series = X_test_TNF[:, :, flow_idx]  # standardized flow (same scale as training)\n",
    "\n",
    "def true_labels_for_horizon(y_series_TN, input_len, horizon):\n",
    "    \"\"\"\n",
    "    y_series_TN: (T, N)\n",
    "    return yw: (num_samples, N) aligned with the SAME t0 loop used in make_windows\n",
    "    \"\"\"\n",
    "    T = y_series_TN.shape[0]\n",
    "    ys = []\n",
    "    last_start = T - input_len - horizon\n",
    "    for t0 in range(last_start):\n",
    "        ys.append(y_series_TN[t0 + input_len + horizon - 1])\n",
    "    return np.stack(ys, axis=0).astype(np.float32)\n",
    "\n",
    "true_y = {}\n",
    "for h in HORIZONS:\n",
    "    true_y[h] = true_labels_for_horizon(y_test_series, INPUT_LEN, h)\n",
    "    print(h, true_y[h].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c8b57966-b263-4d9a-a310-e029a6d115a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horizon 12h  | MAE: 0.2069 | RMSE: 0.3210\n",
      "Horizon 24h  | MAE: 0.4431 | RMSE: 0.6938\n",
      "Horizon 48h  | MAE: 0.5682 | RMSE: 0.8391\n",
      "Horizon 72h  | MAE: 0.6686 | RMSE: 0.9738\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Metrics table for 12/24/48/72\n",
    "# =========================\n",
    "\n",
    "for h in HORIZONS:\n",
    "    y_pred = results[h]\n",
    "    y_true = true_y[h]\n",
    "\n",
    "    # ensure same length (they should match)\n",
    "    m = min(len(y_pred), len(y_true))\n",
    "    y_pred = y_pred[:m]\n",
    "    y_true = y_true[:m]\n",
    "\n",
    "    mae, rmse = compute_mae_rmse(y_pred, y_true)\n",
    "    print(f\"Horizon {h:>2}h  | MAE: {mae:.4f} | RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bb505064-2760-4167-9ec3-fff82babdc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow mean: 63.93297253757723\n",
      "Flow std: 38.66041689479637\n"
     ]
    }
   ],
   "source": [
    "flow_idx = 0\n",
    "\n",
    "flow_mean = scaler.mean_[flow_idx]\n",
    "flow_std = np.sqrt(scaler.var_[flow_idx])\n",
    "\n",
    "print(\"Flow mean:\", flow_mean)\n",
    "print(\"Flow std:\", flow_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a87a74bd-0624-4916-88a4-c6986aa647f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horizon 12h | MAE: 8.00 veh/hr | RMSE: 12.41 veh/hr\n",
      "Horizon 24h | MAE: 17.13 veh/hr | RMSE: 26.82 veh/hr\n",
      "Horizon 48h | MAE: 21.97 veh/hr | RMSE: 32.44 veh/hr\n",
      "Horizon 72h | MAE: 25.85 veh/hr | RMSE: 37.65 veh/hr\n"
     ]
    }
   ],
   "source": [
    "for h in HORIZONS:\n",
    "    y_pred = results[h]\n",
    "    y_true = true_y[h]\n",
    "\n",
    "    m = min(len(y_pred), len(y_true))\n",
    "    y_pred = y_pred[:m]\n",
    "    y_true = y_true[:m]\n",
    "\n",
    "    mae_z, rmse_z = compute_mae_rmse(y_pred, y_true)\n",
    "\n",
    "    mae_real = mae_z * flow_std\n",
    "    rmse_real = rmse_z * flow_std\n",
    "\n",
    "    print(\n",
    "        f\"Horizon {h:>2}h | \"\n",
    "        f\"MAE: {mae_real:.2f} veh/hr | \"\n",
    "        f\"RMSE: {rmse_real:.2f} veh/hr\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c912fc4b-c5f6-48e7-a434-340b3d526c5a",
   "metadata": {},
   "source": [
    "# GRAPH WAVENET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "32f4ab52-c58d-4ada-8623-e70c559188c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_22372\\1624037659.py:26: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df_graph[TIME_COL] = pd.to_datetime(df_graph[TIME_COL]).dt.floor(\"H\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Nodes (stations): 1766\n",
      " A shape: (1766, 1766)\n",
      " Edges (excluding self-loops): 1723\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "TIME_COL = \"timestamp\"\n",
    "NODE_COL = \"station\"\n",
    "TARGET_COL = \"total_flow\"\n",
    "OPTIONAL_NODE_FEATURES = [\"avg_speed\"]  # will use only if present\n",
    "\n",
    "SEQ_LEN = 24\n",
    "HORIZON = 72\n",
    "EVAL_HORIZONS = [12, 24, 48, 72]\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "# -------------------------\n",
    "# Basic cleaning (train only for graph build)\n",
    "# -------------------------\n",
    "df_graph = df_train.copy()\n",
    "df_graph[TIME_COL] = pd.to_datetime(df_graph[TIME_COL]).dt.floor(\"H\")\n",
    "\n",
    "required_cols = [NODE_COL, \"upstream_station\", \"downstream_station\"]\n",
    "missing = [c for c in required_cols if c not in df_graph.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"df_train missing columns for adjacency: {missing}\")\n",
    "\n",
    "# Fix station types\n",
    "for c in [NODE_COL, \"upstream_station\", \"downstream_station\"]:\n",
    "    df_graph[c] = pd.to_numeric(df_graph[c], errors=\"coerce\")\n",
    "\n",
    "df_graph = df_graph.dropna(subset=[NODE_COL]).copy()\n",
    "df_graph[NODE_COL] = df_graph[NODE_COL].astype(int)\n",
    "\n",
    "stations = np.sort(df_graph[NODE_COL].unique())\n",
    "N = len(stations)\n",
    "station2idx = {s: i for i, s in enumerate(stations)}\n",
    "idx2station = {i: s for s, i in station2idx.items()}\n",
    "\n",
    "print(\" Nodes (stations):\", N)\n",
    "\n",
    "# -------------------------\n",
    "# Build adjacency A (N x N), symmetric, with self-loops\n",
    "# -------------------------\n",
    "A = np.zeros((N, N), dtype=np.float32)\n",
    "np.fill_diagonal(A, 1.0)  # self-loops\n",
    "\n",
    "edges = df_graph[[NODE_COL, \"upstream_station\", \"downstream_station\"]].drop_duplicates()\n",
    "\n",
    "def add_edge(s, n):\n",
    "    if pd.isna(n):\n",
    "        return\n",
    "    n = int(n)\n",
    "    if n in station2idx:\n",
    "        i = station2idx[int(s)]\n",
    "        j = station2idx[n]\n",
    "        A[i, j] = 1.0\n",
    "        A[j, i] = 1.0  # undirected\n",
    "\n",
    "for _, r in edges.iterrows():\n",
    "    s = int(r[NODE_COL])\n",
    "    add_edge(s, r[\"upstream_station\"])\n",
    "    add_edge(s, r[\"downstream_station\"])\n",
    "\n",
    "print(\" A shape:\", A.shape)\n",
    "print(\" Edges (excluding self-loops):\", int((A.sum() - np.trace(A)) / 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4af1a60e-8201-4ece-8ff9-18c480d7f399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Supports: 2 each: torch.Size([1766, 1766])\n"
     ]
    }
   ],
   "source": [
    "def row_normalize(A_np):\n",
    "    A_np = A_np.astype(np.float32)\n",
    "    rowsum = A_np.sum(axis=1, keepdims=True)\n",
    "    rowsum[rowsum == 0] = 1.0\n",
    "    return A_np / rowsum\n",
    "\n",
    "A_rw  = row_normalize(A)\n",
    "A_rwt = row_normalize(A.T)\n",
    "\n",
    "supports = [\n",
    "    torch.tensor(A_rw, dtype=torch.float32, device=DEVICE),\n",
    "    torch.tensor(A_rwt, dtype=torch.float32, device=DEVICE)\n",
    "]\n",
    "\n",
    "print(\" Supports:\", len(supports), \"each:\", supports[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2d2d35d7-7e42-446a-b7ac-023bdbc39b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Feature columns: ['total_flow', 'avg_speed']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_22372\\2952017846.py:3: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df[TIME_COL] = pd.to_datetime(df[TIME_COL]).dt.floor(\"H\")\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_22372\\2952017846.py:3: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df[TIME_COL] = pd.to_datetime(df[TIME_COL]).dt.floor(\"H\")\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_22372\\2952017846.py:43: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  train_times = pd.date_range(ensure_hourly(df_train)[TIME_COL].min(), ensure_hourly(df_train)[TIME_COL].max(), freq=\"H\")\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_22372\\2952017846.py:3: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df[TIME_COL] = pd.to_datetime(df[TIME_COL]).dt.floor(\"H\")\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_22372\\2952017846.py:3: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df[TIME_COL] = pd.to_datetime(df[TIME_COL]).dt.floor(\"H\")\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_22372\\2952017846.py:44: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  val_times   = pd.date_range(ensure_hourly(df_val)[TIME_COL].min(),   ensure_hourly(df_val)[TIME_COL].max(),   freq=\"H\")\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_22372\\2952017846.py:3: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df[TIME_COL] = pd.to_datetime(df[TIME_COL]).dt.floor(\"H\")\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_22372\\2952017846.py:3: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df[TIME_COL] = pd.to_datetime(df[TIME_COL]).dt.floor(\"H\")\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_22372\\2952017846.py:45: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  test_times  = pd.date_range(ensure_hourly(df_test)[TIME_COL].min(),  ensure_hourly(df_test)[TIME_COL].max(),  freq=\"H\")\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_22372\\2952017846.py:3: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df[TIME_COL] = pd.to_datetime(df[TIME_COL]).dt.floor(\"H\")\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_22372\\2952017846.py:3: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df[TIME_COL] = pd.to_datetime(df[TIME_COL]).dt.floor(\"H\")\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_22372\\2952017846.py:3: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df[TIME_COL] = pd.to_datetime(df[TIME_COL]).dt.floor(\"H\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train TNF: (1069, 1766, 2) Val TNF: (325, 1766, 2) Test TNF: (709, 1766, 2)\n"
     ]
    }
   ],
   "source": [
    "def ensure_hourly(df):\n",
    "    df = df.copy()\n",
    "    df[TIME_COL] = pd.to_datetime(df[TIME_COL]).dt.floor(\"H\")\n",
    "    return df\n",
    "\n",
    "# Features for the model: always include total_flow + optional features if present\n",
    "feature_cols = [TARGET_COL] + [c for c in OPTIONAL_NODE_FEATURES if c in df_train.columns]\n",
    "print(\" Feature columns:\", feature_cols)\n",
    "\n",
    "def build_TNF(df_split, times, stations, feature_cols):\n",
    "    \"\"\"\n",
    "    Build (T, N, F) for a split using a fixed time index (times) and station list (stations).\n",
    "    Missing values are forward-filled per station then zero-filled.\n",
    "    \"\"\"\n",
    "    df_split = ensure_hourly(df_split)\n",
    "    df_split = df_split[df_split[NODE_COL].isin(stations)].copy()\n",
    "\n",
    "    # Reindex full grid\n",
    "    full_index = pd.MultiIndex.from_product([times, stations], names=[TIME_COL, NODE_COL])\n",
    "    df_full = df_split.set_index([TIME_COL, NODE_COL]).reindex(full_index).reset_index()\n",
    "\n",
    "    # fill missing values per station\n",
    "    df_full[feature_cols] = (\n",
    "        df_full.groupby(NODE_COL)[feature_cols]\n",
    "               .apply(lambda g: g.ffill().bfill())\n",
    "               .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    df_full[feature_cols] = df_full[feature_cols].fillna(0.0)\n",
    "\n",
    "    # stack\n",
    "    T = len(times)\n",
    "    N = len(stations)\n",
    "    F_dim = len(feature_cols)\n",
    "    X = np.zeros((T, N, F_dim), dtype=np.float32)\n",
    "\n",
    "    for fi, col in enumerate(feature_cols):\n",
    "        pivot = df_full.pivot(index=TIME_COL, columns=NODE_COL, values=col).reindex(index=times, columns=stations)\n",
    "        X[:, :, fi] = pivot.values.astype(np.float32)\n",
    "\n",
    "    return X\n",
    "\n",
    "# Important: each split should have its own time index\n",
    "train_times = pd.date_range(ensure_hourly(df_train)[TIME_COL].min(), ensure_hourly(df_train)[TIME_COL].max(), freq=\"H\")\n",
    "val_times   = pd.date_range(ensure_hourly(df_val)[TIME_COL].min(),   ensure_hourly(df_val)[TIME_COL].max(),   freq=\"H\")\n",
    "test_times  = pd.date_range(ensure_hourly(df_test)[TIME_COL].min(),  ensure_hourly(df_test)[TIME_COL].max(),  freq=\"H\")\n",
    "\n",
    "X_train_TNF = build_TNF(df_train, train_times, stations, feature_cols)\n",
    "X_val_TNF   = build_TNF(df_val,   val_times,   stations, feature_cols)\n",
    "X_test_TNF  = build_TNF(df_test,  test_times,  stations, feature_cols)\n",
    "\n",
    "print(\"Train TNF:\", X_train_TNF.shape, \"Val TNF:\", X_val_TNF.shape, \"Test TNF:\", X_test_TNF.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "99b84a24-6421-4386-95a8-db11b2298f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Target mean/std from TRAIN: 1005.9266967773438 1343.13134765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_22372\\2711016957.py:26: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  target_mean = float(scaler.mean[..., 0])\n",
      "C:\\Users\\attafuro\\AppData\\Local\\Temp\\ipykernel_22372\\2711016957.py:27: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  target_std  = float(scaler.std[..., 0])\n"
     ]
    }
   ],
   "source": [
    "class StandardScalerTNF:\n",
    "    def __init__(self):\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        # X: (T,N,F)\n",
    "        self.mean = X.mean(axis=(0,1), keepdims=True)\n",
    "        self.std = X.std(axis=(0,1), keepdims=True)\n",
    "        self.std[self.std == 0] = 1.0\n",
    "\n",
    "    def transform(self, X):\n",
    "        return (X - self.mean) / self.std\n",
    "\n",
    "    def inverse_transform_feature(self, Z, fidx=0):\n",
    "        # Z has feature space scaled; convert that feature back to original units\n",
    "        return Z * float(self.std[..., fidx]) + float(self.mean[..., fidx])\n",
    "\n",
    "scaler = StandardScalerTNF()\n",
    "scaler.fit(X_train_TNF)\n",
    "\n",
    "X_train = scaler.transform(X_train_TNF).astype(np.float32)\n",
    "X_val   = scaler.transform(X_val_TNF).astype(np.float32)\n",
    "X_test  = scaler.transform(X_test_TNF).astype(np.float32)\n",
    "\n",
    "target_mean = float(scaler.mean[..., 0])\n",
    "target_std  = float(scaler.std[..., 0])\n",
    "print(\" Target mean/std from TRAIN:\", target_mean, target_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9e810818-4eb0-4181-93fc-1b510066d433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Windows: 973 229 613\n"
     ]
    }
   ],
   "source": [
    "class GWNWindowDataset(Dataset):\n",
    "    def __init__(self, X_tnf, seq_len=24, horizon=72, target_feature_idx=0):\n",
    "        self.X = X_tnf\n",
    "        self.seq_len = seq_len\n",
    "        self.horizon = horizon\n",
    "        self.target_idx = target_feature_idx\n",
    "        self.T = X_tnf.shape[0]\n",
    "\n",
    "        self.valid_t = list(range(seq_len, self.T - horizon))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_t)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t = self.valid_t[idx]\n",
    "\n",
    "        x_hist = self.X[t - self.seq_len:t]                 # (seq_len, N, F)\n",
    "        y_fut  = self.X[t:t + self.horizon, :, self.target_idx]  # (horizon, N)\n",
    "\n",
    "        # to (F, N, seq_len)\n",
    "        x = torch.tensor(x_hist, dtype=torch.float32).permute(2, 1, 0).contiguous()\n",
    "        # to (1, N, horizon)\n",
    "        y = torch.tensor(y_fut, dtype=torch.float32).permute(1, 0).unsqueeze(0).contiguous()\n",
    "        return x, y\n",
    "\n",
    "train_ds = GWNWindowDataset(X_train, seq_len=SEQ_LEN, horizon=HORIZON)\n",
    "val_ds   = GWNWindowDataset(X_val,   seq_len=SEQ_LEN, horizon=HORIZON)\n",
    "test_ds  = GWNWindowDataset(X_test,  seq_len=SEQ_LEN, horizon=HORIZON)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\" Windows:\", len(train_ds), len(val_ds), len(test_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "936ff26b-6636-4e62-b61a-a85b2e04b374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphWaveNet initialized.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# -------------------------\n",
    "# Diffusion Graph Convolution (Graph WaveNet style)\n",
    "# -------------------------\n",
    "class DiffusionGraphConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, supports_len, order=2):\n",
    "        super().__init__()\n",
    "        self.order = order\n",
    "        self.supports_len = supports_len\n",
    "        self.mlp = nn.Conv2d(in_channels * (supports_len * order + 1), out_channels, kernel_size=(1, 1))\n",
    "\n",
    "    def forward(self, x, supports):\n",
    "        # x: [B, C, N, T]\n",
    "        out = [x]\n",
    "        for a in supports:\n",
    "            x1 = torch.einsum(\"nm,bcmt->bcnt\", a, x)  # [B,C,N,T]\n",
    "            out.append(x1)\n",
    "            xk = x1\n",
    "            for _ in range(2, self.order + 1):\n",
    "                xk = torch.einsum(\"nm,bcmt->bcnt\", a, xk)\n",
    "                out.append(xk)\n",
    "        h = torch.cat(out, dim=1)\n",
    "        return self.mlp(h)\n",
    "\n",
    "# -------------------------\n",
    "# Graph WaveNet Model\n",
    "# -------------------------\n",
    "class GraphWaveNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_nodes,\n",
    "        in_dim,\n",
    "        out_dim=1,\n",
    "        horizon=72,\n",
    "        blocks=4,\n",
    "        layers=2,\n",
    "        kernel_size=2,\n",
    "        dropout=0.3,\n",
    "        residual_channels=32,\n",
    "        dilation_channels=32,\n",
    "        skip_channels=256,\n",
    "        end_channels=512,\n",
    "        supports=None,\n",
    "        adaptive_adj=True,\n",
    "        apt_embed_dim=10,\n",
    "        gcn_bool=True,\n",
    "        gcn_order=2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.horizon = horizon\n",
    "        self.dropout = dropout\n",
    "        self.blocks = blocks\n",
    "        self.layers = layers\n",
    "        self.gcn_bool = gcn_bool\n",
    "        self.adaptive_adj = adaptive_adj\n",
    "\n",
    "        self.start_conv = nn.Conv2d(in_dim, residual_channels, kernel_size=(1, 1))\n",
    "\n",
    "        self.supports = supports if supports is not None else []\n",
    "        self.supports_len = len(self.supports)\n",
    "\n",
    "        # Adaptive adjacency\n",
    "        if adaptive_adj:\n",
    "            self.nodevec1 = nn.Parameter(torch.randn(num_nodes, apt_embed_dim), requires_grad=True)\n",
    "            self.nodevec2 = nn.Parameter(torch.randn(apt_embed_dim, num_nodes), requires_grad=True)\n",
    "            self.supports_len += 1\n",
    "\n",
    "        self.filter_convs = nn.ModuleList()\n",
    "        self.gate_convs = nn.ModuleList()\n",
    "        self.residual_convs = nn.ModuleList()\n",
    "        self.skip_convs = nn.ModuleList()\n",
    "        self.bn = nn.ModuleList()\n",
    "        self.gconv = nn.ModuleList()\n",
    "\n",
    "        receptive_field = 1\n",
    "        for _ in range(blocks):\n",
    "            additional_scope = kernel_size - 1\n",
    "            new_dilation = 1\n",
    "            for _ in range(layers):\n",
    "                # Dilated temporal convolutions (along time axis)\n",
    "                self.filter_convs.append(\n",
    "                    nn.Conv2d(residual_channels, dilation_channels,\n",
    "                              kernel_size=(1, kernel_size), dilation=(1, new_dilation))\n",
    "                )\n",
    "                self.gate_convs.append(\n",
    "                    nn.Conv2d(residual_channels, dilation_channels,\n",
    "                              kernel_size=(1, kernel_size), dilation=(1, new_dilation))\n",
    "                )\n",
    "\n",
    "                self.residual_convs.append(nn.Conv2d(dilation_channels, residual_channels, kernel_size=(1, 1)))\n",
    "                self.skip_convs.append(nn.Conv2d(dilation_channels, skip_channels, kernel_size=(1, 1)))\n",
    "                self.bn.append(nn.BatchNorm2d(residual_channels))\n",
    "\n",
    "                if gcn_bool:\n",
    "                    self.gconv.append(\n",
    "                        DiffusionGraphConv(dilation_channels, residual_channels,\n",
    "                                           supports_len=self.supports_len, order=gcn_order)\n",
    "                    )\n",
    "\n",
    "                receptive_field += additional_scope * new_dilation\n",
    "                new_dilation *= 2\n",
    "\n",
    "        self.receptive_field = receptive_field\n",
    "        self.end_conv_1 = nn.Conv2d(skip_channels, end_channels, kernel_size=(1, 1))\n",
    "        self.end_conv_2 = nn.Conv2d(end_channels, out_dim * horizon, kernel_size=(1, 1))\n",
    "\n",
    "    def forward(self, x, supports_override=None):\n",
    "        \"\"\"\n",
    "        x: [B, F, N, T]\n",
    "        returns: [B, out_dim, N, horizon]\n",
    "        \"\"\"\n",
    "        if x.size(-1) < self.receptive_field:\n",
    "            x = F.pad(x, (self.receptive_field - x.size(-1), 0, 0, 0))\n",
    "\n",
    "        x = self.start_conv(x)\n",
    "        skip = None\n",
    "\n",
    "        # supports: physical + adaptive\n",
    "        supports = list(self.supports) if supports_override is None else list(supports_override)\n",
    "        if self.adaptive_adj:\n",
    "            adp = F.softmax(F.relu(torch.matmul(self.nodevec1, self.nodevec2)), dim=1)  # [N,N]\n",
    "            supports = supports + [adp]\n",
    "\n",
    "        for i in range(self.blocks * self.layers):\n",
    "            residual = x\n",
    "\n",
    "            # gated temporal conv\n",
    "            filter_out = torch.tanh(self.filter_convs[i](x))\n",
    "            gate_out = torch.sigmoid(self.gate_convs[i](x))\n",
    "            x = filter_out * gate_out\n",
    "            x = F.dropout(x, self.dropout, training=self.training)\n",
    "\n",
    "            # skip\n",
    "            s = self.skip_convs[i](x)\n",
    "            skip = s if skip is None else (skip[..., -s.size(-1):] + s)\n",
    "\n",
    "            # graph conv\n",
    "            if self.gcn_bool:\n",
    "                x = self.gconv[i](x, supports)\n",
    "            else:\n",
    "                x = self.residual_convs[i](x)\n",
    "\n",
    "            # residual + norm\n",
    "            x = x + residual[..., -x.size(-1):]\n",
    "            x = self.bn[i](x)\n",
    "\n",
    "        x = F.relu(skip)\n",
    "        x = F.relu(self.end_conv_1(x))\n",
    "        x = self.end_conv_2(x)  # [B, out_dim*horizon, N, T']\n",
    "        x = x[:, :, :, -1]      # [B, out_dim*horizon, N]\n",
    "        x = x.view(x.size(0), self.out_dim, self.horizon, self.num_nodes)\n",
    "        x = x.permute(0, 1, 3, 2).contiguous()  # [B, out_dim, N, horizon]\n",
    "        return x\n",
    "\n",
    "# Instantiate\n",
    "in_dim = len(feature_cols)\n",
    "model = GraphWaveNet(\n",
    "    num_nodes=N,\n",
    "    in_dim=in_dim,\n",
    "    out_dim=1,\n",
    "    horizon=HORIZON,\n",
    "    supports=supports,\n",
    "    adaptive_adj=True,\n",
    "    gcn_bool=True,\n",
    "    dropout=0.3,\n",
    "    residual_channels=32,\n",
    "    dilation_channels=32,\n",
    "    skip_channels=256,\n",
    "    end_channels=512\n",
    ").to(DEVICE)\n",
    "\n",
    "print(model.__class__.__name__, \"initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77485ab-2da1-4567-bcae-18fc96c2d086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mae_loss(pred, true):\n",
    "    # pred/true: [B,1,N,H]\n",
    "    return torch.mean(torch.abs(pred - true))\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_val_loss(model, loader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for x, y in loader:\n",
    "        x = x.to(DEVICE)  # [B,F,N,T]\n",
    "        y = y.to(DEVICE)  # [B,1,N,H]\n",
    "        pred = model(x)\n",
    "        losses.append(mae_loss(pred, y).item())\n",
    "    return float(np.mean(losses)) if losses else np.nan\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "EPOCHS = 25\n",
    "PATIENCE = 6\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "best_state = None\n",
    "no_improve = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        loss = mae_loss(pred, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    train_loss = float(np.mean(train_losses))\n",
    "    val_loss = evaluate_val_loss(model, val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}/{EPOCHS} | Train MAE(z): {train_loss:.4f} | Val MAE(z): {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val - 1e-6:\n",
    "        best_val = val_loss\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        no_improve = 0\n",
    "        print(\"   Best model updated.\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= PATIENCE:\n",
    "            print(\"   Early stopping.\")\n",
    "            break\n",
    "\n",
    "# Load best\n",
    "model.load_state_dict(best_state)\n",
    "model.to(DEVICE)\n",
    "print(\"Loaded best model with Val MAE(z):\", best_val)\n",
    "\n",
    "# Save checkpoint\n",
    "torch.save(model.state_dict(), \"gwn_best.pth\")\n",
    "print(\"Saved: gwn_best.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b3ec9b-320e-40de-be3e-cc8b009d53a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_full_horizon(model, loader):\n",
    "    \"\"\"\n",
    "    Returns pred_z, true_z in scaled space:\n",
    "      pred_z, true_z shape: [num_samples, 1, N, H]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    for x, y in loader:\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        pred = model(x)  # [B,1,N,H]\n",
    "        preds.append(pred.detach().cpu().numpy())\n",
    "        trues.append(y.detach().cpu().numpy())\n",
    "    return np.concatenate(preds, axis=0), np.concatenate(trues, axis=0)\n",
    "\n",
    "def to_real_units(z):\n",
    "    # z is scaled flow; convert back to veh/hr using train mean/std for the target feature\n",
    "    return z * target_std + target_mean\n",
    "\n",
    "def mae_rmse(pred, true):\n",
    "    err = pred - true\n",
    "    mae = float(np.mean(np.abs(err)))\n",
    "    rmse = float(np.sqrt(np.mean(err**2)))\n",
    "    return mae, rmse\n",
    "\n",
    "def report_metrics(pred_z, true_z, label):\n",
    "    pred_real = to_real_units(pred_z)\n",
    "    true_real = to_real_units(true_z)\n",
    "\n",
    "    print(f\"\\n=== {label} metrics (veh/hr) ===\")\n",
    "    for h in EVAL_HORIZONS:\n",
    "        k = h - 1  # 12h -> index 11\n",
    "        p = pred_real[:, 0, :, k]  # [S,N]\n",
    "        t = true_real[:, 0, :, k]\n",
    "        m, r = mae_rmse(p, t)\n",
    "        print(f\"Horizon {h:>2}h | MAE: {m:.3f} | RMSE: {r:.3f}\")\n",
    "\n",
    "# VAL\n",
    "val_pred_z, val_true_z = predict_full_horizon(model, val_loader)\n",
    "report_metrics(val_pred_z, val_true_z, \"VALIDATION\")\n",
    "\n",
    "# TEST\n",
    "test_pred_z, test_true_z = predict_full_horizon(model, test_loader)\n",
    "report_metrics(test_pred_z, test_true_z, \"TEST\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88336f03-acd3-4a12-9912-99519012c8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Save arrays for plots/thesis figures\n",
    "np.save(\"gwn_val_pred_real.npy\", to_real_units(val_pred_z))\n",
    "np.save(\"gwn_val_true_real.npy\", to_real_units(val_true_z))\n",
    "np.save(\"gwn_test_pred_real.npy\", to_real_units(test_pred_z))\n",
    "np.save(\"gwn_test_true_real.npy\", to_real_units(test_true_z))\n",
    "print(\"Saved prediction arrays for VAL/TEST (real units).\")\n",
    "\n",
    "# Build a results table for TEST\n",
    "rows = []\n",
    "pred_real = to_real_units(test_pred_z)\n",
    "true_real = to_real_units(test_true_z)\n",
    "\n",
    "for h in EVAL_HORIZONS:\n",
    "    k = h - 1\n",
    "    p = pred_real[:, 0, :, k]\n",
    "    t = true_real[:, 0, :, k]\n",
    "    m, r = mae_rmse(p, t)\n",
    "    rows.append({\"Model\": \"GraphWaveNet\", \"Horizon_hours\": h, \"MAE\": m, \"RMSE\": r})\n",
    "\n",
    "gwn_results_df = pd.DataFrame(rows)\n",
    "print(gwn_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56254c1a-1d04-42d2-b171-ae194ae8ce2f",
   "metadata": {},
   "source": [
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28556fce-b8d6-429d-9323-f8f091aa6020",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_HORIZON = 12   # train only for 12h\n",
    "EVAL_HORIZONS = [12, 24, 48, 72]\n",
    "SEQ_LEN = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a90a5f-3d30-4267-9e56-566180ad9559",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GWNWindowDataset_12(Dataset):\n",
    "    def __init__(self, X_tnf, seq_len=24, horizon=12, target_feature_idx=0):\n",
    "        self.X = X_tnf\n",
    "        self.seq_len = seq_len\n",
    "        self.horizon = horizon\n",
    "        self.target_idx = target_feature_idx\n",
    "        self.T = X_tnf.shape[0]\n",
    "        self.valid_t = list(range(seq_len, self.T - horizon))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_t)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t = self.valid_t[idx]\n",
    "        x_hist = self.X[t - self.seq_len:t]                        # (seq_len, N, F)\n",
    "        y_fut  = self.X[t:t + self.horizon, :, self.target_idx]    # (horizon, N)\n",
    "\n",
    "        x = torch.tensor(x_hist, dtype=torch.float32).permute(2, 1, 0).contiguous()  # (F,N,T)\n",
    "        y = torch.tensor(y_fut, dtype=torch.float32).permute(1, 0).unsqueeze(0).contiguous()  # (1,N,H)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "train_ds = GWNWindowDataset_12(X_train, seq_len=SEQ_LEN, horizon=BASE_HORIZON)\n",
    "val_ds   = GWNWindowDataset_12(X_val,   seq_len=SEQ_LEN, horizon=BASE_HORIZON)\n",
    "test_ds  = GWNWindowDataset_12(X_test,  seq_len=SEQ_LEN, horizon=BASE_HORIZON)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"Windows:\", len(train_ds), len(val_ds), len(test_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b98cdc-bbda-4e0b-82f9-58791af97f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = len(feature_cols)\n",
    "\n",
    "model = GraphWaveNet(\n",
    "    num_nodes=N,\n",
    "    in_dim=in_dim,\n",
    "    out_dim=1,\n",
    "    horizon=BASE_HORIZON,  # <-- KEY CHANGE\n",
    "    supports=supports,\n",
    "    adaptive_adj=True,\n",
    "    gcn_bool=True,\n",
    "    dropout=0.3,\n",
    "    residual_channels=32,\n",
    "    dilation_channels=32,\n",
    "    skip_channels=256,\n",
    "    end_channels=512\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3daa16-a9e4-4082-a2f4-49223f7ad976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "#  Train Graph WaveNet (+12h only) with Val monitoring + Best checkpoint\n",
    "# Assumes you already have:\n",
    "#   model, train_loader, val_loader, optimizer, DEVICE\n",
    "# and BASE_HORIZON=12 was used in the dataset/model (so y shape is [B,1,N,12])\n",
    "# =========================\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def mae_loss(pred, true):\n",
    "    # pred/true: [B,1,N,12]\n",
    "    return torch.mean(torch.abs(pred - true))\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_val_mae(model, loader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for x, y in loader:\n",
    "        x = x.to(DEVICE)  # [B,F,N,T]\n",
    "        y = y.to(DEVICE)  # [B,1,N,12]\n",
    "        pred = model(x)   # [B,1,N,12]\n",
    "        losses.append(mae_loss(pred, y).item())\n",
    "    return float(np.mean(losses)) if losses else np.nan\n",
    "\n",
    "# ---- training config ----\n",
    "EPOCHS = 25\n",
    "PATIENCE = 6\n",
    "CLIP_NORM = 5.0\n",
    "SAVE_PATH = \"gwn_best_12h.pth\"\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "best_state = None\n",
    "no_improve = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(DEVICE)  # [B,F,N,SEQ_LEN]\n",
    "        y = y.to(DEVICE)  # [B,1,N,12]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)           # [B,1,N,12]\n",
    "        loss = mae_loss(pred, y)  # scalar\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    train_mae = float(np.mean(train_losses))\n",
    "    val_mae = evaluate_val_mae(model, val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}/{EPOCHS} | Train MAE(z): {train_mae:.4f} | Val MAE(z): {val_mae:.4f}\")\n",
    "\n",
    "    # ---- keep best by Val MAE ----\n",
    "    if val_mae < best_val - 1e-6:\n",
    "        best_val = val_mae\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        no_improve = 0\n",
    "        print(\"   Best model updated.\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= PATIENCE:\n",
    "            print(\"   Early stopping.\")\n",
    "            break\n",
    "\n",
    "# ---- load best + save ----\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    model.to(DEVICE)\n",
    "    torch.save(model.state_dict(), SAVE_PATH)\n",
    "    print(f\" Loaded + saved best model: {SAVE_PATH} | Best Val MAE(z): {best_val:.4f}\")\n",
    "else:\n",
    "    print(\" No best_state saved (unexpected). Check training/validation loaders.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6770671-2ad4-4f16-9c13-d70cf2cb28ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def gwn_recursive_rollout(model, x0, steps, flow_feature_index=0):\n",
    "    \"\"\"\n",
    "    model: trained for BASE_HORIZON=12\n",
    "    x0: [B, F, N, SEQ_LEN] in scaled space\n",
    "    steps: number of 12-hour blocks to roll (1=12h, 2=24h, 4=48h, 6=72h)\n",
    "    Returns: final prediction for the last block at its LAST step: [B, N]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    x = x0.clone()  # [B,F,N,T]\n",
    "\n",
    "    last_block_pred = None\n",
    "\n",
    "    for _ in range(steps):\n",
    "        y_block = model(x)             # [B,1,N,12]  (scaled)\n",
    "        last_block_pred = y_block[:, 0, :, -1]  # take the last step of the 12h block -> [B,N]\n",
    "\n",
    "        # Roll the input window forward by 12 hours:\n",
    "        # shift left by 12 and append 12 new \"future\" steps.\n",
    "        shift = BASE_HORIZON\n",
    "        B, Fch, Nn, T = x.shape\n",
    "\n",
    "        if shift >= T:\n",
    "            raise ValueError(\"BASE_HORIZON must be smaller than SEQ_LEN (24).\")\n",
    "\n",
    "        # shift existing history left\n",
    "        x[:, :, :, :-shift] = x[:, :, :, shift:]\n",
    "\n",
    "        # fill the last 12 positions\n",
    "        # We ONLY update total_flow channel. Other channels stay constant (approx).\n",
    "        for tfill in range(shift):\n",
    "            x[:, flow_feature_index, :, T - shift + tfill] = last_block_pred\n",
    "\n",
    "    return last_block_pred  # [B,N] scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4e4caa-4270-4c64-bfb5-18fd3321469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_labels_from_series(X_tnf_scaled, seq_len, horizon, flow_idx=0):\n",
    "    \"\"\"\n",
    "    X_tnf_scaled: (T,N,F)\n",
    "    return: (num_samples, N) for the horizon at t + horizon\n",
    "    aligned with dataset indexing (t from seq_len ... T-horizon-1)\n",
    "    \"\"\"\n",
    "    y_series = X_tnf_scaled[:, :, flow_idx]  # (T,N)\n",
    "    ys = []\n",
    "    for t in range(seq_len, X_tnf_scaled.shape[0] - horizon):\n",
    "        ys.append(y_series[t + horizon - 1])\n",
    "    return np.stack(ys, axis=0).astype(np.float32)\n",
    "\n",
    "def inverse_units(z):\n",
    "    return z * target_std + target_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3e3a5a-c85f-4ffc-8ae1-e593df79b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def mae_rmse_np(p, t):\n",
    "    err = p - t\n",
    "    mae = float(np.mean(np.abs(err)))\n",
    "    rmse = float(np.sqrt(np.mean(err**2)))\n",
    "    return mae, rmse\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_recursive(model, loader, X_tnf_scaled, label=\"TEST\", flow_feature_index=0):\n",
    "    print(f\"\\n=== {label} Recursive Evaluation (veh/hr) ===\")\n",
    "\n",
    "    # true labels for each horizon\n",
    "    true_dict = {h: true_labels_from_series(X_tnf_scaled, SEQ_LEN, h, flow_idx=0) for h in EVAL_HORIZONS}\n",
    "\n",
    "    # collect predictions per horizon\n",
    "    pred_dict = {h: [] for h in EVAL_HORIZONS}\n",
    "\n",
    "    sample_cursor = 0  # aligns loader batches to true labels\n",
    "\n",
    "    for x_batch, _ in loader:\n",
    "        x_batch = x_batch.to(DEVICE)  # [B,F,N,T]\n",
    "        B = x_batch.shape[0]\n",
    "\n",
    "        for h in EVAL_HORIZONS:\n",
    "            steps = h // BASE_HORIZON\n",
    "            pred_scaled = gwn_recursive_rollout(model, x_batch, steps, flow_feature_index=flow_feature_index)  # [B,N]\n",
    "            pred_dict[h].append(pred_scaled.detach().cpu().numpy())\n",
    "\n",
    "        sample_cursor += B\n",
    "\n",
    "    # compute metrics\n",
    "    for h in EVAL_HORIZONS:\n",
    "        pred_scaled_all = np.concatenate(pred_dict[h], axis=0)  # (S,N)\n",
    "        true_scaled_all = true_dict[h]\n",
    "\n",
    "        m = min(len(pred_scaled_all), len(true_scaled_all))\n",
    "        pred_real = inverse_units(pred_scaled_all[:m])\n",
    "        true_real = inverse_units(true_scaled_all[:m])\n",
    "\n",
    "        mae, rmse = mae_rmse_np(pred_real, true_real)\n",
    "        print(f\"Horizon {h:>2}h | MAE: {mae:.3f} | RMSE: {rmse:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020e7fa5-ce64-404a-8df9-6ce5385a89c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training and loading best model:\n",
    "eval_recursive(model, val_loader, X_val, label=\"VALIDATION\", flow_feature_index=0)\n",
    "eval_recursive(model, test_loader, X_test, label=\"TEST\", flow_feature_index=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5594a7a2-27e7-44ff-bc7c-82627cdd4b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb9bf1b-6b68-46fb-9069-bf3e59ef1f45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

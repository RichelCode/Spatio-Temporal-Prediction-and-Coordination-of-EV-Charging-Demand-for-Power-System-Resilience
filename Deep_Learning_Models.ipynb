{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9795e03-a5db-4abb-947f-65a818b13483",
   "metadata": {},
   "source": [
    "# Multi-Horizon Traffic Forecasting on PeMS (Graph Models)\n",
    "\n",
    "## Goal (Paper Claim)\n",
    "Build a leakage-safe, reproducible pipeline on PeMS traffic data and evaluate multi-horizon forecasting models fairly.\n",
    "\n",
    "Primary goal:\n",
    "- Demonstrate the proposed **GraphWaveNet-GRU-LSTM** performs best on PeMS under the same train/val/test protocol.\n",
    "\n",
    "Key principles:\n",
    "- No time leakage (all statistics computed from train only).\n",
    "- One shared dataset representation for all deep models: **X ∈ R^{T×N×F}, Y ∈ R^{T×N}**.\n",
    "- One fixed evaluation harness (same horizons, same metrics, same seeds).\n",
    "- Strong baselines + ablations:\n",
    "  - HA / Persistence\n",
    "  - GRU / LSTM (non-graph)\n",
    "  - GraphWaveNet\n",
    "  - GraphWaveNet+GRU\n",
    "  - GraphWaveNet+LSTM\n",
    "  - **GraphWaveNet+GRU+LSTM (proposed)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dceeef19-2130-4a23-931a-06196ed31757",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T19:36:12.615948Z",
     "iopub.status.busy": "2026-02-07T19:36:12.615649Z",
     "iopub.status.idle": "2026-02-07T19:36:20.583394Z",
     "shell.execute_reply": "2026-02-07T19:36:20.582642Z",
     "shell.execute_reply.started": "2026-02-07T19:36:12.615922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb15451a-d0ef-44ab-a5ae-c33eebe63c39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T19:36:29.430693Z",
     "iopub.status.busy": "2026-02-07T19:36:29.430355Z",
     "iopub.status.idle": "2026-02-07T19:36:33.161009Z",
     "shell.execute_reply": "2026-02-07T19:36:33.160283Z",
     "shell.execute_reply.started": "2026-02-07T19:36:29.430665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install numpy pandas openpyxl scikit-learn torch tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70d3f8bf-2c00-4b4c-acfb-5ce948648cf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T19:36:50.204022Z",
     "iopub.status.busy": "2026-02-07T19:36:50.203372Z",
     "iopub.status.idle": "2026-02-07T19:36:52.731520Z",
     "shell.execute_reply": "2026-02-07T19:36:52.730792Z",
     "shell.execute_reply.started": "2026-02-07T19:36:50.203991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.1.1+cu121\n",
      "Device: cuda\n",
      "GPU: Quadro P5000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def set_seed(seed: int = 42, deterministic: bool = True):\n",
    "    \"\"\"\n",
    "    Sets seeds for reproducibility.\n",
    "    deterministic=True makes results more reproducible but can reduce speed.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    if deterministic:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    else:\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED, deterministic=True)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Device:\", DEVICE)\n",
    "if DEVICE == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91ddbd0-0ab3-4784-8fe2-930de04d11c3",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "We fix:\n",
    "- Input window length (`IN_LEN`) and forecast horizon length (`OUT_LEN`)\n",
    "- Train/val/test boundaries (time-based split)\n",
    "- Station inclusion rule (coverage threshold)\n",
    "- Output dataset artifact path (so every model uses the same processed dataset)\n",
    "\n",
    "Important:\n",
    "GraphWaveNet expects a consistent node set and continuous time axis,\n",
    "so we build a clean matrix format (timestamp × station).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "907ef442-d82f-4601-ba92-8f0b62467aa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T20:30:36.024501Z",
     "iopub.status.busy": "2026-02-07T20:30:36.023555Z",
     "iopub.status.idle": "2026-02-07T20:30:36.031533Z",
     "shell.execute_reply": "2026-02-07T20:30:36.030825Z",
     "shell.execute_reply.started": "2026-02-07T20:30:36.024473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will save processed dataset to: artifacts/pems_graph_dataset.npz\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Paths (your files are visible in the Paperspace file pane)\n",
    "# -------------------------\n",
    "TRAFFIC_CSV = Path(\"cleaned_traffic_data.csv\")\n",
    "META_XLSX   = Path(\"pems_output.xlsx\")\n",
    "\n",
    "assert TRAFFIC_CSV.exists(), f\"Missing {TRAFFIC_CSV}\"\n",
    "assert META_XLSX.exists(), f\"Missing {META_XLSX}\"\n",
    "\n",
    "# -------------------------\n",
    "# Split boundaries (same as your earlier work)\n",
    "# -------------------------\n",
    "TRAIN_END = pd.Timestamp(\"2024-11-15 23:59:59\")\n",
    "VAL_END   = pd.Timestamp(\"2024-11-30 23:59:59\")\n",
    "\n",
    "# -------------------------\n",
    "# Forecast setup\n",
    "# -------------------------\n",
    "IN_LEN  = 24     # hours of history used as input\n",
    "OUT_LEN = 72     # predict next 72 hours (we will evaluate at 12/24/48/72)\n",
    "\n",
    "EVAL_HORIZONS = [12, 24, 48, 72]  # hours ahead\n",
    "\n",
    "# -------------------------\n",
    "# Station coverage threshold\n",
    "# -------------------------\n",
    "# 1.0 means station must have ALL timestamps present.\n",
    "# 0.98 is often a good compromise if some stations are missing few points.\n",
    "COVERAGE_THRESHOLD = 0.98\n",
    "\n",
    "# -------------------------\n",
    "# Adjacency setup (static graph baseline)\n",
    "# -------------------------\n",
    "K_NEIGHBORS = 2   # connect up to 2 upstream + 2 downstream along the freeway chain\n",
    "\n",
    "# -------------------------\n",
    "# Output artifact (important for reproducibility)\n",
    "# -------------------------\n",
    "OUT_DIR = Path(\"artifacts\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "DATASET_NPZ = OUT_DIR / \"pems_graph_dataset.npz\"\n",
    "print(\"Will save processed dataset to:\", DATASET_NPZ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714660ee-bbf4-44ce-b128-8d23ca17d379",
   "metadata": {},
   "source": [
    "## Load raw traffic + metadata\n",
    "\n",
    "We:\n",
    "1) Load cleaned traffic data\n",
    "2) Load station metadata\n",
    "3) Standardize column names\n",
    "4) Merge metadata onto traffic records (inner join so every station has metadata)\n",
    "5) Verify timestamp parsing and basic integrity checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2e82616-9813-4b63-9281-dd4766488c4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T20:32:05.462983Z",
     "iopub.status.busy": "2026-02-07T20:32:05.462422Z",
     "iopub.status.idle": "2026-02-07T20:32:05.468279Z",
     "shell.execute_reply": "2026-02-07T20:32:05.467288Z",
     "shell.execute_reply.started": "2026-02-07T20:32:05.462959Z"
    }
   },
   "outputs": [],
   "source": [
    "def require_col(df: pd.DataFrame, candidates, friendly_name: str):\n",
    "    \"\"\"\n",
    "    Find the first matching column in candidates.\n",
    "    Raise a helpful error if not found.\n",
    "    \"\"\"\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise KeyError(\n",
    "        f\"Could not find column for '{friendly_name}'. Tried: {candidates}\\n\"\n",
    "        f\"Available columns: {list(df.columns)}\"\n",
    "    )\n",
    "\n",
    "def to_datetime_safe(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_datetime(s, errors=\"coerce\")\n",
    "\n",
    "def pct_missing(s: pd.Series) -> float:\n",
    "    return float(s.isna().mean() * 100.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "891ee42c-db17-4a5a-8760-c2d5307dcc93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T20:32:15.478248Z",
     "iopub.status.busy": "2026-02-07T20:32:15.477319Z",
     "iopub.status.idle": "2026-02-07T20:32:35.416432Z",
     "shell.execute_reply": "2026-02-07T20:32:35.415675Z",
     "shell.execute_reply.started": "2026-02-07T20:32:15.478222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traffic shape: (4114680, 42)\n",
      "Meta shape: (1861, 15)\n",
      "After basic parsing: (4114680, 42)\n",
      "Timestamp range: 2024-10-01 00:00:00 → 2024-12-31 23:00:00\n",
      "Meta columns (peek): ['Fwy', 'District', 'County', 'City', 'CA PM', 'Abs PM', 'Length', 'station', 'Name', 'Lanes', 'Type', 'Sensor Type', 'HOV', 'MS ID', 'IRM']\n",
      "Merged df shape: (4051621, 56)\n",
      "Unique stations: 1861\n"
     ]
    }
   ],
   "source": [
    "traffic_raw = pd.read_csv(TRAFFIC_CSV)\n",
    "meta_raw = pd.read_excel(META_XLSX)\n",
    "\n",
    "print(\"Traffic shape:\", traffic_raw.shape)\n",
    "print(\"Meta shape:\", meta_raw.shape)\n",
    "\n",
    "# --- Identify expected columns robustly ---\n",
    "ts_col   = require_col(traffic_raw, [\"Timestamp\", \"timestamp\", \"Time\", \"Datetime\"], \"Timestamp\")\n",
    "st_col   = require_col(traffic_raw, [\"Station\", \"station\", \"ID\"], \"Station ID\")\n",
    "flow_col = require_col(traffic_raw, [\"Total Flow\", \"total_flow\", \"Flow\", \"total flow\"], \"Total Flow\")\n",
    "spd_col  = require_col(traffic_raw, [\"Avg Speed\", \"avg_speed\", \"Speed\", \"Avg speed\"], \"Avg Speed\")\n",
    "\n",
    "lane_col = require_col(traffic_raw, [\"Lane Type\", \"lane_type\", \"LaneType\"], \"Lane Type\")\n",
    "dir_col  = require_col(traffic_raw, [\"Direction of Travel\", \"direction\", \"Dir\"], \"Direction\")\n",
    "dist_col = require_col(traffic_raw, [\"District\", \"district\"], \"District\")\n",
    "\n",
    "# --- Standardize traffic ---\n",
    "traffic = traffic_raw.rename(columns={\n",
    "    ts_col: \"timestamp\",\n",
    "    st_col: \"station\",\n",
    "    flow_col: \"total_flow\",\n",
    "    spd_col: \"avg_speed\",\n",
    "    lane_col: \"lane_type\",\n",
    "    dir_col: \"direction\",\n",
    "    dist_col: \"district\",\n",
    "}).copy()\n",
    "\n",
    "traffic[\"timestamp\"] = to_datetime_safe(traffic[\"timestamp\"])\n",
    "traffic[\"station\"] = pd.to_numeric(traffic[\"station\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "traffic = traffic.dropna(subset=[\"timestamp\", \"station\"]).copy()\n",
    "traffic[\"station\"] = traffic[\"station\"].astype(int)\n",
    "\n",
    "print(\"After basic parsing:\", traffic.shape)\n",
    "print(\"Timestamp range:\", traffic[\"timestamp\"].min(), \"→\", traffic[\"timestamp\"].max())\n",
    "\n",
    "# --- Standardize metadata ---\n",
    "# station id in metadata usually is 'ID'\n",
    "meta_id_col = require_col(meta_raw, [\"ID\", \"station\", \"Station\"], \"Meta Station ID\")\n",
    "meta = meta_raw.rename(columns={meta_id_col: \"station\"}).copy()\n",
    "meta[\"station\"] = pd.to_numeric(meta[\"station\"], errors=\"coerce\").astype(\"Int64\")\n",
    "meta = meta.dropna(subset=[\"station\"]).copy()\n",
    "meta[\"station\"] = meta[\"station\"].astype(int)\n",
    "\n",
    "print(\"Meta columns (peek):\", list(meta.columns)[:20])\n",
    "\n",
    "# Merge metadata (inner ensures we only keep stations that have metadata)\n",
    "df = traffic.merge(meta, on=\"station\", how=\"inner\", validate=\"m:1\")\n",
    "print(\"Merged df shape:\", df.shape)\n",
    "print(\"Unique stations:\", df[\"station\"].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f00d72-390f-4127-8fee-892c70b33868",
   "metadata": {},
   "source": [
    "## Sanity checks\n",
    "\n",
    "We check:\n",
    "- Duplicate rows per (timestamp, station)\n",
    "- Time frequency (hourly vs not)\n",
    "- Missingness rates\n",
    "These checks prevent silent data problems that can invalidate results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ccee1e2-e6e9-4140-835d-2d16cf1beabd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T20:33:17.465616Z",
     "iopub.status.busy": "2026-02-07T20:33:17.465333Z",
     "iopub.status.idle": "2026-02-07T20:33:17.896142Z",
     "shell.execute_reply": "2026-02-07T20:33:17.895372Z",
     "shell.execute_reply.started": "2026-02-07T20:33:17.465593Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate (timestamp, station) rows: 0\n",
      "Most common timestamp deltas:\n",
      " 0 days 01:00:00    2207\n",
      "Name: count, dtype: int64\n",
      "Missing total_flow (%): 7.243150334150209\n",
      "Missing avg_speed (%): 37.88496011843161\n"
     ]
    }
   ],
   "source": [
    "# 1) Duplicates by (timestamp, station)\n",
    "dup_count = df.duplicated(subset=[\"timestamp\", \"station\"]).sum()\n",
    "print(\"Duplicate (timestamp, station) rows:\", int(dup_count))\n",
    "\n",
    "if dup_count > 0:\n",
    "    # Resolve duplicates safely: flow sums, speed averages\n",
    "    df = (df.groupby([\"timestamp\", \"station\"], as_index=False)\n",
    "            .agg({\n",
    "                \"total_flow\": \"sum\",\n",
    "                \"avg_speed\": \"mean\",\n",
    "                \"lane_type\": \"first\",\n",
    "                \"direction\": \"first\",\n",
    "                \"district\": \"first\",\n",
    "                # keep metadata columns by first\n",
    "                **{c: \"first\" for c in meta.columns if c != \"station\"}\n",
    "            }))\n",
    "    print(\"After de-duplication:\", df.shape)\n",
    "\n",
    "# 2) Check time deltas\n",
    "times = pd.DatetimeIndex(sorted(df[\"timestamp\"].unique()))\n",
    "deltas = pd.Series(times[1:] - times[:-1]).value_counts().head(5)\n",
    "print(\"Most common timestamp deltas:\\n\", deltas)\n",
    "\n",
    "# 3) Missingness\n",
    "print(\"Missing total_flow (%):\", pct_missing(df[\"total_flow\"]))\n",
    "print(\"Missing avg_speed (%):\", pct_missing(df[\"avg_speed\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78bf0b1-3cbf-4a41-85bc-b09e0ecd29d9",
   "metadata": {},
   "source": [
    "## Build station-time matrices\n",
    "\n",
    "Graph models require a clean tensor format.\n",
    "We create two matrices:\n",
    "- Flow:  (T timestamps × N stations)\n",
    "- Speed: (T timestamps × N stations)\n",
    "\n",
    "We also select a stable station set using a coverage threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c1913df-2f73-4406-99cc-2cc7019f4f19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T20:37:39.730439Z",
     "iopub.status.busy": "2026-02-07T20:37:39.730115Z",
     "iopub.status.idle": "2026-02-07T20:37:47.228785Z",
     "shell.execute_reply": "2026-02-07T20:37:47.228035Z",
     "shell.execute_reply.started": "2026-02-07T20:37:39.730415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamps (T) = 2208\n",
      "Stations kept (N) = 1821  (coverage threshold=0.98)\n",
      "Flow matrix: (2208, 1821) Speed matrix: (2208, 1821)\n",
      "Flow missing fraction: 0.0712893656137335\n",
      "Speed missing fraction: 0.3772684720928937\n"
     ]
    }
   ],
   "source": [
    "# Full timestamp index\n",
    "all_times = pd.DatetimeIndex(sorted(df[\"timestamp\"].unique()))\n",
    "T = len(all_times)\n",
    "\n",
    "# Station coverage\n",
    "counts = df.groupby(\"station\")[\"timestamp\"].nunique()\n",
    "coverage = counts / T\n",
    "\n",
    "keep_stations = coverage[coverage >= COVERAGE_THRESHOLD].index\n",
    "df2 = df[df[\"station\"].isin(keep_stations)].copy()\n",
    "\n",
    "stations = np.array(sorted(df2[\"station\"].unique()), dtype=int)\n",
    "N = len(stations)\n",
    "\n",
    "print(f\"Timestamps (T) = {T}\")\n",
    "print(f\"Stations kept (N) = {N}  (coverage threshold={COVERAGE_THRESHOLD})\")\n",
    "\n",
    "# Build matrices\n",
    "flow = (df2.pivot(index=\"timestamp\", columns=\"station\", values=\"total_flow\")\n",
    "          .reindex(index=all_times, columns=stations)\n",
    "          .sort_index())\n",
    "\n",
    "speed = (df2.pivot(index=\"timestamp\", columns=\"station\", values=\"avg_speed\")\n",
    "           .reindex(index=all_times, columns=stations)\n",
    "           .sort_index())\n",
    "\n",
    "print(\"Flow matrix:\", flow.shape, \"Speed matrix:\", speed.shape)\n",
    "print(\"Flow missing fraction:\", float(np.isnan(flow.to_numpy()).mean()))\n",
    "print(\"Speed missing fraction:\", float(np.isnan(speed.to_numpy()).mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdefb51-4046-4fe5-8ae7-83654b1afbc3",
   "metadata": {},
   "source": [
    "## Leakage-safe imputation\n",
    "\n",
    "We must not use validation/test information when estimating fill values.\n",
    "\n",
    "Strategy:\n",
    "- Forward-fill across time (realistic streaming behavior).\n",
    "- Remaining NaNs filled using TRAIN statistics only.\n",
    "\n",
    "Flow:\n",
    "- ffill → fill with per-station TRAIN mean → fill with global TRAIN mean\n",
    "\n",
    "Speed:\n",
    "- ffill → fill using a TRAIN-only group lookup (lane_type, meta type, hour, fwy, district)\n",
    "- then per-station TRAIN mean → global TRAIN mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc0cc46e-f59e-436a-9538-76628782eeb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T20:38:52.379831Z",
     "iopub.status.busy": "2026-02-07T20:38:52.379494Z",
     "iopub.status.idle": "2026-02-07T20:39:31.394012Z",
     "shell.execute_reply": "2026-02-07T20:39:31.392919Z",
     "shell.execute_reply.started": "2026-02-07T20:38:52.379793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After imputation:\n",
      "Flow missing fraction: 0.0\n",
      "Speed missing fraction: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Identify metadata columns we need for speed lookup\n",
    "meta_type_col = None\n",
    "for cand in [\"Type\", \"type\", \"Station Type\"]:\n",
    "    if cand in df2.columns:\n",
    "        meta_type_col = cand\n",
    "        break\n",
    "\n",
    "fwy_col = None\n",
    "for cand in [\"Fwy\", \"FWY\", \"fwy\", \"Freeway\"]:\n",
    "    if cand in df2.columns:\n",
    "        fwy_col = cand\n",
    "        break\n",
    "\n",
    "if meta_type_col is None or fwy_col is None:\n",
    "    raise KeyError(f\"Missing metadata columns for speed lookup. Found meta_type={meta_type_col}, fwy={fwy_col}\")\n",
    "\n",
    "train_time_mask = flow.index <= TRAIN_END\n",
    "\n",
    "# -------------------------\n",
    "# Flow imputation\n",
    "# -------------------------\n",
    "flow_ff = flow.ffill()\n",
    "\n",
    "flow_train_mean_station = flow_ff.loc[train_time_mask].mean(axis=0)\n",
    "flow_train_mean_global = flow_ff.loc[train_time_mask].stack().mean()\n",
    "\n",
    "flow_imp = flow_ff.fillna(flow_train_mean_station).fillna(flow_train_mean_global)\n",
    "\n",
    "# -------------------------\n",
    "# Speed lookup (TRAIN only)\n",
    "# -------------------------\n",
    "train_rows = df2[df2[\"timestamp\"] <= TRAIN_END].copy()\n",
    "train_rows[\"hour\"] = train_rows[\"timestamp\"].dt.hour\n",
    "\n",
    "speed_grp_cols = [\"lane_type\", meta_type_col, \"hour\", fwy_col, \"district\"]\n",
    "speed_lookup = train_rows.groupby(speed_grp_cols)[\"avg_speed\"].mean()\n",
    "\n",
    "global_speed_train_mean = train_rows[\"avg_speed\"].mean()\n",
    "\n",
    "# Station-level \"mode\" descriptors used when applying the lookup\n",
    "station_info = (df2.groupby(\"station\")\n",
    "                  .agg(\n",
    "                      lane_type=(\"lane_type\", lambda x: x.mode().iloc[0] if len(x.mode()) else x.iloc[0]),\n",
    "                      meta_type=(meta_type_col, lambda x: x.mode().iloc[0] if len(x.mode()) else x.iloc[0]),\n",
    "                      fwy=(fwy_col, lambda x: x.mode().iloc[0] if len(x.mode()) else x.iloc[0]),\n",
    "                      district=(\"district\", lambda x: x.mode().iloc[0] if len(x.mode()) else x.iloc[0]),\n",
    "                  )\n",
    "                  .reindex(stations))\n",
    "\n",
    "speed_ff = speed.ffill()\n",
    "speed_np = speed_ff.to_numpy(dtype=np.float32)\n",
    "miss = np.isnan(speed_np)\n",
    "hours = speed_ff.index.hour.values\n",
    "\n",
    "# Fill with lookup\n",
    "for j, st in enumerate(stations):\n",
    "    if not miss[:, j].any():\n",
    "        continue\n",
    "    info = station_info.loc[st]\n",
    "    lane_type = info[\"lane_type\"]\n",
    "    meta_type = info[\"meta_type\"]\n",
    "    fwy = info[\"fwy\"]\n",
    "    district = info[\"district\"]\n",
    "\n",
    "    idxs = np.where(miss[:, j])[0]\n",
    "    fill_vals = []\n",
    "    for t_idx in idxs:\n",
    "        h = int(hours[t_idx])\n",
    "        key = (lane_type, meta_type, h, fwy, district)\n",
    "        fill_vals.append(speed_lookup.get(key, np.nan))\n",
    "    speed_np[idxs, j] = np.array(fill_vals, dtype=np.float32)\n",
    "\n",
    "speed_imp = pd.DataFrame(speed_np, index=speed_ff.index, columns=speed_ff.columns)\n",
    "\n",
    "# Remaining NaNs → station TRAIN mean → global TRAIN mean\n",
    "speed_train_mean_station = speed_imp.loc[train_time_mask].mean(axis=0)\n",
    "speed_imp = speed_imp.fillna(speed_train_mean_station).fillna(global_speed_train_mean)\n",
    "\n",
    "print(\"After imputation:\")\n",
    "print(\"Flow missing fraction:\", float(np.isnan(flow_imp.to_numpy()).mean()))\n",
    "print(\"Speed missing fraction:\", float(np.isnan(speed_imp.to_numpy()).mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4ef022-a962-4b61-8216-b9f447935673",
   "metadata": {},
   "source": [
    "## Build graph-ready tensors\n",
    "\n",
    "We create:\n",
    "- X: (T, N, F)\n",
    "  Features include:\n",
    "  - flow (1)\n",
    "  - speed (1)\n",
    "  - time encodings: hour_sin, hour_cos, dow_sin, dow_cos (4)\n",
    "  Total F = 6\n",
    "\n",
    "- Y: (T, N)\n",
    "  Target is flow at each station.\n",
    "\n",
    "Later, each training sample is a sliding window:\n",
    "- Input:  X[t : t+IN_LEN]\n",
    "- Output: Y[t+IN_LEN : t+IN_LEN+OUT_LEN]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "979fd9ed-9d39-4ad5-8941-d65abd464ec5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T20:39:31.395785Z",
     "iopub.status.busy": "2026-02-07T20:39:31.395279Z",
     "iopub.status.idle": "2026-02-07T20:39:31.523145Z",
     "shell.execute_reply": "2026-02-07T20:39:31.522242Z",
     "shell.execute_reply.started": "2026-02-07T20:39:31.395760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (2208, 1821, 6)  (T,N,F)\n",
      "Y shape: (2208, 1821)  (T,N)\n"
     ]
    }
   ],
   "source": [
    "def make_time_features(timestamps: pd.DatetimeIndex) -> np.ndarray:\n",
    "    hours = timestamps.hour.values\n",
    "    dow   = timestamps.dayofweek.values\n",
    "    hour_sin = np.sin(2*np.pi*hours/24.0)\n",
    "    hour_cos = np.cos(2*np.pi*hours/24.0)\n",
    "    dow_sin  = np.sin(2*np.pi*dow/7.0)\n",
    "    dow_cos  = np.cos(2*np.pi*dow/7.0)\n",
    "    return np.stack([hour_sin, hour_cos, dow_sin, dow_cos], axis=1).astype(np.float32)  # (T,4)\n",
    "\n",
    "time_feats = make_time_features(flow_imp.index)  # (T,4)\n",
    "time_feats_b = np.repeat(time_feats[:, None, :], repeats=N, axis=1)  # (T,N,4)\n",
    "\n",
    "flow_arr  = flow_imp.to_numpy(dtype=np.float32)[:, :, None]   # (T,N,1)\n",
    "speed_arr = speed_imp.to_numpy(dtype=np.float32)[:, :, None]  # (T,N,1)\n",
    "\n",
    "X = np.concatenate([flow_arr, speed_arr, time_feats_b], axis=2)  # (T,N,6)\n",
    "Y = flow_arr.squeeze(-1).astype(np.float32)                      # (T,N)\n",
    "\n",
    "print(\"X shape:\", X.shape, \" (T,N,F)\")\n",
    "print(\"Y shape:\", Y.shape, \" (T,N)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316168d6-874e-4fb3-a6f0-9de8d05c5830",
   "metadata": {},
   "source": [
    "## Build adjacency matrix A (static graph baseline)\n",
    "\n",
    "We build a physical-neighborhood adjacency using metadata:\n",
    "- Sort stations by (freeway, absolute postmile)\n",
    "- Connect K neighbors upstream + downstream\n",
    "- Weight edges using a Gaussian kernel of distance\n",
    "- Add self-loops\n",
    "\n",
    "Note:\n",
    "GraphWaveNet can also learn an adaptive adjacency; this static graph is a strong baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "def5e213-a0b5-43ea-b332-25a80dfaa72b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T20:40:39.110976Z",
     "iopub.status.busy": "2026-02-07T20:40:39.110692Z",
     "iopub.status.idle": "2026-02-07T20:40:39.239413Z",
     "shell.execute_reply": "2026-02-07T20:40:39.238547Z",
     "shell.execute_reply.started": "2026-02-07T20:40:39.110955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A shape: (1821, 1821)\n",
      "Adjacency density (A>0): 0.0023693916932872663\n"
     ]
    }
   ],
   "source": [
    "def build_adjacency_from_metadata(meta_df: pd.DataFrame, stations: np.ndarray, k_neighbors: int = 2) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build adjacency within each freeway chain using Abs PM order.\n",
    "    Edge weights = exp(-(dist^2 / sigma^2)), sigma = median neighbor distance.\n",
    "    \"\"\"\n",
    "    # Find needed meta columns\n",
    "    id_col = \"station\"\n",
    "    abs_pm_col = None\n",
    "    for cand in [\"Abs PM\", \"abs_pm\", \"AbsPM\", \"Postmile\", \"PM\"]:\n",
    "        if cand in meta_df.columns:\n",
    "            abs_pm_col = cand\n",
    "            break\n",
    "    fwy_col2 = None\n",
    "    for cand in [\"Fwy\", \"FWY\", \"fwy\", \"Freeway\"]:\n",
    "        if cand in meta_df.columns:\n",
    "            fwy_col2 = cand\n",
    "            break\n",
    "\n",
    "    if abs_pm_col is None or fwy_col2 is None:\n",
    "        raise KeyError(f\"Metadata missing Abs PM or Fwy columns. Found AbsPM={abs_pm_col}, Fwy={fwy_col2}\")\n",
    "\n",
    "    meta_sub = meta_df[meta_df[id_col].isin(stations)].copy()\n",
    "    meta_sub[\"abs_pm\"] = pd.to_numeric(meta_sub[abs_pm_col], errors=\"coerce\")\n",
    "    meta_sub[\"fwy\"] = meta_sub[fwy_col2].astype(str)\n",
    "\n",
    "    # station index map\n",
    "    station_to_idx = {s: i for i, s in enumerate(stations)}\n",
    "    N = len(stations)\n",
    "    A = np.zeros((N, N), dtype=np.float32)\n",
    "\n",
    "    # estimate sigma from typical neighbor distances\n",
    "    all_dists = []\n",
    "    for fwy, grp in meta_sub.sort_values([\"fwy\", \"abs_pm\"]).groupby(\"fwy\"):\n",
    "        pm = grp[\"abs_pm\"].dropna().values\n",
    "        if len(pm) < 2:\n",
    "            continue\n",
    "        d = np.diff(np.sort(pm))\n",
    "        d = d[d > 0]\n",
    "        all_dists.extend(d.tolist())\n",
    "\n",
    "    sigma = float(np.median(all_dists)) if len(all_dists) else 0.5\n",
    "    sigma = max(sigma, 1e-3)\n",
    "\n",
    "    def w(dist):  # gaussian weight\n",
    "        return float(np.exp(- (dist**2) / (sigma**2)))\n",
    "\n",
    "    # connect neighbors\n",
    "    for fwy, grp in meta_sub.sort_values([\"fwy\", \"abs_pm\"]).groupby(\"fwy\"):\n",
    "        grp = grp.dropna(subset=[\"abs_pm\"]).sort_values(\"abs_pm\")\n",
    "        ids = grp[id_col].astype(int).tolist()\n",
    "        pms = grp[\"abs_pm\"].astype(float).tolist()\n",
    "\n",
    "        for i, sid in enumerate(ids):\n",
    "            ii = station_to_idx[sid]\n",
    "            for step in range(1, k_neighbors + 1):\n",
    "                if i - step >= 0:\n",
    "                    sj = ids[i - step]; jj = station_to_idx[sj]\n",
    "                    A[ii, jj] = w(abs(pms[i] - pms[i-step]))\n",
    "                if i + step < len(ids):\n",
    "                    sj = ids[i + step]; jj = station_to_idx[sj]\n",
    "                    A[ii, jj] = w(abs(pms[i] - pms[i+step]))\n",
    "\n",
    "    # self loops + symmetrize\n",
    "    np.fill_diagonal(A, 1.0)\n",
    "    A = np.maximum(A, A.T)\n",
    "    return A\n",
    "\n",
    "# metadata table for adjacency should be meta with standardized station column\n",
    "meta_for_adj = meta.copy()\n",
    "meta_for_adj[\"station\"] = meta_for_adj[\"station\"].astype(int)\n",
    "\n",
    "A = build_adjacency_from_metadata(meta_for_adj, stations=stations, k_neighbors=K_NEIGHBORS)\n",
    "print(\"A shape:\", A.shape)\n",
    "print(\"Adjacency density (A>0):\", float((A > 0).mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf7f0ba-39f7-4563-a319-8809315ec010",
   "metadata": {},
   "source": [
    "## Sliding windows + splits\n",
    "\n",
    "Each sample uses:\n",
    "- Input window:  X[t : t+IN_LEN]\n",
    "- Output window: Y[t+IN_LEN : t+IN_LEN+OUT_LEN]\n",
    "\n",
    "We split by the **time of the first predicted hour** (t + IN_LEN):\n",
    "- Train if output_start_time ≤ TRAIN_END\n",
    "- Val   if TRAIN_END < output_start_time ≤ VAL_END\n",
    "- Test  if output_start_time > VAL_END\n",
    "\n",
    "Then we save everything to a single `.npz` artifact so every model reads the exact same dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ae1abeb-d866-4754-84c2-8e9e7c9353ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T20:41:15.441072Z",
     "iopub.status.busy": "2026-02-07T20:41:15.440758Z",
     "iopub.status.idle": "2026-02-07T20:41:19.936565Z",
     "shell.execute_reply": "2026-02-07T20:41:19.935601Z",
     "shell.execute_reply.started": "2026-02-07T20:41:15.441048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window starts: train=1080, val=360, test=673\n",
      "Saved: artifacts/pems_graph_dataset.npz\n"
     ]
    }
   ],
   "source": [
    "# Sliding window starts\n",
    "T_total = X.shape[0]\n",
    "max_t = T_total - (IN_LEN + OUT_LEN) + 1\n",
    "starts = np.arange(max_t, dtype=np.int32)\n",
    "\n",
    "timestamps = pd.DatetimeIndex(flow_imp.index)\n",
    "out_start_times = timestamps[starts + IN_LEN]\n",
    "\n",
    "train_starts = starts[out_start_times <= TRAIN_END]\n",
    "val_starts   = starts[(out_start_times > TRAIN_END) & (out_start_times <= VAL_END)]\n",
    "test_starts  = starts[out_start_times > VAL_END]\n",
    "\n",
    "print(f\"Window starts: train={len(train_starts)}, val={len(val_starts)}, test={len(test_starts)}\")\n",
    "\n",
    "# Train-only scalers (per node) for flow and speed (channels 0 and 1)\n",
    "train_time_mask = timestamps <= TRAIN_END\n",
    "\n",
    "flow_mean = X[train_time_mask, :, 0].mean(axis=0).astype(np.float32)\n",
    "flow_std  = (X[train_time_mask, :, 0].std(axis=0) + 1e-6).astype(np.float32)\n",
    "\n",
    "speed_mean = X[train_time_mask, :, 1].mean(axis=0).astype(np.float32)\n",
    "speed_std  = (X[train_time_mask, :, 1].std(axis=0) + 1e-6).astype(np.float32)\n",
    "\n",
    "np.savez_compressed(\n",
    "    DATASET_NPZ,\n",
    "    X=X.astype(np.float32),\n",
    "    Y=Y.astype(np.float32),\n",
    "    A=A.astype(np.float32),\n",
    "    stations=stations.astype(np.int32),\n",
    "    timestamps=np.array(timestamps.astype(\"datetime64[ns]\")),\n",
    "    train_starts=train_starts,\n",
    "    val_starts=val_starts,\n",
    "    test_starts=test_starts,\n",
    "    in_len=np.array([IN_LEN], dtype=np.int32),\n",
    "    out_len=np.array([OUT_LEN], dtype=np.int32),\n",
    "    flow_mean=flow_mean, flow_std=flow_std,\n",
    "    speed_mean=speed_mean, speed_std=speed_std,\n",
    "    seed=np.array([SEED], dtype=np.int32),\n",
    ")\n",
    "\n",
    "print(\"Saved:\", DATASET_NPZ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604d7ebe-5650-4bac-b5be-468fe9cfb24d",
   "metadata": {},
   "source": [
    "## Fix window split leakage (strict horizon containment)\n",
    "\n",
    "A window starting at time t uses:\n",
    "- Input:  X[t : t+IN_LEN]\n",
    "- Output: Y[t+IN_LEN : t+IN_LEN+OUT_LEN]\n",
    "\n",
    "To prevent label leakage across train/val/test boundaries, we require:\n",
    "- Train: output_end_time ≤ TRAIN_END\n",
    "- Val:   output_start_time > TRAIN_END AND output_end_time ≤ VAL_END\n",
    "- Test:  output_start_time > VAL_END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26396172-e9bd-4411-b484-0b5dd724bd1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T21:10:34.519135Z",
     "iopub.status.busy": "2026-02-07T21:10:34.518819Z",
     "iopub.status.idle": "2026-02-07T21:10:39.611410Z",
     "shell.execute_reply": "2026-02-07T21:10:39.607849Z",
     "shell.execute_reply.started": "2026-02-07T21:10:34.519115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRICT window starts:\n",
      "train: 1009\n",
      "val:   289\n",
      "test:  673\n",
      "Saved strict dataset to: artifacts/pems_graph_dataset_strict.npz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATASET_NPZ = Path(\"artifacts/pems_graph_dataset.npz\")\n",
    "DATASET_NPZ_STRICT = Path(\"artifacts/pems_graph_dataset_strict.npz\")\n",
    "\n",
    "data = np.load(DATASET_NPZ, allow_pickle=True)\n",
    "\n",
    "X = data[\"X\"]\n",
    "Y = data[\"Y\"]\n",
    "A = data[\"A\"]\n",
    "stations = data[\"stations\"]\n",
    "timestamps = pd.to_datetime(data[\"timestamps\"])\n",
    "\n",
    "IN_LEN = int(data[\"in_len\"][0])\n",
    "OUT_LEN = int(data[\"out_len\"][0])\n",
    "\n",
    "flow_mean = data[\"flow_mean\"]\n",
    "flow_std  = data[\"flow_std\"]\n",
    "speed_mean = data[\"speed_mean\"]\n",
    "speed_std  = data[\"speed_std\"]\n",
    "\n",
    "T_total = X.shape[0]\n",
    "max_t = T_total - (IN_LEN + OUT_LEN) + 1\n",
    "starts = np.arange(max_t, dtype=np.int32)\n",
    "\n",
    "out_start_times = timestamps[starts + IN_LEN]\n",
    "out_end_times   = timestamps[starts + IN_LEN + OUT_LEN - 1]\n",
    "\n",
    "TRAIN_END = pd.Timestamp(\"2024-11-15 23:59:59\")\n",
    "VAL_END   = pd.Timestamp(\"2024-11-30 23:59:59\")\n",
    "\n",
    "# Strict splits\n",
    "train_starts = starts[out_end_times <= TRAIN_END]\n",
    "val_starts   = starts[(out_start_times > TRAIN_END) & (out_end_times <= VAL_END)]\n",
    "test_starts  = starts[out_start_times > VAL_END]\n",
    "\n",
    "print(\"STRICT window starts:\")\n",
    "print(\"train:\", len(train_starts))\n",
    "print(\"val:  \", len(val_starts))\n",
    "print(\"test: \", len(test_starts))\n",
    "\n",
    "np.savez_compressed(\n",
    "    DATASET_NPZ_STRICT,\n",
    "    X=X.astype(np.float32),\n",
    "    Y=Y.astype(np.float32),\n",
    "    A=A.astype(np.float32),\n",
    "    stations=stations.astype(np.int32),\n",
    "    timestamps=np.array(timestamps.astype(\"datetime64[ns]\")),\n",
    "    train_starts=train_starts,\n",
    "    val_starts=val_starts,\n",
    "    test_starts=test_starts,\n",
    "    in_len=np.array([IN_LEN], dtype=np.int32),\n",
    "    out_len=np.array([OUT_LEN], dtype=np.int32),\n",
    "    flow_mean=flow_mean.astype(np.float32),\n",
    "    flow_std=flow_std.astype(np.float32),\n",
    "    speed_mean=speed_mean.astype(np.float32),\n",
    "    speed_std=speed_std.astype(np.float32),\n",
    ")\n",
    "\n",
    "print(\"Saved strict dataset to:\", DATASET_NPZ_STRICT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb6355d-15db-480a-88c0-4504c380b16a",
   "metadata": {},
   "source": [
    "## Load strict dataset artifact\n",
    "\n",
    "We will only use the strict `.npz` going forward to ensure no leakage in labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5515da5a-67a7-47dd-a481-a12fc40907e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T21:11:06.047641Z",
     "iopub.status.busy": "2026-02-07T21:11:06.047230Z",
     "iopub.status.idle": "2026-02-07T21:11:06.540671Z",
     "shell.execute_reply": "2026-02-07T21:11:06.539927Z",
     "shell.execute_reply.started": "2026-02-07T21:11:06.047617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (2208, 1821, 6) Y: (2208, 1821) A: (1821, 1821)\n",
      "starts: 1009 289 673\n",
      "IN_LEN: 24 OUT_LEN: 72\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATASET_NPZ_STRICT = Path(\"artifacts/pems_graph_dataset_strict.npz\")\n",
    "d = np.load(DATASET_NPZ_STRICT, allow_pickle=True)\n",
    "\n",
    "X = d[\"X\"]          # (T,N,F)\n",
    "Y = d[\"Y\"]          # (T,N)\n",
    "A = d[\"A\"]          # (N,N)\n",
    "stations = d[\"stations\"]\n",
    "timestamps = pd.to_datetime(d[\"timestamps\"])\n",
    "\n",
    "train_starts = d[\"train_starts\"]\n",
    "val_starts   = d[\"val_starts\"]\n",
    "test_starts  = d[\"test_starts\"]\n",
    "\n",
    "IN_LEN = int(d[\"in_len\"][0])\n",
    "OUT_LEN = int(d[\"out_len\"][0])\n",
    "\n",
    "flow_mean = d[\"flow_mean\"]  # (N,)\n",
    "flow_std  = d[\"flow_std\"]   # (N,)\n",
    "\n",
    "print(\"X:\", X.shape, \"Y:\", Y.shape, \"A:\", A.shape)\n",
    "print(\"starts:\", len(train_starts), len(val_starts), len(test_starts))\n",
    "print(\"IN_LEN:\", IN_LEN, \"OUT_LEN:\", OUT_LEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ada36a4-6634-47c0-98c5-7353202d241c",
   "metadata": {},
   "source": [
    "## Baseline evaluation\n",
    "\n",
    "We evaluate at horizons: 12, 24, 48, 72 hours ahead.\n",
    "\n",
    "Important detail:\n",
    "Our output sequence begins at +1 hour ahead of the last input time.\n",
    "So horizon `h` corresponds to output index `h-1` in the 72-step target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "737f6784-238b-4c35-a5bc-ea736646d0b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T21:12:53.206372Z",
     "iopub.status.busy": "2026-02-07T21:12:53.205790Z",
     "iopub.status.idle": "2026-02-07T21:12:53.212959Z",
     "shell.execute_reply": "2026-02-07T21:12:53.211829Z",
     "shell.execute_reply.started": "2026-02-07T21:12:53.206347Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "EVAL_HORIZONS = [12, 24, 48, 72]\n",
    "\n",
    "def init_metric_accumulators(horizons):\n",
    "    return {\n",
    "        h: {\"abs_sum\": 0.0, \"sq_sum\": 0.0, \"count\": 0}\n",
    "        for h in horizons\n",
    "    }\n",
    "\n",
    "def finalize_metrics(acc):\n",
    "    out = {}\n",
    "    for h, v in acc.items():\n",
    "        mae = v[\"abs_sum\"] / max(v[\"count\"], 1)\n",
    "        rmse = np.sqrt(v[\"sq_sum\"] / max(v[\"count\"], 1))\n",
    "        out[h] = {\"MAE\": mae, \"RMSE\": rmse}\n",
    "    return out\n",
    "\n",
    "def print_metrics(title, metrics_dict):\n",
    "    print(\"\\n\" + title)\n",
    "    for h in sorted(metrics_dict.keys()):\n",
    "        print(f\"  {h:>3}h  MAE={metrics_dict[h]['MAE']:.3f}  RMSE={metrics_dict[h]['RMSE']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd500bf-d0f5-4205-85c3-2c7a169a645d",
   "metadata": {},
   "source": [
    "### Baseline 1 — Persistence\n",
    "\n",
    "For each station:\n",
    "- Predict that all future horizons equal the **last observed flow** in the input window.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8e7a280-81bd-4fea-8134-396e459526ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T21:14:18.584055Z",
     "iopub.status.busy": "2026-02-07T21:14:18.583297Z",
     "iopub.status.idle": "2026-02-07T21:14:18.770971Z",
     "shell.execute_reply": "2026-02-07T21:14:18.769750Z",
     "shell.execute_reply.started": "2026-02-07T21:14:18.584029Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49fa4fe544b943cfaf7ccc68a472cdd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Persistence (val):   0%|          | 0/289 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8248ff923c3a46dba30cc6d3c4398268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Persistence (test):   0%|          | 0/673 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Persistence — Validation\n",
      "   12h  MAE=910.133  RMSE=1437.403\n",
      "   24h  MAE=151.755  RMSE=354.476\n",
      "   48h  MAE=203.020  RMSE=451.021\n",
      "   72h  MAE=220.150  RMSE=478.588\n",
      "\n",
      "Persistence — Test\n",
      "   12h  MAE=917.014  RMSE=1455.414\n",
      "   24h  MAE=147.896  RMSE=340.247\n",
      "   48h  MAE=200.592  RMSE=443.364\n",
      "   72h  MAE=196.856  RMSE=431.611\n"
     ]
    }
   ],
   "source": [
    "def eval_persistence(X, Y, starts, in_len, horizons, desc=\"\"):\n",
    "    acc = init_metric_accumulators(horizons)\n",
    "\n",
    "    for t in tqdm(starts, desc=desc):\n",
    "        # last observed flow at the end of input window\n",
    "        last_flow = X[t + in_len - 1, :, 0]  # (N,)\n",
    "\n",
    "        for h in horizons:\n",
    "            idx = h - 1\n",
    "            true = Y[t + in_len + idx, :]     # (N,)\n",
    "            pred = last_flow                  # (N,)\n",
    "\n",
    "            err = pred - true\n",
    "            acc[h][\"abs_sum\"] += float(np.abs(err).sum())\n",
    "            acc[h][\"sq_sum\"]  += float((err ** 2).sum())\n",
    "            acc[h][\"count\"]   += err.size\n",
    "\n",
    "    return finalize_metrics(acc)\n",
    "\n",
    "pers_val  = eval_persistence(X, Y, val_starts, IN_LEN, EVAL_HORIZONS, desc=\"Persistence (val)\")\n",
    "pers_test = eval_persistence(X, Y, test_starts, IN_LEN, EVAL_HORIZONS, desc=\"Persistence (test)\")\n",
    "\n",
    "print_metrics(\"Persistence — Validation\", pers_val)\n",
    "print_metrics(\"Persistence — Test\", pers_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61797eea-3aad-422c-ab05-4cfc275c3280",
   "metadata": {},
   "source": [
    "### Baseline 2 — Historical Average (HA-168)\n",
    "\n",
    "We compute a per-node seasonal mean using train data only:\n",
    "- slot = (day_of_week * 24 + hour) ∈ [0..167]\n",
    "- mean_flow[slot, node] = average flow in train for that slot\n",
    "\n",
    "Forecast:\n",
    "- For each horizon step, use the slot mean of that future timestamp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f1ecda2-abdd-4241-bb4e-3215a5cb224d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T21:15:48.783046Z",
     "iopub.status.busy": "2026-02-07T21:15:48.782558Z",
     "iopub.status.idle": "2026-02-07T21:15:48.972239Z",
     "shell.execute_reply": "2026-02-07T21:15:48.971202Z",
     "shell.execute_reply.started": "2026-02-07T21:15:48.783009Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab956bdc0c8415f836d0662d4fb5523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HA-168 (val):   0%|          | 0/289 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c08a3390fe74619a202d791c8b2c9dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HA-168 (test):   0%|          | 0/673 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HA-168 — Validation\n",
      "   12h  MAE=116.424  RMSE=258.349\n",
      "   24h  MAE=123.997  RMSE=277.816\n",
      "   48h  MAE=134.454  RMSE=302.695\n",
      "   72h  MAE=137.523  RMSE=306.085\n",
      "\n",
      "HA-168 — Test\n",
      "   12h  MAE=119.650  RMSE=283.458\n",
      "   24h  MAE=120.382  RMSE=284.451\n",
      "   48h  MAE=122.195  RMSE=286.807\n",
      "   72h  MAE=126.187  RMSE=293.463\n"
     ]
    }
   ],
   "source": [
    "def build_ha168_means(Y, timestamps, train_end):\n",
    "    train_mask = timestamps <= train_end\n",
    "    Y_train = Y[train_mask]  # (T_train, N)\n",
    "    ts_train = timestamps[train_mask]\n",
    "\n",
    "    slot = ts_train.dayofweek.to_numpy() * 24 + ts_train.hour.to_numpy()  # (T_train,)\n",
    "    G = 168\n",
    "    N = Y.shape[1]\n",
    "\n",
    "    means = np.zeros((G, N), dtype=np.float32)\n",
    "    counts = np.zeros((G,), dtype=np.int64)\n",
    "\n",
    "    for g in range(G):\n",
    "        m = (slot == g)\n",
    "        if m.any():\n",
    "            means[g] = Y_train[m].mean(axis=0)\n",
    "            counts[g] = int(m.sum())\n",
    "        else:\n",
    "            # fallback (should be rare)\n",
    "            means[g] = Y_train.mean(axis=0)\n",
    "            counts[g] = 0\n",
    "    return means, counts\n",
    "\n",
    "def eval_ha168(Y, timestamps, starts, in_len, horizons, means_ha168, desc=\"\"):\n",
    "    acc = init_metric_accumulators(horizons)\n",
    "\n",
    "    for t in tqdm(starts, desc=desc):\n",
    "        for h in horizons:\n",
    "            idx = h - 1\n",
    "            future_time = timestamps[t + in_len + idx]\n",
    "            g = int(future_time.dayofweek * 24 + future_time.hour)\n",
    "\n",
    "            pred = means_ha168[g, :]                 # (N,)\n",
    "            true = Y[t + in_len + idx, :]            # (N,)\n",
    "\n",
    "            err = pred - true\n",
    "            acc[h][\"abs_sum\"] += float(np.abs(err).sum())\n",
    "            acc[h][\"sq_sum\"]  += float((err ** 2).sum())\n",
    "            acc[h][\"count\"]   += err.size\n",
    "\n",
    "    return finalize_metrics(acc)\n",
    "\n",
    "TRAIN_END = pd.Timestamp(\"2024-11-15 23:59:59\")\n",
    "ha_means, ha_counts = build_ha168_means(Y, timestamps, TRAIN_END)\n",
    "\n",
    "ha_val  = eval_ha168(Y, timestamps, val_starts, IN_LEN, EVAL_HORIZONS, ha_means, desc=\"HA-168 (val)\")\n",
    "ha_test = eval_ha168(Y, timestamps, test_starts, IN_LEN, EVAL_HORIZONS, ha_means, desc=\"HA-168 (test)\")\n",
    "\n",
    "print_metrics(\"HA-168 — Validation\", ha_val)\n",
    "print_metrics(\"HA-168 — Test\", ha_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcc74e4-add9-43ab-a18c-123e4dc5142b",
   "metadata": {},
   "source": [
    "## PyTorch Dataset for sliding windows\n",
    "\n",
    "Each item returns:\n",
    "- x: (C, N, IN_LEN)   where C = number of features (6)\n",
    "- y: (OUT_LEN, N)     scaled flow targets\n",
    "\n",
    "Scaling:\n",
    "- We scale flow and speed using TRAIN-only mean/std (per node)\n",
    "- Time features remain unchanged (already bounded by sin/cos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2c00304-797e-43d5-b857-245c266f37d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T21:16:13.532982Z",
     "iopub.status.busy": "2026-02-07T21:16:13.532696Z",
     "iopub.status.idle": "2026-02-07T21:16:13.906511Z",
     "shell.execute_reply": "2026-02-07T21:16:13.905799Z",
     "shell.execute_reply.started": "2026-02-07T21:16:13.532959Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch x: torch.Size([16, 6, 1821, 24]) Batch y: torch.Size([16, 72, 1821])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PemsWindowDataset(Dataset):\n",
    "    def __init__(self, X, Y, starts, in_len, out_len, flow_mean, flow_std, speed_mean, speed_std):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.starts = starts\n",
    "        self.in_len = in_len\n",
    "        self.out_len = out_len\n",
    "\n",
    "        self.flow_mean = flow_mean.astype(np.float32)\n",
    "        self.flow_std  = flow_std.astype(np.float32)\n",
    "        self.speed_mean = speed_mean.astype(np.float32)\n",
    "        self.speed_std  = speed_std.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.starts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t = int(self.starts[idx])\n",
    "\n",
    "        x = self.X[t : t + self.in_len].copy().astype(np.float32)  # (IN_LEN, N, F)\n",
    "        y = self.Y[t + self.in_len : t + self.in_len + self.out_len].copy().astype(np.float32)  # (OUT_LEN, N)\n",
    "\n",
    "        # scale input channels: flow=0, speed=1\n",
    "        x[..., 0] = (x[..., 0] - self.flow_mean[None, :]) / self.flow_std[None, :]\n",
    "        x[..., 1] = (x[..., 1] - self.speed_mean[None, :]) / self.speed_std[None, :]\n",
    "\n",
    "        # scale targets (flow)\n",
    "        y = (y - self.flow_mean[None, :]) / self.flow_std[None, :]\n",
    "\n",
    "        # rearrange x to (C, N, IN_LEN) for conv models\n",
    "        x = np.transpose(x, (2, 1, 0))  # (F, N, IN_LEN)\n",
    "\n",
    "        return torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "# Load speed scaler too\n",
    "speed_mean = d[\"speed_mean\"]\n",
    "speed_std  = d[\"speed_std\"]\n",
    "\n",
    "train_ds = PemsWindowDataset(X, Y, train_starts, IN_LEN, OUT_LEN, flow_mean, flow_std, speed_mean, speed_std)\n",
    "val_ds   = PemsWindowDataset(X, Y, val_starts,   IN_LEN, OUT_LEN, flow_mean, flow_std, speed_mean, speed_std)\n",
    "test_ds  = PemsWindowDataset(X, Y, test_starts,  IN_LEN, OUT_LEN, flow_mean, flow_std, speed_mean, speed_std)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "xb, yb = next(iter(train_loader))\n",
    "print(\"Batch x:\", xb.shape, \"Batch y:\", yb.shape)\n",
    "# Expect: x=(B, 6, N, 24) and y=(B, 72, N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fac0c314-8f62-4624-9d71-37bed7c48631",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T21:57:23.849329Z",
     "iopub.status.busy": "2026-02-07T21:57:23.849007Z",
     "iopub.status.idle": "2026-02-07T21:57:23.916084Z",
     "shell.execute_reply": "2026-02-07T21:57:23.915230Z",
     "shell.execute_reply.started": "2026-02-07T21:57:23.849307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean |Δ| over 1h shift: 156.388\n",
      "Mean |Δ| over 6h shift: 666.328\n",
      "Mean |Δ| over 12h shift: 921.578\n",
      "Mean |Δ| over 24h shift: 143.347\n",
      "Mean |Δ| over 48h shift: 202.141\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mean_abs_change(Y, h):\n",
    "    return float(np.abs(Y[h:] - Y[:-h]).mean())\n",
    "\n",
    "for h in [1, 6, 12, 24, 48]:\n",
    "    print(f\"Mean |Δ| over {h}h shift: {mean_abs_change(Y, h):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7bb9a1-ed40-4e4c-aca1-cdb694fe9ab4",
   "metadata": {},
   "source": [
    "## Graph supports (normalized adjacency)\n",
    "\n",
    "GraphWaveNet uses graph propagation through adjacency matrices (\"supports\").\n",
    "We will build:\n",
    "- A_rw  = row-normalized adjacency (random-walk normalization)\n",
    "- A_rwT = transpose support (helps if graph is directed; still ok for symmetric graphs)\n",
    "\n",
    "We store them as sparse tensors for speed (our adjacency is very sparse).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a205f5bd-ad93-49f4-ab2f-ff60b6e200bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T21:59:44.385916Z",
     "iopub.status.busy": "2026-02-07T21:59:44.385011Z",
     "iopub.status.idle": "2026-02-07T21:59:44.530577Z",
     "shell.execute_reply": "2026-02-07T21:59:44.529646Z",
     "shell.execute_reply.started": "2026-02-07T21:59:44.385891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supports: [torch.Size([1821, 1821]), torch.Size([1821, 1821])] nnz: [7856, 7856]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def row_normalize_dense(A: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    d = A.sum(axis=1, keepdims=True)\n",
    "    return A / (d + eps)\n",
    "\n",
    "def dense_to_torch_sparse(A: np.ndarray, device: str):\n",
    "    A = A.astype(np.float32)\n",
    "    idx = np.nonzero(A)\n",
    "    values = A[idx]\n",
    "    indices = np.vstack(idx)  # (2, nnz)\n",
    "\n",
    "    indices = torch.tensor(indices, dtype=torch.long, device=device)\n",
    "    values  = torch.tensor(values, dtype=torch.float32, device=device)\n",
    "    shape = A.shape\n",
    "\n",
    "    sp = torch.sparse_coo_tensor(indices, values, size=shape, device=device).coalesce()\n",
    "    return sp\n",
    "\n",
    "A_rw = row_normalize_dense(A)\n",
    "A_rwT = row_normalize_dense(A.T)\n",
    "\n",
    "supports = [\n",
    "    dense_to_torch_sparse(A_rw, DEVICE),\n",
    "    dense_to_torch_sparse(A_rwT, DEVICE),\n",
    "]\n",
    "\n",
    "print(\"Supports:\", [s.shape for s in supports], \"nnz:\", [int(s._nnz()) for s in supports])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01d37d9-e314-48ec-8a9b-0368538be343",
   "metadata": {},
   "source": [
    "## Diffusion graph convolution (sparse)\n",
    "\n",
    "We need a fast way to compute:\n",
    "A @ X  (graph propagation)\n",
    "\n",
    "X is batched with shape (B, C, N, T).\n",
    "We reshape into (N, B*C*T) so sparse matrix multiply works efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6b573ca-1a08-4222-b75b-c6829443ea64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T22:00:11.218542Z",
     "iopub.status.busy": "2026-02-07T22:00:11.218057Z",
     "iopub.status.idle": "2026-02-07T22:00:11.226774Z",
     "shell.execute_reply": "2026-02-07T22:00:11.225879Z",
     "shell.execute_reply.started": "2026-02-07T22:00:11.218517Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NConv(nn.Module):\n",
    "    \"\"\"Sparse graph multiplication: (N,N) @ (B,C,N,T) -> (B,C,N,T)\"\"\"\n",
    "    def forward(self, x, A_sp):\n",
    "        # x: (B,C,N,T)\n",
    "        B, C, N, T = x.shape\n",
    "        x_r = x.permute(2, 0, 1, 3).reshape(N, -1)      # (N, B*C*T)\n",
    "        x_r = torch.sparse.mm(A_sp, x_r)                # (N, B*C*T)\n",
    "        x_out = x_r.reshape(N, B, C, T).permute(1, 2, 0, 3)  # (B,C,N,T)\n",
    "        return x_out\n",
    "\n",
    "class DiffusionGraphConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Diffusion graph conv:\n",
    "    concat [X, A1X, A2X, ...] then 1x1 conv.\n",
    "\n",
    "    order=1 means we use only one hop per support (fast and stable).\n",
    "    \"\"\"\n",
    "    def __init__(self, c_in, c_out, supports, order=1, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.nconv = NConv()\n",
    "        self.supports = supports\n",
    "        self.order = order\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # total input channels after concatenation\n",
    "        c_total = c_in * (1 + len(supports) * order)\n",
    "        self.mlp = nn.Conv2d(c_total, c_out, kernel_size=(1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = [x]\n",
    "        for A in self.supports:\n",
    "            x1 = self.nconv(x, A)\n",
    "            out.append(x1)\n",
    "            for _ in range(2, self.order + 1):\n",
    "                x1 = self.nconv(x1, A)\n",
    "                out.append(x1)\n",
    "\n",
    "        h = torch.cat(out, dim=1)\n",
    "        h = self.mlp(h)\n",
    "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90555df0-af2c-407e-b789-20335ccd6ad3",
   "metadata": {},
   "source": [
    "## GraphWaveNet baseline model\n",
    "\n",
    "We use:\n",
    "- Causal dilated temporal convolutions (TCN-style)\n",
    "- Gated activations (tanh * sigmoid)\n",
    "- Residual + skip connections\n",
    "- Diffusion graph convolution inside each layer\n",
    "\n",
    "Output head:\n",
    "- Uses the final time step embedding to predict OUT_LEN horizons directly\n",
    "- Output shape: (B, OUT_LEN, N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21a86ee7-6121-4910-ab01-db89a3a19e2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T22:04:50.463959Z",
     "iopub.status.busy": "2026-02-07T22:04:50.463528Z",
     "iopub.status.idle": "2026-02-07T22:04:50.478854Z",
     "shell.execute_reply": "2026-02-07T22:04:50.477780Z",
     "shell.execute_reply.started": "2026-02-07T22:04:50.463930Z"
    }
   },
   "outputs": [],
   "source": [
    "class CausalConv2d(nn.Module):\n",
    "    \"\"\"Causal conv along time axis only (last dimension).\"\"\"\n",
    "    def __init__(self, c_in, c_out, kernel_size=2, dilation=1):\n",
    "        super().__init__()\n",
    "        self.pad = (kernel_size - 1) * dilation\n",
    "        self.conv = nn.Conv2d(\n",
    "            c_in, c_out,\n",
    "            kernel_size=(1, kernel_size),\n",
    "            dilation=(1, dilation)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pad left on time dimension: (left_pad, right_pad, top, bottom) for 2d -> (time_left, time_right, node_left, node_right)\n",
    "        x = F.pad(x, (self.pad, 0, 0, 0))\n",
    "        return self.conv(x)\n",
    "\n",
    "class GraphWaveNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_nodes: int,\n",
    "        in_dim: int,\n",
    "        out_len: int,\n",
    "        supports,\n",
    "        residual_channels=32,\n",
    "        dilation_channels=32,\n",
    "        skip_channels=64,\n",
    "        end_channels=128,\n",
    "        kernel_size=2,\n",
    "        blocks=2,\n",
    "        layers_per_block=3,\n",
    "        gcn_order=1,\n",
    "        dropout=0.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.in_dim = in_dim\n",
    "        self.out_len = out_len\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.start_conv = nn.Conv2d(in_dim, residual_channels, kernel_size=(1, 1))\n",
    "\n",
    "        self.filter_convs = nn.ModuleList()\n",
    "        self.gate_convs   = nn.ModuleList()\n",
    "        self.res_convs    = nn.ModuleList()\n",
    "        self.skip_convs   = nn.ModuleList()\n",
    "        self.bn           = nn.ModuleList()\n",
    "        self.gconvs       = nn.ModuleList()\n",
    "\n",
    "        # Build temporal + graph blocks\n",
    "        for _ in range(blocks):\n",
    "            for i in range(layers_per_block):\n",
    "                dilation = 2 ** i\n",
    "\n",
    "                self.filter_convs.append(CausalConv2d(residual_channels, dilation_channels, kernel_size, dilation))\n",
    "                self.gate_convs.append(CausalConv2d(residual_channels, dilation_channels, kernel_size, dilation))\n",
    "\n",
    "                self.res_convs.append(nn.Conv2d(dilation_channels, residual_channels, kernel_size=(1, 1)))\n",
    "                self.skip_convs.append(nn.Conv2d(dilation_channels, skip_channels, kernel_size=(1, 1)))\n",
    "\n",
    "                self.gconvs.append(\n",
    "                    DiffusionGraphConv(dilation_channels, residual_channels, supports, order=gcn_order, dropout=dropout)\n",
    "                )\n",
    "\n",
    "                self.bn.append(nn.BatchNorm2d(residual_channels))\n",
    "\n",
    "        self.end_conv_1 = nn.Conv2d(skip_channels, end_channels, kernel_size=(1, 1))\n",
    "        self.end_conv_2 = nn.Conv2d(end_channels, out_len, kernel_size=(1, 1))  # outputs OUT_LEN channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, F, N, T_in)\n",
    "        return: (B, OUT_LEN, N)\n",
    "        \"\"\"\n",
    "        x = self.start_conv(x)  # (B, residual, N, T)\n",
    "        skip = None\n",
    "\n",
    "        for i in range(len(self.filter_convs)):\n",
    "            residual = x\n",
    "\n",
    "            # gated TCN\n",
    "            filt = torch.tanh(self.filter_convs[i](x))\n",
    "            gate = torch.sigmoid(self.gate_convs[i](x))\n",
    "            x = filt * gate\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "            # skip\n",
    "            s = self.skip_convs[i](x)\n",
    "            skip = s if skip is None else (skip + s)\n",
    "\n",
    "            # graph conv -> residual channels\n",
    "            x = self.gconvs[i](x)\n",
    "\n",
    "            # residual connection (time length is preserved by causal padding)\n",
    "            x = x + residual\n",
    "            x = self.bn[i](x)\n",
    "\n",
    "        x = F.relu(skip)\n",
    "        x = F.relu(self.end_conv_1(x))\n",
    "\n",
    "        # Use last time step to predict future horizons\n",
    "        x_last = x[..., -1:].contiguous()          # (B, end_channels, N, 1)\n",
    "        out = self.end_conv_2(x_last).squeeze(-1)  # (B, OUT_LEN, N)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937a1415-bd0e-41d1-9020-7e38b9a329e2",
   "metadata": {},
   "source": [
    "## Training & evaluation loop\n",
    "\n",
    "We train using MSE on scaled targets (stable optimization),\n",
    "then compute MAE/RMSE on the original scale at horizons 12/24/48/72.\n",
    "\n",
    "Early stopping monitors average validation MAE across horizons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f25c2c5-41a5-4944-be65-bc89dd8a13c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T22:19:03.564943Z",
     "iopub.status.busy": "2026-02-07T22:19:03.564622Z",
     "iopub.status.idle": "2026-02-07T22:39:25.208699Z",
     "shell.execute_reply": "2026-02-07T22:39:25.207909Z",
     "shell.execute_reply.started": "2026-02-07T22:19:03.564921Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ab6472702d435694f36e4deac1d1b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "980a9332521b48d7815184e6f0e7c747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss=0.529129  val_avg_MAE=203.973\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=225.832  RMSE=393.051\n",
      "   24h  MAE=184.901  RMSE=343.538\n",
      "   48h  MAE=208.168  RMSE=379.516\n",
      "   72h  MAE=196.993  RMSE=369.422\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb6c6ec16c124129a7fcdc834952f9de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a076df23f04d2fa0e008ca01d6d064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss=0.325922  val_avg_MAE=185.356\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=193.387  RMSE=348.977\n",
      "   24h  MAE=166.734  RMSE=319.229\n",
      "   48h  MAE=185.582  RMSE=349.533\n",
      "   72h  MAE=195.720  RMSE=369.310\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491a7e91c8e5492d98cac243d4e14e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d577d368f05448ab97b31f0d01929de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss=0.287739  val_avg_MAE=180.951\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=179.654  RMSE=333.382\n",
      "   24h  MAE=157.967  RMSE=309.919\n",
      "   48h  MAE=187.718  RMSE=347.820\n",
      "   72h  MAE=198.465  RMSE=367.385\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06deca55dca4e5aa09d631507ec9b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d0e9bbbaddf4b49870b54da71a1f864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train_loss=0.268916  val_avg_MAE=175.876\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=186.728  RMSE=342.408\n",
      "   24h  MAE=154.416  RMSE=309.196\n",
      "   48h  MAE=171.948  RMSE=328.877\n",
      "   72h  MAE=190.413  RMSE=357.469\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be87cd92ff8443796bbc85255e98952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2ee564a5b64113ac9e02ed39401539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: train_loss=0.257975  val_avg_MAE=170.299\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=184.271  RMSE=338.048\n",
      "   24h  MAE=148.209  RMSE=299.617\n",
      "   48h  MAE=168.291  RMSE=323.040\n",
      "   72h  MAE=180.426  RMSE=343.043\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8a08f28ea04bf193504fbf316bf607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef25c89d44c47c3bdfac3e06acfd025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: train_loss=0.250068  val_avg_MAE=164.475\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=162.037  RMSE=303.679\n",
      "   24h  MAE=150.514  RMSE=301.402\n",
      "   48h  MAE=164.907  RMSE=316.562\n",
      "   72h  MAE=180.444  RMSE=343.314\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221d56ecd7bf48158f7c8e6c7fccf845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f10d31b6ed954c099a6f40153beea0fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: train_loss=0.244371  val_avg_MAE=167.873\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=164.548  RMSE=308.877\n",
      "   24h  MAE=156.601  RMSE=304.825\n",
      "   48h  MAE=176.487  RMSE=327.605\n",
      "   72h  MAE=173.856  RMSE=331.363\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc03844fc2e499a994cf2d2e8723459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5121344730d45cca619fc294c6a65ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: train_loss=0.239806  val_avg_MAE=160.063\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=160.283  RMSE=302.982\n",
      "   24h  MAE=148.200  RMSE=298.973\n",
      "   48h  MAE=161.825  RMSE=314.139\n",
      "   72h  MAE=169.943  RMSE=325.858\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe87bf5bcf0d458080d7970e67f046ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e70d0d3058c4b2e96963a75d951c006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: train_loss=0.234704  val_avg_MAE=163.708\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=158.480  RMSE=303.029\n",
      "   24h  MAE=158.671  RMSE=308.487\n",
      "   48h  MAE=163.579  RMSE=313.825\n",
      "   72h  MAE=174.101  RMSE=332.094\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1c22d395b441018b83008d3e3bae4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "208b7bf85e04448d8fd6b1cb9670adca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train_loss=0.234201  val_avg_MAE=155.655\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=153.175  RMSE=294.953\n",
      "   24h  MAE=141.512  RMSE=293.586\n",
      "   48h  MAE=159.561  RMSE=315.417\n",
      "   72h  MAE=168.371  RMSE=323.676\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d981020918f49f2b4eb1c6be10ab020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0863d122755d440b8aa7f42a953e9943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: train_loss=0.231096  val_avg_MAE=154.549\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=152.754  RMSE=293.402\n",
      "   24h  MAE=139.974  RMSE=287.064\n",
      "   48h  MAE=157.335  RMSE=307.804\n",
      "   72h  MAE=168.134  RMSE=323.799\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ebd152c3c94941814572045d6a0646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb14cd1a94e4b60b0e3d9db38cd1980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: train_loss=0.229034  val_avg_MAE=164.792\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=151.243  RMSE=291.496\n",
      "   24h  MAE=147.712  RMSE=297.619\n",
      "   48h  MAE=167.106  RMSE=321.370\n",
      "   72h  MAE=193.109  RMSE=358.011\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71483139dfd4492193841d2d40300a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b5cf6455764f83900159e26f91a438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: train_loss=0.223892  val_avg_MAE=164.343\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=148.523  RMSE=289.870\n",
      "   24h  MAE=146.654  RMSE=296.672\n",
      "   48h  MAE=173.357  RMSE=328.243\n",
      "   72h  MAE=188.839  RMSE=347.545\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4932205875a843888c1c00ad43dfa3fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d5da2bbe944fcca18e36d44f3fa7dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: train_loss=0.222705  val_avg_MAE=166.283\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=153.376  RMSE=294.916\n",
      "   24h  MAE=155.567  RMSE=312.574\n",
      "   48h  MAE=176.343  RMSE=332.107\n",
      "   72h  MAE=179.847  RMSE=333.986\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe223285f59049948fae5bcdfbca00a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f13fc0332146d5a5f1627fcab5ba7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: train_loss=0.221832  val_avg_MAE=163.332\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=169.177  RMSE=314.841\n",
      "   24h  MAE=143.233  RMSE=290.326\n",
      "   48h  MAE=168.777  RMSE=325.777\n",
      "   72h  MAE=172.140  RMSE=329.309\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732e182a6ff14a22b91901bb24989380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95238e9cc1f346f3aaad86a8acd8456f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: train_loss=0.221454  val_avg_MAE=155.481\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=144.691  RMSE=279.696\n",
      "   24h  MAE=139.187  RMSE=284.711\n",
      "   48h  MAE=161.616  RMSE=311.975\n",
      "   72h  MAE=176.428  RMSE=331.767\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35154a1017dd48a38e86a9d2b076f1ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f0e536b7c3405eb42729c619559838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: train_loss=0.217759  val_avg_MAE=155.037\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=151.572  RMSE=293.354\n",
      "   24h  MAE=139.192  RMSE=286.145\n",
      "   48h  MAE=161.421  RMSE=316.536\n",
      "   72h  MAE=167.963  RMSE=321.738\n",
      "Early stopping at epoch 17. Best val_avg_MAE=154.549\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a0eebe30e2b4417b5d1366a8e47ad89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1014d6bab2704946b74a6266db129349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GraphWaveNet — Validation\n",
      "   12h  MAE=152.754  RMSE=293.402\n",
      "   24h  MAE=139.974  RMSE=287.064\n",
      "   48h  MAE=157.335  RMSE=307.804\n",
      "   72h  MAE=168.134  RMSE=323.799\n",
      "\n",
      "GraphWaveNet — Test\n",
      "   12h  MAE=156.900  RMSE=298.198\n",
      "   24h  MAE=134.966  RMSE=269.862\n",
      "   48h  MAE=153.469  RMSE=300.160\n",
      "   72h  MAE=161.221  RMSE=310.938\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# tensors for unscaling (put on device once)\n",
    "flow_mean_t = torch.tensor(flow_mean, dtype=torch.float32, device=DEVICE).view(1, 1, -1)\n",
    "flow_std_t  = torch.tensor(flow_std,  dtype=torch.float32, device=DEVICE).view(1, 1, -1)\n",
    "\n",
    "EVAL_HORIZONS = [12, 24, 48, 72]\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_horizons(model, loader):\n",
    "    model.eval()\n",
    "    acc = {h: {\"abs\": 0.0, \"sq\": 0.0, \"count\": 0} for h in EVAL_HORIZONS}\n",
    "\n",
    "    for xb, yb in tqdm(loader, desc=\"Eval\", leave=False):\n",
    "        xb = xb.to(DEVICE, non_blocking=True)   # (B,F,N,T)\n",
    "        yb = yb.to(DEVICE, non_blocking=True)   # (B,OUT,N) scaled\n",
    "\n",
    "        pred = model(xb)                        # (B,OUT,N) scaled\n",
    "\n",
    "        # unscale to original flow units\n",
    "        pred_u = pred * flow_std_t + flow_mean_t\n",
    "        true_u = yb   * flow_std_t + flow_mean_t\n",
    "\n",
    "        for h in EVAL_HORIZONS:\n",
    "            idx = h - 1\n",
    "            err = pred_u[:, idx, :] - true_u[:, idx, :]\n",
    "            acc[h][\"abs\"] += float(err.abs().sum())\n",
    "            acc[h][\"sq\"]  += float((err ** 2).sum())\n",
    "            acc[h][\"count\"] += err.numel()\n",
    "\n",
    "    metrics = {}\n",
    "    for h in EVAL_HORIZONS:\n",
    "        mae = acc[h][\"abs\"] / acc[h][\"count\"]\n",
    "        rmse = (acc[h][\"sq\"] / acc[h][\"count\"]) ** 0.5\n",
    "        metrics[h] = {\"MAE\": mae, \"RMSE\": rmse}\n",
    "    return metrics\n",
    "\n",
    "def print_metrics(title, metrics):\n",
    "    print(\"\\n\" + title)\n",
    "    for h in sorted(metrics.keys()):\n",
    "        print(f\"  {h:>3}h  MAE={metrics[h]['MAE']:.3f}  RMSE={metrics[h]['RMSE']:.3f}\")\n",
    "\n",
    "def avg_mae(metrics):\n",
    "    return float(np.mean([metrics[h][\"MAE\"] for h in metrics]))\n",
    "\n",
    "def train_gwn(\n",
    "    epochs=30,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    clip=5.0,\n",
    "    patience=6,\n",
    "):\n",
    "    model = GraphWaveNet(\n",
    "        num_nodes=X.shape[1],\n",
    "        in_dim=X.shape[2],     # 6 features\n",
    "        out_len=OUT_LEN,\n",
    "        supports=supports,\n",
    "        residual_channels=32,\n",
    "        dilation_channels=32,\n",
    "        skip_channels=64,\n",
    "        end_channels=128,\n",
    "        kernel_size=2,\n",
    "        blocks=2,\n",
    "        layers_per_block=3,\n",
    "        gcn_order=1,\n",
    "        dropout=0.3,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    use_amp = False\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=False)\n",
    "\n",
    "    best_score = float(\"inf\")\n",
    "    best_state = None\n",
    "    bad = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "\n",
    "        for xb, yb in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\", leave=False):\n",
    "            xb = xb.to(DEVICE, non_blocking=True)\n",
    "            yb = yb.to(DEVICE, non_blocking=True)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "                pred = model(xb)\n",
    "                loss = loss_fn(pred, yb)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(opt)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "\n",
    "            running += float(loss.item())\n",
    "\n",
    "        # validation metrics (original scale)\n",
    "        val_metrics = eval_horizons(model, val_loader)\n",
    "        score = avg_mae(val_metrics)\n",
    "\n",
    "        print(f\"Epoch {epoch}: train_loss={running/len(train_loader):.6f}  val_avg_MAE={score:.3f}\")\n",
    "        print_metrics(\"Val metrics\", val_metrics)\n",
    "\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}. Best val_avg_MAE={best_score:.3f}\")\n",
    "                break\n",
    "\n",
    "    # load best\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "gwn_model = train_gwn(epochs=30, lr=1e-3, weight_decay=1e-4, clip=5.0, patience=6)\n",
    "\n",
    "# Final evaluation\n",
    "val_m = eval_horizons(gwn_model, val_loader)\n",
    "test_m = eval_horizons(gwn_model, test_loader)\n",
    "\n",
    "print_metrics(\"GraphWaveNet — Validation\", val_m)\n",
    "print_metrics(\"GraphWaveNet — Test\", test_m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fa62ce-4074-4dbb-a733-6ac61f525cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b060ac7e-7976-46c8-af82-ff35c5da77be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9795e03-a5db-4abb-947f-65a818b13483",
   "metadata": {},
   "source": [
    "# Multi-Horizon Traffic Forecasting on PeMS (Graph Models)\n",
    "\n",
    "## Goal (Paper Claim)\n",
    "Build a leakage-safe, reproducible pipeline on PeMS traffic data and evaluate multi-horizon forecasting models fairly.\n",
    "\n",
    "Primary goal:\n",
    "- Demonstrate the proposed **GraphWaveNet-GRU-LSTM** performs best on PeMS under the same train/val/test protocol.\n",
    "\n",
    "Key principles:\n",
    "- No time leakage (all statistics computed from train only).\n",
    "- One shared dataset representation for all deep models: **X ∈ R^{T×N×F}, Y ∈ R^{T×N}**.\n",
    "- One fixed evaluation harness (same horizons, same metrics, same seeds).\n",
    "- Strong baselines + ablations:\n",
    "  - HA / Persistence\n",
    "  - GRU / LSTM (non-graph)\n",
    "  - GraphWaveNet\n",
    "  - GraphWaveNet+GRU\n",
    "  - GraphWaveNet+LSTM\n",
    "  - **GraphWaveNet+GRU+LSTM (proposed)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dceeef19-2130-4a23-931a-06196ed31757",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T19:36:12.615948Z",
     "iopub.status.busy": "2026-02-07T19:36:12.615649Z",
     "iopub.status.idle": "2026-02-07T19:36:20.583394Z",
     "shell.execute_reply": "2026-02-07T19:36:20.582642Z",
     "shell.execute_reply.started": "2026-02-07T19:36:12.615922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb15451a-d0ef-44ab-a5ae-c33eebe63c39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T19:36:29.430693Z",
     "iopub.status.busy": "2026-02-07T19:36:29.430355Z",
     "iopub.status.idle": "2026-02-07T19:36:33.161009Z",
     "shell.execute_reply": "2026-02-07T19:36:33.160283Z",
     "shell.execute_reply.started": "2026-02-07T19:36:29.430665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install numpy pandas openpyxl scikit-learn torch tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70d3f8bf-2c00-4b4c-acfb-5ce948648cf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T19:36:50.204022Z",
     "iopub.status.busy": "2026-02-07T19:36:50.203372Z",
     "iopub.status.idle": "2026-02-07T19:36:52.731520Z",
     "shell.execute_reply": "2026-02-07T19:36:52.730792Z",
     "shell.execute_reply.started": "2026-02-07T19:36:50.203991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.1.1+cu121\n",
      "Device: cuda\n",
      "GPU: Quadro P5000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def set_seed(seed: int = 42, deterministic: bool = True):\n",
    "    \"\"\"\n",
    "    Sets seeds for reproducibility.\n",
    "    deterministic=True makes results more reproducible but can reduce speed.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    if deterministic:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    else:\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED, deterministic=True)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Device:\", DEVICE)\n",
    "if DEVICE == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91ddbd0-0ab3-4784-8fe2-930de04d11c3",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "We fix:\n",
    "- Input window length (`IN_LEN`) and forecast horizon length (`OUT_LEN`)\n",
    "- Train/val/test boundaries (time-based split)\n",
    "- Station inclusion rule (coverage threshold)\n",
    "- Output dataset artifact path (so every model uses the same processed dataset)\n",
    "\n",
    "Important:\n",
    "GraphWaveNet expects a consistent node set and continuous time axis,\n",
    "so we build a clean matrix format (timestamp × station).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "907ef442-d82f-4601-ba92-8f0b62467aa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T20:30:36.024501Z",
     "iopub.status.busy": "2026-02-07T20:30:36.023555Z",
     "iopub.status.idle": "2026-02-07T20:30:36.031533Z",
     "shell.execute_reply": "2026-02-07T20:30:36.030825Z",
     "shell.execute_reply.started": "2026-02-07T20:30:36.024473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will save processed dataset to: artifacts/pems_graph_dataset.npz\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Paths (your files are visible in the Paperspace file pane)\n",
    "# -------------------------\n",
    "TRAFFIC_CSV = Path(\"cleaned_traffic_data.csv\")\n",
    "META_XLSX   = Path(\"pems_output.xlsx\")\n",
    "\n",
    "assert TRAFFIC_CSV.exists(), f\"Missing {TRAFFIC_CSV}\"\n",
    "assert META_XLSX.exists(), f\"Missing {META_XLSX}\"\n",
    "\n",
    "# -------------------------\n",
    "# Split boundaries (same as your earlier work)\n",
    "# -------------------------\n",
    "TRAIN_END = pd.Timestamp(\"2024-11-15 23:59:59\")\n",
    "VAL_END   = pd.Timestamp(\"2024-11-30 23:59:59\")\n",
    "\n",
    "# -------------------------\n",
    "# Forecast setup\n",
    "# -------------------------\n",
    "IN_LEN  = 24     # hours of history used as input\n",
    "OUT_LEN = 72     # predict next 72 hours (we will evaluate at 12/24/48/72)\n",
    "\n",
    "EVAL_HORIZONS = [12, 24, 48, 72]  # hours ahead\n",
    "\n",
    "# -------------------------\n",
    "# Station coverage threshold\n",
    "# -------------------------\n",
    "# 1.0 means station must have ALL timestamps present.\n",
    "# 0.98 is often a good compromise if some stations are missing few points.\n",
    "COVERAGE_THRESHOLD = 0.98\n",
    "\n",
    "# -------------------------\n",
    "# Adjacency setup (static graph baseline)\n",
    "# -------------------------\n",
    "K_NEIGHBORS = 2   # connect up to 2 upstream + 2 downstream along the freeway chain\n",
    "\n",
    "# -------------------------\n",
    "# Output artifact (important for reproducibility)\n",
    "# -------------------------\n",
    "OUT_DIR = Path(\"artifacts\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "DATASET_NPZ = OUT_DIR / \"pems_graph_dataset.npz\"\n",
    "print(\"Will save processed dataset to:\", DATASET_NPZ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714660ee-bbf4-44ce-b128-8d23ca17d379",
   "metadata": {},
   "source": [
    "## Load raw traffic + metadata\n",
    "\n",
    "We:\n",
    "1) Load cleaned traffic data\n",
    "2) Load station metadata\n",
    "3) Standardize column names\n",
    "4) Merge metadata onto traffic records (inner join so every station has metadata)\n",
    "5) Verify timestamp parsing and basic integrity checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2e82616-9813-4b63-9281-dd4766488c4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T20:32:05.462983Z",
     "iopub.status.busy": "2026-02-07T20:32:05.462422Z",
     "iopub.status.idle": "2026-02-07T20:32:05.468279Z",
     "shell.execute_reply": "2026-02-07T20:32:05.467288Z",
     "shell.execute_reply.started": "2026-02-07T20:32:05.462959Z"
    }
   },
   "outputs": [],
   "source": [
    "def require_col(df: pd.DataFrame, candidates, friendly_name: str):\n",
    "    \"\"\"\n",
    "    Find the first matching column in candidates.\n",
    "    Raise a helpful error if not found.\n",
    "    \"\"\"\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise KeyError(\n",
    "        f\"Could not find column for '{friendly_name}'. Tried: {candidates}\\n\"\n",
    "        f\"Available columns: {list(df.columns)}\"\n",
    "    )\n",
    "\n",
    "def to_datetime_safe(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_datetime(s, errors=\"coerce\")\n",
    "\n",
    "def pct_missing(s: pd.Series) -> float:\n",
    "    return float(s.isna().mean() * 100.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "891ee42c-db17-4a5a-8760-c2d5307dcc93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T20:32:15.478248Z",
     "iopub.status.busy": "2026-02-07T20:32:15.477319Z",
     "iopub.status.idle": "2026-02-07T20:32:35.416432Z",
     "shell.execute_reply": "2026-02-07T20:32:35.415675Z",
     "shell.execute_reply.started": "2026-02-07T20:32:15.478222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traffic shape: (4114680, 42)\n",
      "Meta shape: (1861, 15)\n",
      "After basic parsing: (4114680, 42)\n",
      "Timestamp range: 2024-10-01 00:00:00 → 2024-12-31 23:00:00\n",
      "Meta columns (peek): ['Fwy', 'District', 'County', 'City', 'CA PM', 'Abs PM', 'Length', 'station', 'Name', 'Lanes', 'Type', 'Sensor Type', 'HOV', 'MS ID', 'IRM']\n",
      "Merged df shape: (4051621, 56)\n",
      "Unique stations: 1861\n"
     ]
    }
   ],
   "source": [
    "traffic_raw = pd.read_csv(TRAFFIC_CSV)\n",
    "meta_raw = pd.read_excel(META_XLSX)\n",
    "\n",
    "print(\"Traffic shape:\", traffic_raw.shape)\n",
    "print(\"Meta shape:\", meta_raw.shape)\n",
    "\n",
    "# --- Identify expected columns robustly ---\n",
    "ts_col   = require_col(traffic_raw, [\"Timestamp\", \"timestamp\", \"Time\", \"Datetime\"], \"Timestamp\")\n",
    "st_col   = require_col(traffic_raw, [\"Station\", \"station\", \"ID\"], \"Station ID\")\n",
    "flow_col = require_col(traffic_raw, [\"Total Flow\", \"total_flow\", \"Flow\", \"total flow\"], \"Total Flow\")\n",
    "spd_col  = require_col(traffic_raw, [\"Avg Speed\", \"avg_speed\", \"Speed\", \"Avg speed\"], \"Avg Speed\")\n",
    "\n",
    "lane_col = require_col(traffic_raw, [\"Lane Type\", \"lane_type\", \"LaneType\"], \"Lane Type\")\n",
    "dir_col  = require_col(traffic_raw, [\"Direction of Travel\", \"direction\", \"Dir\"], \"Direction\")\n",
    "dist_col = require_col(traffic_raw, [\"District\", \"district\"], \"District\")\n",
    "\n",
    "# --- Standardize traffic ---\n",
    "traffic = traffic_raw.rename(columns={\n",
    "    ts_col: \"timestamp\",\n",
    "    st_col: \"station\",\n",
    "    flow_col: \"total_flow\",\n",
    "    spd_col: \"avg_speed\",\n",
    "    lane_col: \"lane_type\",\n",
    "    dir_col: \"direction\",\n",
    "    dist_col: \"district\",\n",
    "}).copy()\n",
    "\n",
    "traffic[\"timestamp\"] = to_datetime_safe(traffic[\"timestamp\"])\n",
    "traffic[\"station\"] = pd.to_numeric(traffic[\"station\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "traffic = traffic.dropna(subset=[\"timestamp\", \"station\"]).copy()\n",
    "traffic[\"station\"] = traffic[\"station\"].astype(int)\n",
    "\n",
    "print(\"After basic parsing:\", traffic.shape)\n",
    "print(\"Timestamp range:\", traffic[\"timestamp\"].min(), \"→\", traffic[\"timestamp\"].max())\n",
    "\n",
    "# --- Standardize metadata ---\n",
    "# station id in metadata usually is 'ID'\n",
    "meta_id_col = require_col(meta_raw, [\"ID\", \"station\", \"Station\"], \"Meta Station ID\")\n",
    "meta = meta_raw.rename(columns={meta_id_col: \"station\"}).copy()\n",
    "meta[\"station\"] = pd.to_numeric(meta[\"station\"], errors=\"coerce\").astype(\"Int64\")\n",
    "meta = meta.dropna(subset=[\"station\"]).copy()\n",
    "meta[\"station\"] = meta[\"station\"].astype(int)\n",
    "\n",
    "print(\"Meta columns (peek):\", list(meta.columns)[:20])\n",
    "\n",
    "# Merge metadata (inner ensures we only keep stations that have metadata)\n",
    "df = traffic.merge(meta, on=\"station\", how=\"inner\", validate=\"m:1\")\n",
    "print(\"Merged df shape:\", df.shape)\n",
    "print(\"Unique stations:\", df[\"station\"].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f00d72-390f-4127-8fee-892c70b33868",
   "metadata": {},
   "source": [
    "## Sanity checks\n",
    "\n",
    "We check:\n",
    "- Duplicate rows per (timestamp, station)\n",
    "- Time frequency (hourly vs not)\n",
    "- Missingness rates\n",
    "These checks prevent silent data problems that can invalidate results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ccee1e2-e6e9-4140-835d-2d16cf1beabd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T20:33:17.465616Z",
     "iopub.status.busy": "2026-02-07T20:33:17.465333Z",
     "iopub.status.idle": "2026-02-07T20:33:17.896142Z",
     "shell.execute_reply": "2026-02-07T20:33:17.895372Z",
     "shell.execute_reply.started": "2026-02-07T20:33:17.465593Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate (timestamp, station) rows: 0\n",
      "Most common timestamp deltas:\n",
      " 0 days 01:00:00    2207\n",
      "Name: count, dtype: int64\n",
      "Missing total_flow (%): 7.243150334150209\n",
      "Missing avg_speed (%): 37.88496011843161\n"
     ]
    }
   ],
   "source": [
    "# 1) Duplicates by (timestamp, station)\n",
    "dup_count = df.duplicated(subset=[\"timestamp\", \"station\"]).sum()\n",
    "print(\"Duplicate (timestamp, station) rows:\", int(dup_count))\n",
    "\n",
    "if dup_count > 0:\n",
    "    # Resolve duplicates safely: flow sums, speed averages\n",
    "    df = (df.groupby([\"timestamp\", \"station\"], as_index=False)\n",
    "            .agg({\n",
    "                \"total_flow\": \"sum\",\n",
    "                \"avg_speed\": \"mean\",\n",
    "                \"lane_type\": \"first\",\n",
    "                \"direction\": \"first\",\n",
    "                \"district\": \"first\",\n",
    "                # keep metadata columns by first\n",
    "                **{c: \"first\" for c in meta.columns if c != \"station\"}\n",
    "            }))\n",
    "    print(\"After de-duplication:\", df.shape)\n",
    "\n",
    "# 2) Check time deltas\n",
    "times = pd.DatetimeIndex(sorted(df[\"timestamp\"].unique()))\n",
    "deltas = pd.Series(times[1:] - times[:-1]).value_counts().head(5)\n",
    "print(\"Most common timestamp deltas:\\n\", deltas)\n",
    "\n",
    "# 3) Missingness\n",
    "print(\"Missing total_flow (%):\", pct_missing(df[\"total_flow\"]))\n",
    "print(\"Missing avg_speed (%):\", pct_missing(df[\"avg_speed\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78bf0b1-3cbf-4a41-85bc-b09e0ecd29d9",
   "metadata": {},
   "source": [
    "## Build station-time matrices\n",
    "\n",
    "Graph models require a clean tensor format.\n",
    "We create two matrices:\n",
    "- Flow:  (T timestamps × N stations)\n",
    "- Speed: (T timestamps × N stations)\n",
    "\n",
    "We also select a stable station set using a coverage threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c1913df-2f73-4406-99cc-2cc7019f4f19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T20:37:39.730439Z",
     "iopub.status.busy": "2026-02-07T20:37:39.730115Z",
     "iopub.status.idle": "2026-02-07T20:37:47.228785Z",
     "shell.execute_reply": "2026-02-07T20:37:47.228035Z",
     "shell.execute_reply.started": "2026-02-07T20:37:39.730415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamps (T) = 2208\n",
      "Stations kept (N) = 1821  (coverage threshold=0.98)\n",
      "Flow matrix: (2208, 1821) Speed matrix: (2208, 1821)\n",
      "Flow missing fraction: 0.0712893656137335\n",
      "Speed missing fraction: 0.3772684720928937\n"
     ]
    }
   ],
   "source": [
    "# Full timestamp index\n",
    "all_times = pd.DatetimeIndex(sorted(df[\"timestamp\"].unique()))\n",
    "T = len(all_times)\n",
    "\n",
    "# Station coverage\n",
    "counts = df.groupby(\"station\")[\"timestamp\"].nunique()\n",
    "coverage = counts / T\n",
    "\n",
    "keep_stations = coverage[coverage >= COVERAGE_THRESHOLD].index\n",
    "df2 = df[df[\"station\"].isin(keep_stations)].copy()\n",
    "\n",
    "stations = np.array(sorted(df2[\"station\"].unique()), dtype=int)\n",
    "N = len(stations)\n",
    "\n",
    "print(f\"Timestamps (T) = {T}\")\n",
    "print(f\"Stations kept (N) = {N}  (coverage threshold={COVERAGE_THRESHOLD})\")\n",
    "\n",
    "# Build matrices\n",
    "flow = (df2.pivot(index=\"timestamp\", columns=\"station\", values=\"total_flow\")\n",
    "          .reindex(index=all_times, columns=stations)\n",
    "          .sort_index())\n",
    "\n",
    "speed = (df2.pivot(index=\"timestamp\", columns=\"station\", values=\"avg_speed\")\n",
    "           .reindex(index=all_times, columns=stations)\n",
    "           .sort_index())\n",
    "\n",
    "print(\"Flow matrix:\", flow.shape, \"Speed matrix:\", speed.shape)\n",
    "print(\"Flow missing fraction:\", float(np.isnan(flow.to_numpy()).mean()))\n",
    "print(\"Speed missing fraction:\", float(np.isnan(speed.to_numpy()).mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdefb51-4046-4fe5-8ae7-83654b1afbc3",
   "metadata": {},
   "source": [
    "## Leakage-safe imputation\n",
    "\n",
    "We must not use validation/test information when estimating fill values.\n",
    "\n",
    "Strategy:\n",
    "- Forward-fill across time (realistic streaming behavior).\n",
    "- Remaining NaNs filled using TRAIN statistics only.\n",
    "\n",
    "Flow:\n",
    "- ffill → fill with per-station TRAIN mean → fill with global TRAIN mean\n",
    "\n",
    "Speed:\n",
    "- ffill → fill using a TRAIN-only group lookup (lane_type, meta type, hour, fwy, district)\n",
    "- then per-station TRAIN mean → global TRAIN mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc0cc46e-f59e-436a-9538-76628782eeb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T20:38:52.379831Z",
     "iopub.status.busy": "2026-02-07T20:38:52.379494Z",
     "iopub.status.idle": "2026-02-07T20:39:31.394012Z",
     "shell.execute_reply": "2026-02-07T20:39:31.392919Z",
     "shell.execute_reply.started": "2026-02-07T20:38:52.379793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After imputation:\n",
      "Flow missing fraction: 0.0\n",
      "Speed missing fraction: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Identify metadata columns we need for speed lookup\n",
    "meta_type_col = None\n",
    "for cand in [\"Type\", \"type\", \"Station Type\"]:\n",
    "    if cand in df2.columns:\n",
    "        meta_type_col = cand\n",
    "        break\n",
    "\n",
    "fwy_col = None\n",
    "for cand in [\"Fwy\", \"FWY\", \"fwy\", \"Freeway\"]:\n",
    "    if cand in df2.columns:\n",
    "        fwy_col = cand\n",
    "        break\n",
    "\n",
    "if meta_type_col is None or fwy_col is None:\n",
    "    raise KeyError(f\"Missing metadata columns for speed lookup. Found meta_type={meta_type_col}, fwy={fwy_col}\")\n",
    "\n",
    "train_time_mask = flow.index <= TRAIN_END\n",
    "\n",
    "# -------------------------\n",
    "# Flow imputation\n",
    "# -------------------------\n",
    "flow_ff = flow.ffill()\n",
    "\n",
    "flow_train_mean_station = flow_ff.loc[train_time_mask].mean(axis=0)\n",
    "flow_train_mean_global = flow_ff.loc[train_time_mask].stack().mean()\n",
    "\n",
    "flow_imp = flow_ff.fillna(flow_train_mean_station).fillna(flow_train_mean_global)\n",
    "\n",
    "# -------------------------\n",
    "# Speed lookup (TRAIN only)\n",
    "# -------------------------\n",
    "train_rows = df2[df2[\"timestamp\"] <= TRAIN_END].copy()\n",
    "train_rows[\"hour\"] = train_rows[\"timestamp\"].dt.hour\n",
    "\n",
    "speed_grp_cols = [\"lane_type\", meta_type_col, \"hour\", fwy_col, \"district\"]\n",
    "speed_lookup = train_rows.groupby(speed_grp_cols)[\"avg_speed\"].mean()\n",
    "\n",
    "global_speed_train_mean = train_rows[\"avg_speed\"].mean()\n",
    "\n",
    "# Station-level \"mode\" descriptors used when applying the lookup\n",
    "station_info = (df2.groupby(\"station\")\n",
    "                  .agg(\n",
    "                      lane_type=(\"lane_type\", lambda x: x.mode().iloc[0] if len(x.mode()) else x.iloc[0]),\n",
    "                      meta_type=(meta_type_col, lambda x: x.mode().iloc[0] if len(x.mode()) else x.iloc[0]),\n",
    "                      fwy=(fwy_col, lambda x: x.mode().iloc[0] if len(x.mode()) else x.iloc[0]),\n",
    "                      district=(\"district\", lambda x: x.mode().iloc[0] if len(x.mode()) else x.iloc[0]),\n",
    "                  )\n",
    "                  .reindex(stations))\n",
    "\n",
    "speed_ff = speed.ffill()\n",
    "speed_np = speed_ff.to_numpy(dtype=np.float32)\n",
    "miss = np.isnan(speed_np)\n",
    "hours = speed_ff.index.hour.values\n",
    "\n",
    "# Fill with lookup\n",
    "for j, st in enumerate(stations):\n",
    "    if not miss[:, j].any():\n",
    "        continue\n",
    "    info = station_info.loc[st]\n",
    "    lane_type = info[\"lane_type\"]\n",
    "    meta_type = info[\"meta_type\"]\n",
    "    fwy = info[\"fwy\"]\n",
    "    district = info[\"district\"]\n",
    "\n",
    "    idxs = np.where(miss[:, j])[0]\n",
    "    fill_vals = []\n",
    "    for t_idx in idxs:\n",
    "        h = int(hours[t_idx])\n",
    "        key = (lane_type, meta_type, h, fwy, district)\n",
    "        fill_vals.append(speed_lookup.get(key, np.nan))\n",
    "    speed_np[idxs, j] = np.array(fill_vals, dtype=np.float32)\n",
    "\n",
    "speed_imp = pd.DataFrame(speed_np, index=speed_ff.index, columns=speed_ff.columns)\n",
    "\n",
    "# Remaining NaNs → station TRAIN mean → global TRAIN mean\n",
    "speed_train_mean_station = speed_imp.loc[train_time_mask].mean(axis=0)\n",
    "speed_imp = speed_imp.fillna(speed_train_mean_station).fillna(global_speed_train_mean)\n",
    "\n",
    "print(\"After imputation:\")\n",
    "print(\"Flow missing fraction:\", float(np.isnan(flow_imp.to_numpy()).mean()))\n",
    "print(\"Speed missing fraction:\", float(np.isnan(speed_imp.to_numpy()).mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4ef022-a962-4b61-8216-b9f447935673",
   "metadata": {},
   "source": [
    "## Build graph-ready tensors\n",
    "\n",
    "We create:\n",
    "- X: (T, N, F)\n",
    "  Features include:\n",
    "  - flow (1)\n",
    "  - speed (1)\n",
    "  - time encodings: hour_sin, hour_cos, dow_sin, dow_cos (4)\n",
    "  Total F = 6\n",
    "\n",
    "- Y: (T, N)\n",
    "  Target is flow at each station.\n",
    "\n",
    "Later, each training sample is a sliding window:\n",
    "- Input:  X[t : t+IN_LEN]\n",
    "- Output: Y[t+IN_LEN : t+IN_LEN+OUT_LEN]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "979fd9ed-9d39-4ad5-8941-d65abd464ec5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T20:39:31.395785Z",
     "iopub.status.busy": "2026-02-07T20:39:31.395279Z",
     "iopub.status.idle": "2026-02-07T20:39:31.523145Z",
     "shell.execute_reply": "2026-02-07T20:39:31.522242Z",
     "shell.execute_reply.started": "2026-02-07T20:39:31.395760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (2208, 1821, 6)  (T,N,F)\n",
      "Y shape: (2208, 1821)  (T,N)\n"
     ]
    }
   ],
   "source": [
    "def make_time_features(timestamps: pd.DatetimeIndex) -> np.ndarray:\n",
    "    hours = timestamps.hour.values\n",
    "    dow   = timestamps.dayofweek.values\n",
    "    hour_sin = np.sin(2*np.pi*hours/24.0)\n",
    "    hour_cos = np.cos(2*np.pi*hours/24.0)\n",
    "    dow_sin  = np.sin(2*np.pi*dow/7.0)\n",
    "    dow_cos  = np.cos(2*np.pi*dow/7.0)\n",
    "    return np.stack([hour_sin, hour_cos, dow_sin, dow_cos], axis=1).astype(np.float32)  # (T,4)\n",
    "\n",
    "time_feats = make_time_features(flow_imp.index)  # (T,4)\n",
    "time_feats_b = np.repeat(time_feats[:, None, :], repeats=N, axis=1)  # (T,N,4)\n",
    "\n",
    "flow_arr  = flow_imp.to_numpy(dtype=np.float32)[:, :, None]   # (T,N,1)\n",
    "speed_arr = speed_imp.to_numpy(dtype=np.float32)[:, :, None]  # (T,N,1)\n",
    "\n",
    "X = np.concatenate([flow_arr, speed_arr, time_feats_b], axis=2)  # (T,N,6)\n",
    "Y = flow_arr.squeeze(-1).astype(np.float32)                      # (T,N)\n",
    "\n",
    "print(\"X shape:\", X.shape, \" (T,N,F)\")\n",
    "print(\"Y shape:\", Y.shape, \" (T,N)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316168d6-874e-4fb3-a6f0-9de8d05c5830",
   "metadata": {},
   "source": [
    "## Build adjacency matrix A (static graph baseline)\n",
    "\n",
    "We build a physical-neighborhood adjacency using metadata:\n",
    "- Sort stations by (freeway, absolute postmile)\n",
    "- Connect K neighbors upstream + downstream\n",
    "- Weight edges using a Gaussian kernel of distance\n",
    "- Add self-loops\n",
    "\n",
    "Note:\n",
    "GraphWaveNet can also learn an adaptive adjacency; this static graph is a strong baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "def5e213-a0b5-43ea-b332-25a80dfaa72b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T20:40:39.110976Z",
     "iopub.status.busy": "2026-02-07T20:40:39.110692Z",
     "iopub.status.idle": "2026-02-07T20:40:39.239413Z",
     "shell.execute_reply": "2026-02-07T20:40:39.238547Z",
     "shell.execute_reply.started": "2026-02-07T20:40:39.110955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A shape: (1821, 1821)\n",
      "Adjacency density (A>0): 0.0023693916932872663\n"
     ]
    }
   ],
   "source": [
    "def build_adjacency_from_metadata(meta_df: pd.DataFrame, stations: np.ndarray, k_neighbors: int = 2) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build adjacency within each freeway chain using Abs PM order.\n",
    "    Edge weights = exp(-(dist^2 / sigma^2)), sigma = median neighbor distance.\n",
    "    \"\"\"\n",
    "    # Find needed meta columns\n",
    "    id_col = \"station\"\n",
    "    abs_pm_col = None\n",
    "    for cand in [\"Abs PM\", \"abs_pm\", \"AbsPM\", \"Postmile\", \"PM\"]:\n",
    "        if cand in meta_df.columns:\n",
    "            abs_pm_col = cand\n",
    "            break\n",
    "    fwy_col2 = None\n",
    "    for cand in [\"Fwy\", \"FWY\", \"fwy\", \"Freeway\"]:\n",
    "        if cand in meta_df.columns:\n",
    "            fwy_col2 = cand\n",
    "            break\n",
    "\n",
    "    if abs_pm_col is None or fwy_col2 is None:\n",
    "        raise KeyError(f\"Metadata missing Abs PM or Fwy columns. Found AbsPM={abs_pm_col}, Fwy={fwy_col2}\")\n",
    "\n",
    "    meta_sub = meta_df[meta_df[id_col].isin(stations)].copy()\n",
    "    meta_sub[\"abs_pm\"] = pd.to_numeric(meta_sub[abs_pm_col], errors=\"coerce\")\n",
    "    meta_sub[\"fwy\"] = meta_sub[fwy_col2].astype(str)\n",
    "\n",
    "    # station index map\n",
    "    station_to_idx = {s: i for i, s in enumerate(stations)}\n",
    "    N = len(stations)\n",
    "    A = np.zeros((N, N), dtype=np.float32)\n",
    "\n",
    "    # estimate sigma from typical neighbor distances\n",
    "    all_dists = []\n",
    "    for fwy, grp in meta_sub.sort_values([\"fwy\", \"abs_pm\"]).groupby(\"fwy\"):\n",
    "        pm = grp[\"abs_pm\"].dropna().values\n",
    "        if len(pm) < 2:\n",
    "            continue\n",
    "        d = np.diff(np.sort(pm))\n",
    "        d = d[d > 0]\n",
    "        all_dists.extend(d.tolist())\n",
    "\n",
    "    sigma = float(np.median(all_dists)) if len(all_dists) else 0.5\n",
    "    sigma = max(sigma, 1e-3)\n",
    "\n",
    "    def w(dist):  # gaussian weight\n",
    "        return float(np.exp(- (dist**2) / (sigma**2)))\n",
    "\n",
    "    # connect neighbors\n",
    "    for fwy, grp in meta_sub.sort_values([\"fwy\", \"abs_pm\"]).groupby(\"fwy\"):\n",
    "        grp = grp.dropna(subset=[\"abs_pm\"]).sort_values(\"abs_pm\")\n",
    "        ids = grp[id_col].astype(int).tolist()\n",
    "        pms = grp[\"abs_pm\"].astype(float).tolist()\n",
    "\n",
    "        for i, sid in enumerate(ids):\n",
    "            ii = station_to_idx[sid]\n",
    "            for step in range(1, k_neighbors + 1):\n",
    "                if i - step >= 0:\n",
    "                    sj = ids[i - step]; jj = station_to_idx[sj]\n",
    "                    A[ii, jj] = w(abs(pms[i] - pms[i-step]))\n",
    "                if i + step < len(ids):\n",
    "                    sj = ids[i + step]; jj = station_to_idx[sj]\n",
    "                    A[ii, jj] = w(abs(pms[i] - pms[i+step]))\n",
    "\n",
    "    # self loops + symmetrize\n",
    "    np.fill_diagonal(A, 1.0)\n",
    "    A = np.maximum(A, A.T)\n",
    "    return A\n",
    "\n",
    "# metadata table for adjacency should be meta with standardized station column\n",
    "meta_for_adj = meta.copy()\n",
    "meta_for_adj[\"station\"] = meta_for_adj[\"station\"].astype(int)\n",
    "\n",
    "A = build_adjacency_from_metadata(meta_for_adj, stations=stations, k_neighbors=K_NEIGHBORS)\n",
    "print(\"A shape:\", A.shape)\n",
    "print(\"Adjacency density (A>0):\", float((A > 0).mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf7f0ba-39f7-4563-a319-8809315ec010",
   "metadata": {},
   "source": [
    "## Sliding windows + splits\n",
    "\n",
    "Each sample uses:\n",
    "- Input window:  X[t : t+IN_LEN]\n",
    "- Output window: Y[t+IN_LEN : t+IN_LEN+OUT_LEN]\n",
    "\n",
    "We split by the **time of the first predicted hour** (t + IN_LEN):\n",
    "- Train if output_start_time ≤ TRAIN_END\n",
    "- Val   if TRAIN_END < output_start_time ≤ VAL_END\n",
    "- Test  if output_start_time > VAL_END\n",
    "\n",
    "Then we save everything to a single `.npz` artifact so every model reads the exact same dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ae1abeb-d866-4754-84c2-8e9e7c9353ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T20:41:15.441072Z",
     "iopub.status.busy": "2026-02-07T20:41:15.440758Z",
     "iopub.status.idle": "2026-02-07T20:41:19.936565Z",
     "shell.execute_reply": "2026-02-07T20:41:19.935601Z",
     "shell.execute_reply.started": "2026-02-07T20:41:15.441048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window starts: train=1080, val=360, test=673\n",
      "Saved: artifacts/pems_graph_dataset.npz\n"
     ]
    }
   ],
   "source": [
    "# Sliding window starts\n",
    "T_total = X.shape[0]\n",
    "max_t = T_total - (IN_LEN + OUT_LEN) + 1\n",
    "starts = np.arange(max_t, dtype=np.int32)\n",
    "\n",
    "timestamps = pd.DatetimeIndex(flow_imp.index)\n",
    "out_start_times = timestamps[starts + IN_LEN]\n",
    "\n",
    "train_starts = starts[out_start_times <= TRAIN_END]\n",
    "val_starts   = starts[(out_start_times > TRAIN_END) & (out_start_times <= VAL_END)]\n",
    "test_starts  = starts[out_start_times > VAL_END]\n",
    "\n",
    "print(f\"Window starts: train={len(train_starts)}, val={len(val_starts)}, test={len(test_starts)}\")\n",
    "\n",
    "# Train-only scalers (per node) for flow and speed (channels 0 and 1)\n",
    "train_time_mask = timestamps <= TRAIN_END\n",
    "\n",
    "flow_mean = X[train_time_mask, :, 0].mean(axis=0).astype(np.float32)\n",
    "flow_std  = (X[train_time_mask, :, 0].std(axis=0) + 1e-6).astype(np.float32)\n",
    "\n",
    "speed_mean = X[train_time_mask, :, 1].mean(axis=0).astype(np.float32)\n",
    "speed_std  = (X[train_time_mask, :, 1].std(axis=0) + 1e-6).astype(np.float32)\n",
    "\n",
    "np.savez_compressed(\n",
    "    DATASET_NPZ,\n",
    "    X=X.astype(np.float32),\n",
    "    Y=Y.astype(np.float32),\n",
    "    A=A.astype(np.float32),\n",
    "    stations=stations.astype(np.int32),\n",
    "    timestamps=np.array(timestamps.astype(\"datetime64[ns]\")),\n",
    "    train_starts=train_starts,\n",
    "    val_starts=val_starts,\n",
    "    test_starts=test_starts,\n",
    "    in_len=np.array([IN_LEN], dtype=np.int32),\n",
    "    out_len=np.array([OUT_LEN], dtype=np.int32),\n",
    "    flow_mean=flow_mean, flow_std=flow_std,\n",
    "    speed_mean=speed_mean, speed_std=speed_std,\n",
    "    seed=np.array([SEED], dtype=np.int32),\n",
    ")\n",
    "\n",
    "print(\"Saved:\", DATASET_NPZ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604d7ebe-5650-4bac-b5be-468fe9cfb24d",
   "metadata": {},
   "source": [
    "## Fix window split leakage (strict horizon containment)\n",
    "\n",
    "A window starting at time t uses:\n",
    "- Input:  X[t : t+IN_LEN]\n",
    "- Output: Y[t+IN_LEN : t+IN_LEN+OUT_LEN]\n",
    "\n",
    "To prevent label leakage across train/val/test boundaries, we require:\n",
    "- Train: output_end_time ≤ TRAIN_END\n",
    "- Val:   output_start_time > TRAIN_END AND output_end_time ≤ VAL_END\n",
    "- Test:  output_start_time > VAL_END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26396172-e9bd-4411-b484-0b5dd724bd1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T21:10:34.519135Z",
     "iopub.status.busy": "2026-02-07T21:10:34.518819Z",
     "iopub.status.idle": "2026-02-07T21:10:39.611410Z",
     "shell.execute_reply": "2026-02-07T21:10:39.607849Z",
     "shell.execute_reply.started": "2026-02-07T21:10:34.519115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRICT window starts:\n",
      "train: 1009\n",
      "val:   289\n",
      "test:  673\n",
      "Saved strict dataset to: artifacts/pems_graph_dataset_strict.npz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATASET_NPZ = Path(\"artifacts/pems_graph_dataset.npz\")\n",
    "DATASET_NPZ_STRICT = Path(\"artifacts/pems_graph_dataset_strict.npz\")\n",
    "\n",
    "data = np.load(DATASET_NPZ, allow_pickle=True)\n",
    "\n",
    "X = data[\"X\"]\n",
    "Y = data[\"Y\"]\n",
    "A = data[\"A\"]\n",
    "stations = data[\"stations\"]\n",
    "timestamps = pd.to_datetime(data[\"timestamps\"])\n",
    "\n",
    "IN_LEN = int(data[\"in_len\"][0])\n",
    "OUT_LEN = int(data[\"out_len\"][0])\n",
    "\n",
    "flow_mean = data[\"flow_mean\"]\n",
    "flow_std  = data[\"flow_std\"]\n",
    "speed_mean = data[\"speed_mean\"]\n",
    "speed_std  = data[\"speed_std\"]\n",
    "\n",
    "T_total = X.shape[0]\n",
    "max_t = T_total - (IN_LEN + OUT_LEN) + 1\n",
    "starts = np.arange(max_t, dtype=np.int32)\n",
    "\n",
    "out_start_times = timestamps[starts + IN_LEN]\n",
    "out_end_times   = timestamps[starts + IN_LEN + OUT_LEN - 1]\n",
    "\n",
    "TRAIN_END = pd.Timestamp(\"2024-11-15 23:59:59\")\n",
    "VAL_END   = pd.Timestamp(\"2024-11-30 23:59:59\")\n",
    "\n",
    "# Strict splits\n",
    "train_starts = starts[out_end_times <= TRAIN_END]\n",
    "val_starts   = starts[(out_start_times > TRAIN_END) & (out_end_times <= VAL_END)]\n",
    "test_starts  = starts[out_start_times > VAL_END]\n",
    "\n",
    "print(\"STRICT window starts:\")\n",
    "print(\"train:\", len(train_starts))\n",
    "print(\"val:  \", len(val_starts))\n",
    "print(\"test: \", len(test_starts))\n",
    "\n",
    "np.savez_compressed(\n",
    "    DATASET_NPZ_STRICT,\n",
    "    X=X.astype(np.float32),\n",
    "    Y=Y.astype(np.float32),\n",
    "    A=A.astype(np.float32),\n",
    "    stations=stations.astype(np.int32),\n",
    "    timestamps=np.array(timestamps.astype(\"datetime64[ns]\")),\n",
    "    train_starts=train_starts,\n",
    "    val_starts=val_starts,\n",
    "    test_starts=test_starts,\n",
    "    in_len=np.array([IN_LEN], dtype=np.int32),\n",
    "    out_len=np.array([OUT_LEN], dtype=np.int32),\n",
    "    flow_mean=flow_mean.astype(np.float32),\n",
    "    flow_std=flow_std.astype(np.float32),\n",
    "    speed_mean=speed_mean.astype(np.float32),\n",
    "    speed_std=speed_std.astype(np.float32),\n",
    ")\n",
    "\n",
    "print(\"Saved strict dataset to:\", DATASET_NPZ_STRICT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb6355d-15db-480a-88c0-4504c380b16a",
   "metadata": {},
   "source": [
    "## Load strict dataset artifact\n",
    "\n",
    "We will only use the strict `.npz` going forward to ensure no leakage in labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5515da5a-67a7-47dd-a481-a12fc40907e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T21:11:06.047641Z",
     "iopub.status.busy": "2026-02-07T21:11:06.047230Z",
     "iopub.status.idle": "2026-02-07T21:11:06.540671Z",
     "shell.execute_reply": "2026-02-07T21:11:06.539927Z",
     "shell.execute_reply.started": "2026-02-07T21:11:06.047617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (2208, 1821, 6) Y: (2208, 1821) A: (1821, 1821)\n",
      "starts: 1009 289 673\n",
      "IN_LEN: 24 OUT_LEN: 72\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATASET_NPZ_STRICT = Path(\"artifacts/pems_graph_dataset_strict.npz\")\n",
    "d = np.load(DATASET_NPZ_STRICT, allow_pickle=True)\n",
    "\n",
    "X = d[\"X\"]          # (T,N,F)\n",
    "Y = d[\"Y\"]          # (T,N)\n",
    "A = d[\"A\"]          # (N,N)\n",
    "stations = d[\"stations\"]\n",
    "timestamps = pd.to_datetime(d[\"timestamps\"])\n",
    "\n",
    "train_starts = d[\"train_starts\"]\n",
    "val_starts   = d[\"val_starts\"]\n",
    "test_starts  = d[\"test_starts\"]\n",
    "\n",
    "IN_LEN = int(d[\"in_len\"][0])\n",
    "OUT_LEN = int(d[\"out_len\"][0])\n",
    "\n",
    "flow_mean = d[\"flow_mean\"]  # (N,)\n",
    "flow_std  = d[\"flow_std\"]   # (N,)\n",
    "\n",
    "print(\"X:\", X.shape, \"Y:\", Y.shape, \"A:\", A.shape)\n",
    "print(\"starts:\", len(train_starts), len(val_starts), len(test_starts))\n",
    "print(\"IN_LEN:\", IN_LEN, \"OUT_LEN:\", OUT_LEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ada36a4-6634-47c0-98c5-7353202d241c",
   "metadata": {},
   "source": [
    "## Baseline evaluation\n",
    "\n",
    "We evaluate at horizons: 12, 24, 48, 72 hours ahead.\n",
    "\n",
    "Important detail:\n",
    "Our output sequence begins at +1 hour ahead of the last input time.\n",
    "So horizon `h` corresponds to output index `h-1` in the 72-step target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "737f6784-238b-4c35-a5bc-ea736646d0b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T21:12:53.206372Z",
     "iopub.status.busy": "2026-02-07T21:12:53.205790Z",
     "iopub.status.idle": "2026-02-07T21:12:53.212959Z",
     "shell.execute_reply": "2026-02-07T21:12:53.211829Z",
     "shell.execute_reply.started": "2026-02-07T21:12:53.206347Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "EVAL_HORIZONS = [12, 24, 48, 72]\n",
    "\n",
    "def init_metric_accumulators(horizons):\n",
    "    return {\n",
    "        h: {\"abs_sum\": 0.0, \"sq_sum\": 0.0, \"count\": 0}\n",
    "        for h in horizons\n",
    "    }\n",
    "\n",
    "def finalize_metrics(acc):\n",
    "    out = {}\n",
    "    for h, v in acc.items():\n",
    "        mae = v[\"abs_sum\"] / max(v[\"count\"], 1)\n",
    "        rmse = np.sqrt(v[\"sq_sum\"] / max(v[\"count\"], 1))\n",
    "        out[h] = {\"MAE\": mae, \"RMSE\": rmse}\n",
    "    return out\n",
    "\n",
    "def print_metrics(title, metrics_dict):\n",
    "    print(\"\\n\" + title)\n",
    "    for h in sorted(metrics_dict.keys()):\n",
    "        print(f\"  {h:>3}h  MAE={metrics_dict[h]['MAE']:.3f}  RMSE={metrics_dict[h]['RMSE']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd500bf-d0f5-4205-85c3-2c7a169a645d",
   "metadata": {},
   "source": [
    "### Baseline 1 — Persistence\n",
    "\n",
    "For each station:\n",
    "- Predict that all future horizons equal the **last observed flow** in the input window.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8e7a280-81bd-4fea-8134-396e459526ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T21:14:18.584055Z",
     "iopub.status.busy": "2026-02-07T21:14:18.583297Z",
     "iopub.status.idle": "2026-02-07T21:14:18.770971Z",
     "shell.execute_reply": "2026-02-07T21:14:18.769750Z",
     "shell.execute_reply.started": "2026-02-07T21:14:18.584029Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49fa4fe544b943cfaf7ccc68a472cdd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Persistence (val):   0%|          | 0/289 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8248ff923c3a46dba30cc6d3c4398268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Persistence (test):   0%|          | 0/673 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Persistence — Validation\n",
      "   12h  MAE=910.133  RMSE=1437.403\n",
      "   24h  MAE=151.755  RMSE=354.476\n",
      "   48h  MAE=203.020  RMSE=451.021\n",
      "   72h  MAE=220.150  RMSE=478.588\n",
      "\n",
      "Persistence — Test\n",
      "   12h  MAE=917.014  RMSE=1455.414\n",
      "   24h  MAE=147.896  RMSE=340.247\n",
      "   48h  MAE=200.592  RMSE=443.364\n",
      "   72h  MAE=196.856  RMSE=431.611\n"
     ]
    }
   ],
   "source": [
    "def eval_persistence(X, Y, starts, in_len, horizons, desc=\"\"):\n",
    "    acc = init_metric_accumulators(horizons)\n",
    "\n",
    "    for t in tqdm(starts, desc=desc):\n",
    "        # last observed flow at the end of input window\n",
    "        last_flow = X[t + in_len - 1, :, 0]  # (N,)\n",
    "\n",
    "        for h in horizons:\n",
    "            idx = h - 1\n",
    "            true = Y[t + in_len + idx, :]     # (N,)\n",
    "            pred = last_flow                  # (N,)\n",
    "\n",
    "            err = pred - true\n",
    "            acc[h][\"abs_sum\"] += float(np.abs(err).sum())\n",
    "            acc[h][\"sq_sum\"]  += float((err ** 2).sum())\n",
    "            acc[h][\"count\"]   += err.size\n",
    "\n",
    "    return finalize_metrics(acc)\n",
    "\n",
    "pers_val  = eval_persistence(X, Y, val_starts, IN_LEN, EVAL_HORIZONS, desc=\"Persistence (val)\")\n",
    "pers_test = eval_persistence(X, Y, test_starts, IN_LEN, EVAL_HORIZONS, desc=\"Persistence (test)\")\n",
    "\n",
    "print_metrics(\"Persistence — Validation\", pers_val)\n",
    "print_metrics(\"Persistence — Test\", pers_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61797eea-3aad-422c-ab05-4cfc275c3280",
   "metadata": {},
   "source": [
    "### Baseline 2 — Historical Average (HA-168)\n",
    "\n",
    "We compute a per-node seasonal mean using train data only:\n",
    "- slot = (day_of_week * 24 + hour) ∈ [0..167]\n",
    "- mean_flow[slot, node] = average flow in train for that slot\n",
    "\n",
    "Forecast:\n",
    "- For each horizon step, use the slot mean of that future timestamp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f1ecda2-abdd-4241-bb4e-3215a5cb224d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T21:15:48.783046Z",
     "iopub.status.busy": "2026-02-07T21:15:48.782558Z",
     "iopub.status.idle": "2026-02-07T21:15:48.972239Z",
     "shell.execute_reply": "2026-02-07T21:15:48.971202Z",
     "shell.execute_reply.started": "2026-02-07T21:15:48.783009Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab956bdc0c8415f836d0662d4fb5523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HA-168 (val):   0%|          | 0/289 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c08a3390fe74619a202d791c8b2c9dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HA-168 (test):   0%|          | 0/673 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HA-168 — Validation\n",
      "   12h  MAE=116.424  RMSE=258.349\n",
      "   24h  MAE=123.997  RMSE=277.816\n",
      "   48h  MAE=134.454  RMSE=302.695\n",
      "   72h  MAE=137.523  RMSE=306.085\n",
      "\n",
      "HA-168 — Test\n",
      "   12h  MAE=119.650  RMSE=283.458\n",
      "   24h  MAE=120.382  RMSE=284.451\n",
      "   48h  MAE=122.195  RMSE=286.807\n",
      "   72h  MAE=126.187  RMSE=293.463\n"
     ]
    }
   ],
   "source": [
    "def build_ha168_means(Y, timestamps, train_end):\n",
    "    train_mask = timestamps <= train_end\n",
    "    Y_train = Y[train_mask]  # (T_train, N)\n",
    "    ts_train = timestamps[train_mask]\n",
    "\n",
    "    slot = ts_train.dayofweek.to_numpy() * 24 + ts_train.hour.to_numpy()  # (T_train,)\n",
    "    G = 168\n",
    "    N = Y.shape[1]\n",
    "\n",
    "    means = np.zeros((G, N), dtype=np.float32)\n",
    "    counts = np.zeros((G,), dtype=np.int64)\n",
    "\n",
    "    for g in range(G):\n",
    "        m = (slot == g)\n",
    "        if m.any():\n",
    "            means[g] = Y_train[m].mean(axis=0)\n",
    "            counts[g] = int(m.sum())\n",
    "        else:\n",
    "            # fallback (should be rare)\n",
    "            means[g] = Y_train.mean(axis=0)\n",
    "            counts[g] = 0\n",
    "    return means, counts\n",
    "\n",
    "def eval_ha168(Y, timestamps, starts, in_len, horizons, means_ha168, desc=\"\"):\n",
    "    acc = init_metric_accumulators(horizons)\n",
    "\n",
    "    for t in tqdm(starts, desc=desc):\n",
    "        for h in horizons:\n",
    "            idx = h - 1\n",
    "            future_time = timestamps[t + in_len + idx]\n",
    "            g = int(future_time.dayofweek * 24 + future_time.hour)\n",
    "\n",
    "            pred = means_ha168[g, :]                 # (N,)\n",
    "            true = Y[t + in_len + idx, :]            # (N,)\n",
    "\n",
    "            err = pred - true\n",
    "            acc[h][\"abs_sum\"] += float(np.abs(err).sum())\n",
    "            acc[h][\"sq_sum\"]  += float((err ** 2).sum())\n",
    "            acc[h][\"count\"]   += err.size\n",
    "\n",
    "    return finalize_metrics(acc)\n",
    "\n",
    "TRAIN_END = pd.Timestamp(\"2024-11-15 23:59:59\")\n",
    "ha_means, ha_counts = build_ha168_means(Y, timestamps, TRAIN_END)\n",
    "\n",
    "ha_val  = eval_ha168(Y, timestamps, val_starts, IN_LEN, EVAL_HORIZONS, ha_means, desc=\"HA-168 (val)\")\n",
    "ha_test = eval_ha168(Y, timestamps, test_starts, IN_LEN, EVAL_HORIZONS, ha_means, desc=\"HA-168 (test)\")\n",
    "\n",
    "print_metrics(\"HA-168 — Validation\", ha_val)\n",
    "print_metrics(\"HA-168 — Test\", ha_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcc74e4-add9-43ab-a18c-123e4dc5142b",
   "metadata": {},
   "source": [
    "## PyTorch Dataset for sliding windows\n",
    "\n",
    "Each item returns:\n",
    "- x: (C, N, IN_LEN)   where C = number of features (6)\n",
    "- y: (OUT_LEN, N)     scaled flow targets\n",
    "\n",
    "Scaling:\n",
    "- We scale flow and speed using TRAIN-only mean/std (per node)\n",
    "- Time features remain unchanged (already bounded by sin/cos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2c00304-797e-43d5-b857-245c266f37d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T21:16:13.532982Z",
     "iopub.status.busy": "2026-02-07T21:16:13.532696Z",
     "iopub.status.idle": "2026-02-07T21:16:13.906511Z",
     "shell.execute_reply": "2026-02-07T21:16:13.905799Z",
     "shell.execute_reply.started": "2026-02-07T21:16:13.532959Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch x: torch.Size([16, 6, 1821, 24]) Batch y: torch.Size([16, 72, 1821])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PemsWindowDataset(Dataset):\n",
    "    def __init__(self, X, Y, starts, in_len, out_len, flow_mean, flow_std, speed_mean, speed_std):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.starts = starts\n",
    "        self.in_len = in_len\n",
    "        self.out_len = out_len\n",
    "\n",
    "        self.flow_mean = flow_mean.astype(np.float32)\n",
    "        self.flow_std  = flow_std.astype(np.float32)\n",
    "        self.speed_mean = speed_mean.astype(np.float32)\n",
    "        self.speed_std  = speed_std.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.starts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t = int(self.starts[idx])\n",
    "\n",
    "        x = self.X[t : t + self.in_len].copy().astype(np.float32)  # (IN_LEN, N, F)\n",
    "        y = self.Y[t + self.in_len : t + self.in_len + self.out_len].copy().astype(np.float32)  # (OUT_LEN, N)\n",
    "\n",
    "        # scale input channels: flow=0, speed=1\n",
    "        x[..., 0] = (x[..., 0] - self.flow_mean[None, :]) / self.flow_std[None, :]\n",
    "        x[..., 1] = (x[..., 1] - self.speed_mean[None, :]) / self.speed_std[None, :]\n",
    "\n",
    "        # scale targets (flow)\n",
    "        y = (y - self.flow_mean[None, :]) / self.flow_std[None, :]\n",
    "\n",
    "        # rearrange x to (C, N, IN_LEN) for conv models\n",
    "        x = np.transpose(x, (2, 1, 0))  # (F, N, IN_LEN)\n",
    "\n",
    "        return torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "# Load speed scaler too\n",
    "speed_mean = d[\"speed_mean\"]\n",
    "speed_std  = d[\"speed_std\"]\n",
    "\n",
    "train_ds = PemsWindowDataset(X, Y, train_starts, IN_LEN, OUT_LEN, flow_mean, flow_std, speed_mean, speed_std)\n",
    "val_ds   = PemsWindowDataset(X, Y, val_starts,   IN_LEN, OUT_LEN, flow_mean, flow_std, speed_mean, speed_std)\n",
    "test_ds  = PemsWindowDataset(X, Y, test_starts,  IN_LEN, OUT_LEN, flow_mean, flow_std, speed_mean, speed_std)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "xb, yb = next(iter(train_loader))\n",
    "print(\"Batch x:\", xb.shape, \"Batch y:\", yb.shape)\n",
    "# Expect: x=(B, 6, N, 24) and y=(B, 72, N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fac0c314-8f62-4624-9d71-37bed7c48631",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T21:57:23.849329Z",
     "iopub.status.busy": "2026-02-07T21:57:23.849007Z",
     "iopub.status.idle": "2026-02-07T21:57:23.916084Z",
     "shell.execute_reply": "2026-02-07T21:57:23.915230Z",
     "shell.execute_reply.started": "2026-02-07T21:57:23.849307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean |Δ| over 1h shift: 156.388\n",
      "Mean |Δ| over 6h shift: 666.328\n",
      "Mean |Δ| over 12h shift: 921.578\n",
      "Mean |Δ| over 24h shift: 143.347\n",
      "Mean |Δ| over 48h shift: 202.141\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mean_abs_change(Y, h):\n",
    "    return float(np.abs(Y[h:] - Y[:-h]).mean())\n",
    "\n",
    "for h in [1, 6, 12, 24, 48]:\n",
    "    print(f\"Mean |Δ| over {h}h shift: {mean_abs_change(Y, h):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7bb9a1-ed40-4e4c-aca1-cdb694fe9ab4",
   "metadata": {},
   "source": [
    "## Graph supports (normalized adjacency)\n",
    "\n",
    "GraphWaveNet uses graph propagation through adjacency matrices (\"supports\").\n",
    "We will build:\n",
    "- A_rw  = row-normalized adjacency (random-walk normalization)\n",
    "- A_rwT = transpose support (helps if graph is directed; still ok for symmetric graphs)\n",
    "\n",
    "We store them as sparse tensors for speed (our adjacency is very sparse).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a205f5bd-ad93-49f4-ab2f-ff60b6e200bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T21:59:44.385916Z",
     "iopub.status.busy": "2026-02-07T21:59:44.385011Z",
     "iopub.status.idle": "2026-02-07T21:59:44.530577Z",
     "shell.execute_reply": "2026-02-07T21:59:44.529646Z",
     "shell.execute_reply.started": "2026-02-07T21:59:44.385891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supports: [torch.Size([1821, 1821]), torch.Size([1821, 1821])] nnz: [7856, 7856]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def row_normalize_dense(A: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    d = A.sum(axis=1, keepdims=True)\n",
    "    return A / (d + eps)\n",
    "\n",
    "def dense_to_torch_sparse(A: np.ndarray, device: str):\n",
    "    A = A.astype(np.float32)\n",
    "    idx = np.nonzero(A)\n",
    "    values = A[idx]\n",
    "    indices = np.vstack(idx)  # (2, nnz)\n",
    "\n",
    "    indices = torch.tensor(indices, dtype=torch.long, device=device)\n",
    "    values  = torch.tensor(values, dtype=torch.float32, device=device)\n",
    "    shape = A.shape\n",
    "\n",
    "    sp = torch.sparse_coo_tensor(indices, values, size=shape, device=device).coalesce()\n",
    "    return sp\n",
    "\n",
    "A_rw = row_normalize_dense(A)\n",
    "A_rwT = row_normalize_dense(A.T)\n",
    "\n",
    "supports = [\n",
    "    dense_to_torch_sparse(A_rw, DEVICE),\n",
    "    dense_to_torch_sparse(A_rwT, DEVICE),\n",
    "]\n",
    "\n",
    "print(\"Supports:\", [s.shape for s in supports], \"nnz:\", [int(s._nnz()) for s in supports])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01d37d9-e314-48ec-8a9b-0368538be343",
   "metadata": {},
   "source": [
    "## Diffusion graph convolution (sparse)\n",
    "\n",
    "We need a fast way to compute:\n",
    "A @ X  (graph propagation)\n",
    "\n",
    "X is batched with shape (B, C, N, T).\n",
    "We reshape into (N, B*C*T) so sparse matrix multiply works efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6b573ca-1a08-4222-b75b-c6829443ea64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T22:00:11.218542Z",
     "iopub.status.busy": "2026-02-07T22:00:11.218057Z",
     "iopub.status.idle": "2026-02-07T22:00:11.226774Z",
     "shell.execute_reply": "2026-02-07T22:00:11.225879Z",
     "shell.execute_reply.started": "2026-02-07T22:00:11.218517Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NConv(nn.Module):\n",
    "    \"\"\"Sparse graph multiplication: (N,N) @ (B,C,N,T) -> (B,C,N,T)\"\"\"\n",
    "    def forward(self, x, A_sp):\n",
    "        # x: (B,C,N,T)\n",
    "        B, C, N, T = x.shape\n",
    "        x_r = x.permute(2, 0, 1, 3).reshape(N, -1)      # (N, B*C*T)\n",
    "        x_r = torch.sparse.mm(A_sp, x_r)                # (N, B*C*T)\n",
    "        x_out = x_r.reshape(N, B, C, T).permute(1, 2, 0, 3)  # (B,C,N,T)\n",
    "        return x_out\n",
    "\n",
    "class DiffusionGraphConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Diffusion graph conv:\n",
    "    concat [X, A1X, A2X, ...] then 1x1 conv.\n",
    "\n",
    "    order=1 means we use only one hop per support (fast and stable).\n",
    "    \"\"\"\n",
    "    def __init__(self, c_in, c_out, supports, order=1, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.nconv = NConv()\n",
    "        self.supports = supports\n",
    "        self.order = order\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # total input channels after concatenation\n",
    "        c_total = c_in * (1 + len(supports) * order)\n",
    "        self.mlp = nn.Conv2d(c_total, c_out, kernel_size=(1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = [x]\n",
    "        for A in self.supports:\n",
    "            x1 = self.nconv(x, A)\n",
    "            out.append(x1)\n",
    "            for _ in range(2, self.order + 1):\n",
    "                x1 = self.nconv(x1, A)\n",
    "                out.append(x1)\n",
    "\n",
    "        h = torch.cat(out, dim=1)\n",
    "        h = self.mlp(h)\n",
    "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90555df0-af2c-407e-b789-20335ccd6ad3",
   "metadata": {},
   "source": [
    "## GraphWaveNet baseline model\n",
    "\n",
    "We use:\n",
    "- Causal dilated temporal convolutions (TCN-style)\n",
    "- Gated activations (tanh * sigmoid)\n",
    "- Residual + skip connections\n",
    "- Diffusion graph convolution inside each layer\n",
    "\n",
    "Output head:\n",
    "- Uses the final time step embedding to predict OUT_LEN horizons directly\n",
    "- Output shape: (B, OUT_LEN, N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21a86ee7-6121-4910-ab01-db89a3a19e2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T22:04:50.463959Z",
     "iopub.status.busy": "2026-02-07T22:04:50.463528Z",
     "iopub.status.idle": "2026-02-07T22:04:50.478854Z",
     "shell.execute_reply": "2026-02-07T22:04:50.477780Z",
     "shell.execute_reply.started": "2026-02-07T22:04:50.463930Z"
    }
   },
   "outputs": [],
   "source": [
    "class CausalConv2d(nn.Module):\n",
    "    \"\"\"Causal conv along time axis only (last dimension).\"\"\"\n",
    "    def __init__(self, c_in, c_out, kernel_size=2, dilation=1):\n",
    "        super().__init__()\n",
    "        self.pad = (kernel_size - 1) * dilation\n",
    "        self.conv = nn.Conv2d(\n",
    "            c_in, c_out,\n",
    "            kernel_size=(1, kernel_size),\n",
    "            dilation=(1, dilation)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pad left on time dimension: (left_pad, right_pad, top, bottom) for 2d -> (time_left, time_right, node_left, node_right)\n",
    "        x = F.pad(x, (self.pad, 0, 0, 0))\n",
    "        return self.conv(x)\n",
    "\n",
    "class GraphWaveNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_nodes: int,\n",
    "        in_dim: int,\n",
    "        out_len: int,\n",
    "        supports,\n",
    "        residual_channels=32,\n",
    "        dilation_channels=32,\n",
    "        skip_channels=64,\n",
    "        end_channels=128,\n",
    "        kernel_size=2,\n",
    "        blocks=2,\n",
    "        layers_per_block=3,\n",
    "        gcn_order=1,\n",
    "        dropout=0.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.in_dim = in_dim\n",
    "        self.out_len = out_len\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.start_conv = nn.Conv2d(in_dim, residual_channels, kernel_size=(1, 1))\n",
    "\n",
    "        self.filter_convs = nn.ModuleList()\n",
    "        self.gate_convs   = nn.ModuleList()\n",
    "        self.res_convs    = nn.ModuleList()\n",
    "        self.skip_convs   = nn.ModuleList()\n",
    "        self.bn           = nn.ModuleList()\n",
    "        self.gconvs       = nn.ModuleList()\n",
    "\n",
    "        # Build temporal + graph blocks\n",
    "        for _ in range(blocks):\n",
    "            for i in range(layers_per_block):\n",
    "                dilation = 2 ** i\n",
    "\n",
    "                self.filter_convs.append(CausalConv2d(residual_channels, dilation_channels, kernel_size, dilation))\n",
    "                self.gate_convs.append(CausalConv2d(residual_channels, dilation_channels, kernel_size, dilation))\n",
    "\n",
    "                self.res_convs.append(nn.Conv2d(dilation_channels, residual_channels, kernel_size=(1, 1)))\n",
    "                self.skip_convs.append(nn.Conv2d(dilation_channels, skip_channels, kernel_size=(1, 1)))\n",
    "\n",
    "                self.gconvs.append(\n",
    "                    DiffusionGraphConv(dilation_channels, residual_channels, supports, order=gcn_order, dropout=dropout)\n",
    "                )\n",
    "\n",
    "                self.bn.append(nn.BatchNorm2d(residual_channels))\n",
    "\n",
    "        self.end_conv_1 = nn.Conv2d(skip_channels, end_channels, kernel_size=(1, 1))\n",
    "        self.end_conv_2 = nn.Conv2d(end_channels, out_len, kernel_size=(1, 1))  # outputs OUT_LEN channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, F, N, T_in)\n",
    "        return: (B, OUT_LEN, N)\n",
    "        \"\"\"\n",
    "        x = self.start_conv(x)  # (B, residual, N, T)\n",
    "        skip = None\n",
    "\n",
    "        for i in range(len(self.filter_convs)):\n",
    "            residual = x\n",
    "\n",
    "            # gated TCN\n",
    "            filt = torch.tanh(self.filter_convs[i](x))\n",
    "            gate = torch.sigmoid(self.gate_convs[i](x))\n",
    "            x = filt * gate\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "            # skip\n",
    "            s = self.skip_convs[i](x)\n",
    "            skip = s if skip is None else (skip + s)\n",
    "\n",
    "            # graph conv -> residual channels\n",
    "            x = self.gconvs[i](x)\n",
    "\n",
    "            # residual connection (time length is preserved by causal padding)\n",
    "            x = x + residual\n",
    "            x = self.bn[i](x)\n",
    "\n",
    "        x = F.relu(skip)\n",
    "        x = F.relu(self.end_conv_1(x))\n",
    "\n",
    "        # Use last time step to predict future horizons\n",
    "        x_last = x[..., -1:].contiguous()          # (B, end_channels, N, 1)\n",
    "        out = self.end_conv_2(x_last).squeeze(-1)  # (B, OUT_LEN, N)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937a1415-bd0e-41d1-9020-7e38b9a329e2",
   "metadata": {},
   "source": [
    "## Training & evaluation loop\n",
    "\n",
    "We train using MSE on scaled targets (stable optimization),\n",
    "then compute MAE/RMSE on the original scale at horizons 12/24/48/72.\n",
    "\n",
    "Early stopping monitors average validation MAE across horizons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f25c2c5-41a5-4944-be65-bc89dd8a13c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T22:19:03.564943Z",
     "iopub.status.busy": "2026-02-07T22:19:03.564622Z",
     "iopub.status.idle": "2026-02-07T22:39:25.208699Z",
     "shell.execute_reply": "2026-02-07T22:39:25.207909Z",
     "shell.execute_reply.started": "2026-02-07T22:19:03.564921Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ab6472702d435694f36e4deac1d1b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "980a9332521b48d7815184e6f0e7c747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss=0.529129  val_avg_MAE=203.973\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=225.832  RMSE=393.051\n",
      "   24h  MAE=184.901  RMSE=343.538\n",
      "   48h  MAE=208.168  RMSE=379.516\n",
      "   72h  MAE=196.993  RMSE=369.422\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb6c6ec16c124129a7fcdc834952f9de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a076df23f04d2fa0e008ca01d6d064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss=0.325922  val_avg_MAE=185.356\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=193.387  RMSE=348.977\n",
      "   24h  MAE=166.734  RMSE=319.229\n",
      "   48h  MAE=185.582  RMSE=349.533\n",
      "   72h  MAE=195.720  RMSE=369.310\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491a7e91c8e5492d98cac243d4e14e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d577d368f05448ab97b31f0d01929de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss=0.287739  val_avg_MAE=180.951\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=179.654  RMSE=333.382\n",
      "   24h  MAE=157.967  RMSE=309.919\n",
      "   48h  MAE=187.718  RMSE=347.820\n",
      "   72h  MAE=198.465  RMSE=367.385\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06deca55dca4e5aa09d631507ec9b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d0e9bbbaddf4b49870b54da71a1f864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train_loss=0.268916  val_avg_MAE=175.876\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=186.728  RMSE=342.408\n",
      "   24h  MAE=154.416  RMSE=309.196\n",
      "   48h  MAE=171.948  RMSE=328.877\n",
      "   72h  MAE=190.413  RMSE=357.469\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be87cd92ff8443796bbc85255e98952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2ee564a5b64113ac9e02ed39401539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: train_loss=0.257975  val_avg_MAE=170.299\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=184.271  RMSE=338.048\n",
      "   24h  MAE=148.209  RMSE=299.617\n",
      "   48h  MAE=168.291  RMSE=323.040\n",
      "   72h  MAE=180.426  RMSE=343.043\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8a08f28ea04bf193504fbf316bf607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef25c89d44c47c3bdfac3e06acfd025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: train_loss=0.250068  val_avg_MAE=164.475\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=162.037  RMSE=303.679\n",
      "   24h  MAE=150.514  RMSE=301.402\n",
      "   48h  MAE=164.907  RMSE=316.562\n",
      "   72h  MAE=180.444  RMSE=343.314\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221d56ecd7bf48158f7c8e6c7fccf845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f10d31b6ed954c099a6f40153beea0fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: train_loss=0.244371  val_avg_MAE=167.873\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=164.548  RMSE=308.877\n",
      "   24h  MAE=156.601  RMSE=304.825\n",
      "   48h  MAE=176.487  RMSE=327.605\n",
      "   72h  MAE=173.856  RMSE=331.363\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc03844fc2e499a994cf2d2e8723459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5121344730d45cca619fc294c6a65ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: train_loss=0.239806  val_avg_MAE=160.063\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=160.283  RMSE=302.982\n",
      "   24h  MAE=148.200  RMSE=298.973\n",
      "   48h  MAE=161.825  RMSE=314.139\n",
      "   72h  MAE=169.943  RMSE=325.858\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe87bf5bcf0d458080d7970e67f046ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e70d0d3058c4b2e96963a75d951c006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: train_loss=0.234704  val_avg_MAE=163.708\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=158.480  RMSE=303.029\n",
      "   24h  MAE=158.671  RMSE=308.487\n",
      "   48h  MAE=163.579  RMSE=313.825\n",
      "   72h  MAE=174.101  RMSE=332.094\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1c22d395b441018b83008d3e3bae4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "208b7bf85e04448d8fd6b1cb9670adca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train_loss=0.234201  val_avg_MAE=155.655\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=153.175  RMSE=294.953\n",
      "   24h  MAE=141.512  RMSE=293.586\n",
      "   48h  MAE=159.561  RMSE=315.417\n",
      "   72h  MAE=168.371  RMSE=323.676\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d981020918f49f2b4eb1c6be10ab020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0863d122755d440b8aa7f42a953e9943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: train_loss=0.231096  val_avg_MAE=154.549\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=152.754  RMSE=293.402\n",
      "   24h  MAE=139.974  RMSE=287.064\n",
      "   48h  MAE=157.335  RMSE=307.804\n",
      "   72h  MAE=168.134  RMSE=323.799\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ebd152c3c94941814572045d6a0646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb14cd1a94e4b60b0e3d9db38cd1980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: train_loss=0.229034  val_avg_MAE=164.792\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=151.243  RMSE=291.496\n",
      "   24h  MAE=147.712  RMSE=297.619\n",
      "   48h  MAE=167.106  RMSE=321.370\n",
      "   72h  MAE=193.109  RMSE=358.011\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71483139dfd4492193841d2d40300a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b5cf6455764f83900159e26f91a438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: train_loss=0.223892  val_avg_MAE=164.343\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=148.523  RMSE=289.870\n",
      "   24h  MAE=146.654  RMSE=296.672\n",
      "   48h  MAE=173.357  RMSE=328.243\n",
      "   72h  MAE=188.839  RMSE=347.545\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4932205875a843888c1c00ad43dfa3fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d5da2bbe944fcca18e36d44f3fa7dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: train_loss=0.222705  val_avg_MAE=166.283\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=153.376  RMSE=294.916\n",
      "   24h  MAE=155.567  RMSE=312.574\n",
      "   48h  MAE=176.343  RMSE=332.107\n",
      "   72h  MAE=179.847  RMSE=333.986\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe223285f59049948fae5bcdfbca00a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f13fc0332146d5a5f1627fcab5ba7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: train_loss=0.221832  val_avg_MAE=163.332\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=169.177  RMSE=314.841\n",
      "   24h  MAE=143.233  RMSE=290.326\n",
      "   48h  MAE=168.777  RMSE=325.777\n",
      "   72h  MAE=172.140  RMSE=329.309\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732e182a6ff14a22b91901bb24989380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95238e9cc1f346f3aaad86a8acd8456f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: train_loss=0.221454  val_avg_MAE=155.481\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=144.691  RMSE=279.696\n",
      "   24h  MAE=139.187  RMSE=284.711\n",
      "   48h  MAE=161.616  RMSE=311.975\n",
      "   72h  MAE=176.428  RMSE=331.767\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35154a1017dd48a38e86a9d2b076f1ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/30:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f0e536b7c3405eb42729c619559838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: train_loss=0.217759  val_avg_MAE=155.037\n",
      "\n",
      "Val metrics\n",
      "   12h  MAE=151.572  RMSE=293.354\n",
      "   24h  MAE=139.192  RMSE=286.145\n",
      "   48h  MAE=161.421  RMSE=316.536\n",
      "   72h  MAE=167.963  RMSE=321.738\n",
      "Early stopping at epoch 17. Best val_avg_MAE=154.549\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a0eebe30e2b4417b5d1366a8e47ad89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1014d6bab2704946b74a6266db129349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GraphWaveNet — Validation\n",
      "   12h  MAE=152.754  RMSE=293.402\n",
      "   24h  MAE=139.974  RMSE=287.064\n",
      "   48h  MAE=157.335  RMSE=307.804\n",
      "   72h  MAE=168.134  RMSE=323.799\n",
      "\n",
      "GraphWaveNet — Test\n",
      "   12h  MAE=156.900  RMSE=298.198\n",
      "   24h  MAE=134.966  RMSE=269.862\n",
      "   48h  MAE=153.469  RMSE=300.160\n",
      "   72h  MAE=161.221  RMSE=310.938\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# tensors for unscaling (put on device once)\n",
    "flow_mean_t = torch.tensor(flow_mean, dtype=torch.float32, device=DEVICE).view(1, 1, -1)\n",
    "flow_std_t  = torch.tensor(flow_std,  dtype=torch.float32, device=DEVICE).view(1, 1, -1)\n",
    "\n",
    "EVAL_HORIZONS = [12, 24, 48, 72]\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_horizons(model, loader):\n",
    "    model.eval()\n",
    "    acc = {h: {\"abs\": 0.0, \"sq\": 0.0, \"count\": 0} for h in EVAL_HORIZONS}\n",
    "\n",
    "    for xb, yb in tqdm(loader, desc=\"Eval\", leave=False):\n",
    "        xb = xb.to(DEVICE, non_blocking=True)   # (B,F,N,T)\n",
    "        yb = yb.to(DEVICE, non_blocking=True)   # (B,OUT,N) scaled\n",
    "\n",
    "        pred = model(xb)                        # (B,OUT,N) scaled\n",
    "\n",
    "        # unscale to original flow units\n",
    "        pred_u = pred * flow_std_t + flow_mean_t\n",
    "        true_u = yb   * flow_std_t + flow_mean_t\n",
    "\n",
    "        for h in EVAL_HORIZONS:\n",
    "            idx = h - 1\n",
    "            err = pred_u[:, idx, :] - true_u[:, idx, :]\n",
    "            acc[h][\"abs\"] += float(err.abs().sum())\n",
    "            acc[h][\"sq\"]  += float((err ** 2).sum())\n",
    "            acc[h][\"count\"] += err.numel()\n",
    "\n",
    "    metrics = {}\n",
    "    for h in EVAL_HORIZONS:\n",
    "        mae = acc[h][\"abs\"] / acc[h][\"count\"]\n",
    "        rmse = (acc[h][\"sq\"] / acc[h][\"count\"]) ** 0.5\n",
    "        metrics[h] = {\"MAE\": mae, \"RMSE\": rmse}\n",
    "    return metrics\n",
    "\n",
    "def print_metrics(title, metrics):\n",
    "    print(\"\\n\" + title)\n",
    "    for h in sorted(metrics.keys()):\n",
    "        print(f\"  {h:>3}h  MAE={metrics[h]['MAE']:.3f}  RMSE={metrics[h]['RMSE']:.3f}\")\n",
    "\n",
    "def avg_mae(metrics):\n",
    "    return float(np.mean([metrics[h][\"MAE\"] for h in metrics]))\n",
    "\n",
    "def train_gwn(\n",
    "    epochs=30,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    clip=5.0,\n",
    "    patience=6,\n",
    "):\n",
    "    model = GraphWaveNet(\n",
    "        num_nodes=X.shape[1],\n",
    "        in_dim=X.shape[2],     # 6 features\n",
    "        out_len=OUT_LEN,\n",
    "        supports=supports,\n",
    "        residual_channels=32,\n",
    "        dilation_channels=32,\n",
    "        skip_channels=64,\n",
    "        end_channels=128,\n",
    "        kernel_size=2,\n",
    "        blocks=2,\n",
    "        layers_per_block=3,\n",
    "        gcn_order=1,\n",
    "        dropout=0.3,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    use_amp = False\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=False)\n",
    "\n",
    "    best_score = float(\"inf\")\n",
    "    best_state = None\n",
    "    bad = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "\n",
    "        for xb, yb in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\", leave=False):\n",
    "            xb = xb.to(DEVICE, non_blocking=True)\n",
    "            yb = yb.to(DEVICE, non_blocking=True)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "                pred = model(xb)\n",
    "                loss = loss_fn(pred, yb)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(opt)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "\n",
    "            running += float(loss.item())\n",
    "\n",
    "        # validation metrics (original scale)\n",
    "        val_metrics = eval_horizons(model, val_loader)\n",
    "        score = avg_mae(val_metrics)\n",
    "\n",
    "        print(f\"Epoch {epoch}: train_loss={running/len(train_loader):.6f}  val_avg_MAE={score:.3f}\")\n",
    "        print_metrics(\"Val metrics\", val_metrics)\n",
    "\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}. Best val_avg_MAE={best_score:.3f}\")\n",
    "                break\n",
    "\n",
    "    # load best\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "gwn_model = train_gwn(epochs=30, lr=1e-3, weight_decay=1e-4, clip=5.0, patience=6)\n",
    "\n",
    "# Final evaluation\n",
    "val_m = eval_horizons(gwn_model, val_loader)\n",
    "test_m = eval_horizons(gwn_model, test_loader)\n",
    "\n",
    "print_metrics(\"GraphWaveNet — Validation\", val_m)\n",
    "print_metrics(\"GraphWaveNet — Test\", test_m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91367f4-27d1-49f3-92b9-469349e7b184",
   "metadata": {},
   "source": [
    "# GraphWaveNet Baseline (Journal-Grade) - Working on it again\n",
    "\n",
    "This notebook section trains a strong GraphWaveNet baseline on PeMS using:\n",
    "- Strict train/val/test window splits (no horizon leakage)\n",
    "- Direction-aware adjacency: edges only within same freeway AND same direction\n",
    "- Known future calendar covariates (hour-of-day/day-of-week), which are available at forecast time\n",
    "- Multi-horizon evaluation at 12/24/48/72 hours\n",
    "\n",
    "Outputs:\n",
    "- Validation and test MAE/RMSE at each horizon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25fa62ce-4074-4dbb-a733-6ac61f525cb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T05:37:12.536048Z",
     "iopub.status.busy": "2026-02-08T05:37:12.535481Z",
     "iopub.status.idle": "2026-02-08T05:37:15.762925Z",
     "shell.execute_reply": "2026-02-08T05:37:15.762026Z",
     "shell.execute_reply.started": "2026-02-08T05:37:12.536022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.1.1+cu121\n",
      "Device: cuda\n",
      "GPU: Quadro P5000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Device:\", DEVICE)\n",
    "if DEVICE == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b060ac7e-7976-46c8-af82-ff35c5da77be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T05:38:41.633148Z",
     "iopub.status.busy": "2026-02-08T05:38:41.632737Z",
     "iopub.status.idle": "2026-02-08T05:38:42.107970Z",
     "shell.execute_reply": "2026-02-08T05:38:42.107272Z",
     "shell.execute_reply.started": "2026-02-08T05:38:41.633124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (2208, 1821, 6) Y: (2208, 1821)\n",
      "N stations: 1821 T: 2208\n",
      "starts: 1009 289 673\n",
      "IN_LEN: 24 OUT_LEN: 72\n"
     ]
    }
   ],
   "source": [
    "DATASET_BASE = Path(\"artifacts/pems_graph_dataset.npz\")\n",
    "DATASET_STRICT = Path(\"artifacts/pems_graph_dataset_strict.npz\")\n",
    "\n",
    "TRAIN_END = pd.Timestamp(\"2024-11-15 23:59:59\")\n",
    "VAL_END   = pd.Timestamp(\"2024-11-30 23:59:59\")\n",
    "\n",
    "def make_strict_dataset(base_npz: Path, strict_npz: Path):\n",
    "    d = np.load(base_npz, allow_pickle=True)\n",
    "\n",
    "    X = d[\"X\"]; Y = d[\"Y\"]; A = d[\"A\"]\n",
    "    stations = d[\"stations\"]\n",
    "    timestamps = pd.to_datetime(d[\"timestamps\"])\n",
    "    IN_LEN = int(d[\"in_len\"][0])\n",
    "    OUT_LEN = int(d[\"out_len\"][0])\n",
    "\n",
    "    flow_mean = d[\"flow_mean\"]; flow_std = d[\"flow_std\"]\n",
    "    speed_mean = d[\"speed_mean\"]; speed_std = d[\"speed_std\"]\n",
    "\n",
    "    T_total = X.shape[0]\n",
    "    max_t = T_total - (IN_LEN + OUT_LEN) + 1\n",
    "    starts = np.arange(max_t, dtype=np.int32)\n",
    "\n",
    "    out_start_times = timestamps[starts + IN_LEN]\n",
    "    out_end_times   = timestamps[starts + IN_LEN + OUT_LEN - 1]\n",
    "\n",
    "    train_starts = starts[out_end_times <= TRAIN_END]\n",
    "    val_starts   = starts[(out_start_times > TRAIN_END) & (out_end_times <= VAL_END)]\n",
    "    test_starts  = starts[out_start_times > VAL_END]\n",
    "\n",
    "    np.savez_compressed(\n",
    "        strict_npz,\n",
    "        X=X.astype(np.float32),\n",
    "        Y=Y.astype(np.float32),\n",
    "        A=A.astype(np.float32),\n",
    "        stations=stations.astype(np.int32),\n",
    "        timestamps=np.array(timestamps.astype(\"datetime64[ns]\")),\n",
    "        train_starts=train_starts,\n",
    "        val_starts=val_starts,\n",
    "        test_starts=test_starts,\n",
    "        in_len=np.array([IN_LEN], dtype=np.int32),\n",
    "        out_len=np.array([OUT_LEN], dtype=np.int32),\n",
    "        flow_mean=flow_mean.astype(np.float32),\n",
    "        flow_std=flow_std.astype(np.float32),\n",
    "        speed_mean=speed_mean.astype(np.float32),\n",
    "        speed_std=speed_std.astype(np.float32),\n",
    "        seed=np.array([SEED], dtype=np.int32),\n",
    "    )\n",
    "    print(\"Saved strict dataset:\", strict_npz)\n",
    "\n",
    "if (not DATASET_STRICT.exists()) and DATASET_BASE.exists():\n",
    "    make_strict_dataset(DATASET_BASE, DATASET_STRICT)\n",
    "\n",
    "assert DATASET_STRICT.exists(), \"Strict dataset not found. Make sure artifacts are created.\"\n",
    "\n",
    "d = np.load(DATASET_STRICT, allow_pickle=True)\n",
    "X = d[\"X\"]                      # (T,N,F)\n",
    "Y = d[\"Y\"]                      # (T,N)\n",
    "stations = d[\"stations\"]        # (N,)\n",
    "timestamps = pd.to_datetime(d[\"timestamps\"])\n",
    "\n",
    "train_starts = d[\"train_starts\"]\n",
    "val_starts   = d[\"val_starts\"]\n",
    "test_starts  = d[\"test_starts\"]\n",
    "\n",
    "IN_LEN = int(d[\"in_len\"][0])\n",
    "OUT_LEN = int(d[\"out_len\"][0])\n",
    "\n",
    "flow_mean = d[\"flow_mean\"]\n",
    "flow_std  = d[\"flow_std\"]\n",
    "speed_mean = d[\"speed_mean\"]\n",
    "speed_std  = d[\"speed_std\"]\n",
    "\n",
    "print(\"X:\", X.shape, \"Y:\", Y.shape)\n",
    "print(\"N stations:\", len(stations), \"T:\", len(timestamps))\n",
    "print(\"starts:\", len(train_starts), len(val_starts), len(test_starts))\n",
    "print(\"IN_LEN:\", IN_LEN, \"OUT_LEN:\", OUT_LEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2a7af3-fea0-4595-af1a-0905b94a7143",
   "metadata": {},
   "source": [
    "## Direction-aware adjacency (recommended baseline graph)\n",
    "\n",
    "We build edges only among sensors that share:\n",
    "- the same freeway (Fwy)\n",
    "- the same direction of travel (Direction of Travel)\n",
    "sorted by Abs PM.\n",
    "\n",
    "This avoids mixing opposite-direction stations, which often destroys graph model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12650ee2-2402-4821-932a-53c80bd6fcc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T05:40:33.814593Z",
     "iopub.status.busy": "2026-02-08T05:40:33.814278Z",
     "iopub.status.idle": "2026-02-08T05:40:40.943491Z",
     "shell.execute_reply": "2026-02-08T05:40:40.942309Z",
     "shell.execute_reply.started": "2026-02-08T05:40:33.814567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direction mapping coverage: 1.0\n",
      "Direction counts:\n",
      " direction\n",
      "E    509\n",
      "W    499\n",
      "S    410\n",
      "N    403\n",
      "Name: count, dtype: int64\n",
      "A_dir shape: (1821, 1821)\n",
      "A_dir density: 0.0038374073179432942\n",
      "Supports nnz: [12720, 12720]\n"
     ]
    }
   ],
   "source": [
    "TRAFFIC_CSV = Path(\"cleaned_traffic_data.csv\")\n",
    "META_XLSX   = Path(\"pems_output.xlsx\")\n",
    "\n",
    "assert TRAFFIC_CSV.exists(), \"cleaned_traffic_data.csv not found in working directory.\"\n",
    "assert META_XLSX.exists(), \"pems_output.xlsx not found in working directory.\"\n",
    "\n",
    "# 1) station -> direction mapping (mode direction per station)\n",
    "tmp = pd.read_csv(TRAFFIC_CSV, usecols=[\"Station\", \"Direction of Travel\"])\n",
    "tmp = tmp.rename(columns={\"Station\": \"station\", \"Direction of Travel\": \"direction\"})\n",
    "tmp[\"station\"] = pd.to_numeric(tmp[\"station\"], errors=\"coerce\").astype(\"Int64\")\n",
    "tmp = tmp.dropna(subset=[\"station\"])\n",
    "tmp[\"station\"] = tmp[\"station\"].astype(int)\n",
    "\n",
    "station_dir = tmp.groupby(\"station\")[\"direction\"].agg(lambda x: x.mode().iloc[0] if len(x.mode()) else x.iloc[0])\n",
    "station_dir = station_dir.reindex(stations)\n",
    "\n",
    "print(\"Direction mapping coverage:\", float(station_dir.notna().mean()))\n",
    "print(\"Direction counts:\\n\", station_dir.value_counts(dropna=False))\n",
    "\n",
    "# 2) metadata\n",
    "meta = pd.read_excel(META_XLSX)\n",
    "# your metadata already has \"station\" column (from your earlier merge output)\n",
    "if \"station\" not in meta.columns and \"ID\" in meta.columns:\n",
    "    meta = meta.rename(columns={\"ID\": \"station\"})\n",
    "meta[\"station\"] = pd.to_numeric(meta[\"station\"], errors=\"coerce\").astype(\"Int64\")\n",
    "meta = meta.dropna(subset=[\"station\"])\n",
    "meta[\"station\"] = meta[\"station\"].astype(int)\n",
    "\n",
    "assert \"Fwy\" in meta.columns and \"Abs PM\" in meta.columns, \"Metadata must contain 'Fwy' and 'Abs PM'.\"\n",
    "\n",
    "def build_adjacency_fwy_dir(meta_df, stations, station_dir, k_neighbors=4):\n",
    "    meta_sub = meta_df[meta_df[\"station\"].isin(stations)].copy()\n",
    "    meta_sub[\"fwy\"] = meta_sub[\"Fwy\"].astype(str)\n",
    "    meta_sub[\"abs_pm\"] = pd.to_numeric(meta_sub[\"Abs PM\"], errors=\"coerce\")\n",
    "    meta_sub[\"direction\"] = meta_sub[\"station\"].map(station_dir)\n",
    "\n",
    "    # If direction missing, we exclude from neighbor edges (but keep self-loop later)\n",
    "    meta_sub = meta_sub.dropna(subset=[\"abs_pm\", \"direction\"]).copy()\n",
    "    meta_sub[\"direction\"] = meta_sub[\"direction\"].astype(str)\n",
    "\n",
    "    station_to_idx = {s: i for i, s in enumerate(stations)}\n",
    "    N = len(stations)\n",
    "    A = np.zeros((N, N), dtype=np.float32)\n",
    "\n",
    "    # sigma from typical neighbor distances\n",
    "    all_dists = []\n",
    "    for (fwy, direc), grp in meta_sub.sort_values([\"fwy\", \"direction\", \"abs_pm\"]).groupby([\"fwy\", \"direction\"]):\n",
    "        pm = grp[\"abs_pm\"].values\n",
    "        if len(pm) < 2:\n",
    "            continue\n",
    "        d = np.diff(np.sort(pm))\n",
    "        d = d[d > 0]\n",
    "        all_dists.extend(d.tolist())\n",
    "\n",
    "    sigma = float(np.median(all_dists)) if len(all_dists) else 0.5\n",
    "    sigma = max(sigma, 1e-3)\n",
    "\n",
    "    def w(dist):\n",
    "        return float(np.exp(- (dist**2) / (sigma**2)))\n",
    "\n",
    "    for (fwy, direc), grp in meta_sub.sort_values([\"fwy\", \"direction\", \"abs_pm\"]).groupby([\"fwy\", \"direction\"]):\n",
    "        grp = grp.sort_values(\"abs_pm\")\n",
    "        ids = grp[\"station\"].astype(int).tolist()\n",
    "        pms = grp[\"abs_pm\"].astype(float).tolist()\n",
    "\n",
    "        for i, sid in enumerate(ids):\n",
    "            ii = station_to_idx[sid]\n",
    "            for step in range(1, k_neighbors + 1):\n",
    "                if i - step >= 0:\n",
    "                    sj = ids[i-step]; jj = station_to_idx[sj]\n",
    "                    A[ii, jj] = w(abs(pms[i] - pms[i-step]))\n",
    "                if i + step < len(ids):\n",
    "                    sj = ids[i+step]; jj = station_to_idx[sj]\n",
    "                    A[ii, jj] = w(abs(pms[i] - pms[i+step]))\n",
    "\n",
    "    np.fill_diagonal(A, 1.0)\n",
    "    A = np.maximum(A, A.T)\n",
    "    return A\n",
    "\n",
    "A_dir = build_adjacency_fwy_dir(meta, stations, station_dir, k_neighbors=4)\n",
    "print(\"A_dir shape:\", A_dir.shape)\n",
    "print(\"A_dir density:\", float((A_dir > 0).mean()))\n",
    "\n",
    "def row_normalize_dense(A, eps=1e-6):\n",
    "    d = A.sum(axis=1, keepdims=True)\n",
    "    return A / (d + eps)\n",
    "\n",
    "def dense_to_sparse(A, device):\n",
    "    idx = np.nonzero(A)\n",
    "    values = A[idx].astype(np.float32)\n",
    "    indices = np.vstack(idx)\n",
    "    return torch.sparse_coo_tensor(\n",
    "        torch.tensor(indices, dtype=torch.long, device=device),\n",
    "        torch.tensor(values, dtype=torch.float32, device=device),\n",
    "        size=A.shape,\n",
    "        device=device\n",
    "    ).coalesce()\n",
    "\n",
    "A_rw  = row_normalize_dense(A_dir)\n",
    "A_rwT = row_normalize_dense(A_dir.T)\n",
    "\n",
    "supports = [dense_to_sparse(A_rw, DEVICE), dense_to_sparse(A_rwT, DEVICE)]\n",
    "print(\"Supports nnz:\", [int(s._nnz()) for s in supports])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0906678f-e4a6-4fcf-8218-b77fc031c3f0",
   "metadata": {},
   "source": [
    "## Dataset and loaders\n",
    "\n",
    "Each sample returns:\n",
    "- x:  (F, N, IN_LEN)\n",
    "- y:  (OUT_LEN, N)  scaled flow target\n",
    "- tf: (OUT_LEN, 4)  known future calendar covariates (hour/dow sin/cos)\n",
    "\n",
    "We compute tf from timestamps (not from future traffic), so it is NOT leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99606c21-0b57-4cdd-bd70-aefb2368818b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T05:41:24.151560Z",
     "iopub.status.busy": "2026-02-08T05:41:24.151224Z",
     "iopub.status.idle": "2026-02-08T05:41:24.222135Z",
     "shell.execute_reply": "2026-02-08T05:41:24.221290Z",
     "shell.execute_reply.started": "2026-02-08T05:41:24.151534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch x: torch.Size([8, 6, 1821, 24]) Batch y: torch.Size([8, 72, 1821]) Batch tf: torch.Size([8, 72, 4])\n"
     ]
    }
   ],
   "source": [
    "def time_encoding(dt_index: pd.DatetimeIndex) -> np.ndarray:\n",
    "    hours = dt_index.hour.values\n",
    "    dow   = dt_index.dayofweek.values\n",
    "    hour_sin = np.sin(2*np.pi*hours/24.0)\n",
    "    hour_cos = np.cos(2*np.pi*hours/24.0)\n",
    "    dow_sin  = np.sin(2*np.pi*dow/7.0)\n",
    "    dow_cos  = np.cos(2*np.pi*dow/7.0)\n",
    "    return np.stack([hour_sin, hour_cos, dow_sin, dow_cos], axis=1).astype(np.float32)  # (T,4)\n",
    "\n",
    "class PemsWindowDatasetTF(Dataset):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      x:  (F, N, IN_LEN)\n",
    "      y:  (OUT_LEN, N) scaled\n",
    "      tf: (OUT_LEN, 4) known future time features\n",
    "    \"\"\"\n",
    "    def __init__(self, X, Y, timestamps, starts, in_len, out_len,\n",
    "                 flow_mean, flow_std, speed_mean, speed_std):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.timestamps = pd.DatetimeIndex(timestamps)\n",
    "        self.starts = starts.astype(np.int32)\n",
    "        self.in_len = int(in_len)\n",
    "        self.out_len = int(out_len)\n",
    "\n",
    "        self.flow_mean = flow_mean.astype(np.float32)\n",
    "        self.flow_std  = flow_std.astype(np.float32)\n",
    "        self.speed_mean = speed_mean.astype(np.float32)\n",
    "        self.speed_std  = speed_std.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.starts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t = int(self.starts[idx])\n",
    "\n",
    "        x = self.X[t:t+self.in_len].copy().astype(np.float32)  # (IN_LEN, N, F)\n",
    "        y = self.Y[t+self.in_len:t+self.in_len+self.out_len].copy().astype(np.float32)  # (OUT_LEN, N)\n",
    "\n",
    "        ts_future = self.timestamps[t+self.in_len:t+self.in_len+self.out_len]\n",
    "        tf = time_encoding(ts_future)  # (OUT_LEN, 4)\n",
    "\n",
    "        # scale inputs: flow=0 speed=1\n",
    "        x[..., 0] = (x[..., 0] - self.flow_mean[None, :]) / self.flow_std[None, :]\n",
    "        x[..., 1] = (x[..., 1] - self.speed_mean[None, :]) / self.speed_std[None, :]\n",
    "\n",
    "        # scale targets (flow)\n",
    "        y = (y - self.flow_mean[None, :]) / self.flow_std[None, :]\n",
    "\n",
    "        # (IN_LEN, N, F) -> (F, N, IN_LEN)\n",
    "        x = np.transpose(x, (2, 1, 0))\n",
    "\n",
    "        return torch.from_numpy(x), torch.from_numpy(y), torch.from_numpy(tf)\n",
    "\n",
    "# Safer batch size for GraphWaveNet on N=1821\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_ds = PemsWindowDatasetTF(X, Y, timestamps, train_starts, IN_LEN, OUT_LEN, flow_mean, flow_std, speed_mean, speed_std)\n",
    "val_ds   = PemsWindowDatasetTF(X, Y, timestamps, val_starts,   IN_LEN, OUT_LEN, flow_mean, flow_std, speed_mean, speed_std)\n",
    "test_ds  = PemsWindowDatasetTF(X, Y, timestamps, test_starts,  IN_LEN, OUT_LEN, flow_mean, flow_std, speed_mean, speed_std)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "xb, yb, tfb = next(iter(train_loader))\n",
    "print(\"Batch x:\", xb.shape, \"Batch y:\", yb.shape, \"Batch tf:\", tfb.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfec408-6279-4717-901b-66fc4c51aa30",
   "metadata": {},
   "source": [
    "## GraphWaveNet baseline (sparse diffusion graph conv + dilated temporal conv)\n",
    "\n",
    "We:\n",
    "- use causal dilated temporal convolutions + gating\n",
    "- do sparse graph propagation with the supports\n",
    "- use residual + skip connections\n",
    "- use a time-aware output head with future calendar covariates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe61380b-ab8f-4410-aa56-2b71c8416a3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T05:43:04.443729Z",
     "iopub.status.busy": "2026-02-08T05:43:04.443282Z",
     "iopub.status.idle": "2026-02-08T05:43:04.463987Z",
     "shell.execute_reply": "2026-02-08T05:43:04.462888Z",
     "shell.execute_reply.started": "2026-02-08T05:43:04.443693Z"
    }
   },
   "outputs": [],
   "source": [
    "class NConv(nn.Module):\n",
    "    \"\"\"Sparse graph multiplication: (N,N) @ (B,C,N,T) -> (B,C,N,T)\"\"\"\n",
    "    def forward(self, x, A_sp):\n",
    "        B, C, N, T = x.shape\n",
    "        x_r = x.permute(2, 0, 1, 3).reshape(N, -1)  # (N, B*C*T)\n",
    "\n",
    "        # sparse.mm requires float32 on cuda reliably\n",
    "        out = torch.sparse.mm(A_sp, x_r.float())\n",
    "        out = out.reshape(N, B, C, T).permute(1, 2, 0, 3)\n",
    "        return out.to(dtype=x.dtype)\n",
    "\n",
    "class DiffusionGraphConv(nn.Module):\n",
    "    def __init__(self, c_in, c_out, supports, order=1, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.nconv = NConv()\n",
    "        self.supports = supports\n",
    "        self.order = order\n",
    "        self.dropout = dropout\n",
    "\n",
    "        c_total = c_in * (1 + len(supports) * order)\n",
    "        self.mlp = nn.Conv2d(c_total, c_out, kernel_size=(1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = [x]\n",
    "        for A in self.supports:\n",
    "            x1 = self.nconv(x, A)\n",
    "            out.append(x1)\n",
    "            for _ in range(2, self.order + 1):\n",
    "                x1 = self.nconv(x1, A)\n",
    "                out.append(x1)\n",
    "\n",
    "        h = torch.cat(out, dim=1)\n",
    "        h = self.mlp(h)\n",
    "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        return h\n",
    "\n",
    "class CausalConv2d(nn.Module):\n",
    "    \"\"\"Causal conv along time axis only.\"\"\"\n",
    "    def __init__(self, c_in, c_out, kernel_size=2, dilation=1):\n",
    "        super().__init__()\n",
    "        self.pad = (kernel_size - 1) * dilation\n",
    "        self.conv = nn.Conv2d(c_in, c_out, kernel_size=(1, kernel_size), dilation=(1, dilation))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.pad(x, (self.pad, 0, 0, 0))\n",
    "        return self.conv(x)\n",
    "\n",
    "class GraphWaveNetTimeAware(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_nodes,\n",
    "        in_dim,\n",
    "        out_len,\n",
    "        supports,\n",
    "        residual_channels=32,\n",
    "        dilation_channels=32,\n",
    "        skip_channels=64,\n",
    "        end_channels=128,\n",
    "        kernel_size=2,\n",
    "        blocks=2,\n",
    "        layers_per_block=4,   # ensures receptive field >= 24\n",
    "        gcn_order=1,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.in_dim = in_dim\n",
    "        self.out_len = out_len\n",
    "        self.dropout = dropout\n",
    "        self.kernel_size = kernel_size\n",
    "        self.blocks = blocks\n",
    "        self.layers_per_block = layers_per_block\n",
    "\n",
    "        # receptive field\n",
    "        receptive_field = 1\n",
    "        for _ in range(blocks):\n",
    "            for i in range(layers_per_block):\n",
    "                receptive_field += (kernel_size - 1) * (2 ** i)\n",
    "        self.receptive_field = receptive_field\n",
    "\n",
    "        self.start_conv = nn.Conv2d(in_dim, residual_channels, kernel_size=(1, 1))\n",
    "\n",
    "        self.filter_convs = nn.ModuleList()\n",
    "        self.gate_convs   = nn.ModuleList()\n",
    "        self.skip_convs   = nn.ModuleList()\n",
    "        self.bn           = nn.ModuleList()\n",
    "        self.gconvs       = nn.ModuleList()\n",
    "\n",
    "        for _ in range(blocks):\n",
    "            for i in range(layers_per_block):\n",
    "                dilation = 2 ** i\n",
    "                self.filter_convs.append(CausalConv2d(residual_channels, dilation_channels, kernel_size, dilation))\n",
    "                self.gate_convs.append(CausalConv2d(residual_channels, dilation_channels, kernel_size, dilation))\n",
    "\n",
    "                self.skip_convs.append(nn.Conv2d(dilation_channels, skip_channels, kernel_size=(1, 1)))\n",
    "                self.gconvs.append(DiffusionGraphConv(dilation_channels, residual_channels, supports, order=gcn_order, dropout=dropout))\n",
    "                self.bn.append(nn.BatchNorm2d(residual_channels))\n",
    "\n",
    "        self.end_conv_1 = nn.Conv2d(skip_channels, end_channels, kernel_size=(1, 1))\n",
    "\n",
    "        # time-aware head\n",
    "        self.time_embed = nn.Linear(4, end_channels)\n",
    "        self.horizon_out = nn.Linear(end_channels, 1)\n",
    "\n",
    "    def forward(self, x, tf_future):\n",
    "        \"\"\"\n",
    "        x:        (B, F, N, IN_LEN)\n",
    "        tf_future:(B, OUT_LEN, 4)\n",
    "        out:      (B, OUT_LEN, N)\n",
    "        \"\"\"\n",
    "        # pad input if needed to match receptive field\n",
    "        if x.size(-1) < self.receptive_field:\n",
    "            pad_len = self.receptive_field - x.size(-1)\n",
    "            x = F.pad(x, (pad_len, 0, 0, 0))\n",
    "\n",
    "        x = self.start_conv(x)  # (B, residual, N, T)\n",
    "        skip = None\n",
    "\n",
    "        for i in range(len(self.filter_convs)):\n",
    "            residual = x\n",
    "\n",
    "            filt = torch.tanh(self.filter_convs[i](x))\n",
    "            gate = torch.sigmoid(self.gate_convs[i](x))\n",
    "            x = filt * gate\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "            s = self.skip_convs[i](x)\n",
    "            skip = s if skip is None else (skip + s)\n",
    "\n",
    "            x = self.gconvs[i](x)\n",
    "            x = x + residual\n",
    "            x = self.bn[i](x)\n",
    "\n",
    "        x = F.relu(skip)\n",
    "        x = F.relu(self.end_conv_1(x))     # (B, end_channels, N, T)\n",
    "\n",
    "        z = x[..., -1]                     # (B, end_channels, N)\n",
    "        z = z.permute(0, 2, 1)             # (B, N, end_channels)\n",
    "\n",
    "        te = self.time_embed(tf_future)    # (B, OUT_LEN, end_channels)\n",
    "\n",
    "        h = F.relu(z.unsqueeze(1) + te.unsqueeze(2))  # (B, OUT_LEN, N, end_channels)\n",
    "        out = self.horizon_out(h).squeeze(-1)         # (B, OUT_LEN, N)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9401fe2-eea4-46ed-8ec2-96289780940a",
   "metadata": {},
   "source": [
    "## Training + evaluation\n",
    "\n",
    "- Loss: SmoothL1 (Huber) on SCALED targets\n",
    "- Metrics: MAE/RMSE on ORIGINAL scale\n",
    "- Early stopping on average validation MAE (across horizons)\n",
    "- AMP disabled (sparse graph ops do not support fp16 reliably)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a40363-ba7f-456f-8e7b-b1fb10bd5445",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T05:43:54.926801Z",
     "iopub.status.busy": "2026-02-08T05:43:54.926420Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f544b23a86124fdf99f1f8b046c03b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/40:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d610ce314b914f019b9319be99aa1919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: train_loss=0.166336  val_avg_MAE=192.611\n",
      "\n",
      "Validation metrics\n",
      "   12h  MAE=191.403  RMSE=358.438\n",
      "   24h  MAE=189.733  RMSE=363.071\n",
      "   48h  MAE=196.351  RMSE=372.638\n",
      "   72h  MAE=192.957  RMSE=360.658\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef15afb91a84425cbad2132a929a6a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/40:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2046b04b4e0473eab72d8d93d123eb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: train_loss=0.117438  val_avg_MAE=189.943\n",
      "\n",
      "Validation metrics\n",
      "   12h  MAE=182.940  RMSE=338.154\n",
      "   24h  MAE=187.478  RMSE=348.945\n",
      "   48h  MAE=195.416  RMSE=359.328\n",
      "   72h  MAE=193.937  RMSE=352.510\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "782135e6b3184bdf9156d49a663d22a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/40:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc2d1a3cb5884b58ade52ffd0889c527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: train_loss=0.104683  val_avg_MAE=183.902\n",
      "\n",
      "Validation metrics\n",
      "   12h  MAE=178.179  RMSE=326.433\n",
      "   24h  MAE=180.125  RMSE=333.626\n",
      "   48h  MAE=189.455  RMSE=346.797\n",
      "   72h  MAE=187.850  RMSE=341.038\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48907e6d35334654ae684401bd4d5c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/40:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a4ec63100e4a8ea43b11429f6d82b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: train_loss=0.097027  val_avg_MAE=166.079\n",
      "\n",
      "Validation metrics\n",
      "   12h  MAE=158.517  RMSE=299.517\n",
      "   24h  MAE=160.295  RMSE=308.539\n",
      "   48h  MAE=172.899  RMSE=327.715\n",
      "   72h  MAE=172.604  RMSE=323.193\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e0c5a4a9984884a0dd28fcdd1bcd49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/40:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0d551cdba94d48b7855f02c050892c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: train_loss=0.091661  val_avg_MAE=167.928\n",
      "\n",
      "Validation metrics\n",
      "   12h  MAE=158.100  RMSE=296.278\n",
      "   24h  MAE=162.273  RMSE=308.964\n",
      "   48h  MAE=175.807  RMSE=330.409\n",
      "   72h  MAE=175.530  RMSE=328.313\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475f18ad4c964cfc9ee1f11f328a4be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/40:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bfdd3cce71f469aa90efbbbcf9dd2bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6: train_loss=0.088536  val_avg_MAE=163.555\n",
      "\n",
      "Validation metrics\n",
      "   12h  MAE=154.952  RMSE=290.983\n",
      "   24h  MAE=155.721  RMSE=300.056\n",
      "   48h  MAE=170.827  RMSE=322.243\n",
      "   72h  MAE=172.719  RMSE=322.721\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e504d85cc21b42889539fbbee3013e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/40:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f74b379fc944548ac23b6a04c142abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7: train_loss=0.085676  val_avg_MAE=168.625\n",
      "\n",
      "Validation metrics\n",
      "   12h  MAE=154.776  RMSE=290.849\n",
      "   24h  MAE=162.844  RMSE=308.113\n",
      "   48h  MAE=177.034  RMSE=329.264\n",
      "   72h  MAE=179.846  RMSE=333.807\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0542c91b7a984e3f8c20384bf53f0b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/40:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4436a2a8e414f2586eaf94001473038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: train_loss=0.083878  val_avg_MAE=165.275\n",
      "\n",
      "Validation metrics\n",
      "   12h  MAE=150.444  RMSE=283.392\n",
      "   24h  MAE=157.638  RMSE=302.180\n",
      "   48h  MAE=174.519  RMSE=327.460\n",
      "   72h  MAE=178.499  RMSE=334.144\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1946e5fc87c842cca81b3cbc6cff9f19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/40:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704abe82f3e64f31b1c3155da6703d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: train_loss=0.082351  val_avg_MAE=161.167\n",
      "\n",
      "Validation metrics\n",
      "   12h  MAE=143.012  RMSE=274.647\n",
      "   24h  MAE=154.429  RMSE=299.217\n",
      "   48h  MAE=172.190  RMSE=325.996\n",
      "   72h  MAE=175.035  RMSE=329.621\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4252ea1ddee84f6398033113f2622a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/40:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b595fe7c2ca4c258993fc3419ef59f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: train_loss=0.081194  val_avg_MAE=161.562\n",
      "\n",
      "Validation metrics\n",
      "   12h  MAE=144.849  RMSE=278.380\n",
      "   24h  MAE=156.305  RMSE=303.425\n",
      "   48h  MAE=171.748  RMSE=326.235\n",
      "   72h  MAE=173.348  RMSE=327.463\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "862ef895a6884a0d8e6227a6165f91eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/40:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae437b8942545d584643d330e52fe49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11: train_loss=0.079560  val_avg_MAE=156.545\n",
      "\n",
      "Validation metrics\n",
      "   12h  MAE=140.382  RMSE=272.981\n",
      "   24h  MAE=149.853  RMSE=294.445\n",
      "   48h  MAE=166.326  RMSE=321.796\n",
      "   72h  MAE=169.619  RMSE=328.382\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05286d9ffadc4f58bea2cfd7ceb5dac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/40:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df17ff3aae07489f9b9bb8e59b54750c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12: train_loss=0.078578  val_avg_MAE=153.288\n",
      "\n",
      "Validation metrics\n",
      "   12h  MAE=135.332  RMSE=264.037\n",
      "   24h  MAE=145.388  RMSE=287.101\n",
      "   48h  MAE=164.178  RMSE=317.646\n",
      "   72h  MAE=168.254  RMSE=326.255\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "726f4b10da7e4ee0974bc94fd3200328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/40:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e4db7f36c940f48253d433a6ba658a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13: train_loss=0.077964  val_avg_MAE=147.968\n",
      "\n",
      "Validation metrics\n",
      "   12h  MAE=132.972  RMSE=263.475\n",
      "   24h  MAE=139.162  RMSE=277.580\n",
      "   48h  MAE=157.886  RMSE=305.720\n",
      "   72h  MAE=161.853  RMSE=312.469\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac60c61bc4a4527ab79016e152a9247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/40:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652438b6a8d1452b92b42620ad91e1d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: train_loss=0.076900  val_avg_MAE=154.376\n",
      "\n",
      "Validation metrics\n",
      "   12h  MAE=139.283  RMSE=273.490\n",
      "   24h  MAE=146.242  RMSE=291.323\n",
      "   48h  MAE=164.243  RMSE=320.756\n",
      "   72h  MAE=167.738  RMSE=327.017\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de5bd8cae794489596f3b79b5b6d4cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/40:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9c8bde38c34402ab2d556ac7dbc0b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15: train_loss=0.075930  val_avg_MAE=155.880\n",
      "\n",
      "Validation metrics\n",
      "   12h  MAE=137.547  RMSE=271.921\n",
      "   24h  MAE=148.102  RMSE=298.113\n",
      "   48h  MAE=167.544  RMSE=326.551\n",
      "   72h  MAE=170.325  RMSE=330.410\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c22a5092c844d69b2bdecbcc3a3778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/40:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "952f2841af734ad69f7ad4fc38722738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16: train_loss=0.075627  val_avg_MAE=151.961\n",
      "\n",
      "Validation metrics\n",
      "   12h  MAE=135.106  RMSE=264.507\n",
      "   24h  MAE=143.022  RMSE=284.876\n",
      "   48h  MAE=162.791  RMSE=315.517\n",
      "   72h  MAE=166.925  RMSE=322.595\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0141a219f38465bb0888effd9c3fff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/40:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43da3e4f69f04a368907eabdd08075ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17: train_loss=0.075027  val_avg_MAE=154.090\n",
      "\n",
      "Validation metrics\n",
      "   12h  MAE=137.682  RMSE=268.177\n",
      "   24h  MAE=145.152  RMSE=287.310\n",
      "   48h  MAE=164.215  RMSE=319.679\n",
      "   72h  MAE=169.310  RMSE=329.051\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6705895a11ae44218c6a002e704f2cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/40:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9fe90d0642542ed83aba6e34947322a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18: train_loss=0.074583  val_avg_MAE=160.386\n",
      "\n",
      "Validation metrics\n",
      "   12h  MAE=138.560  RMSE=268.469\n",
      "   24h  MAE=151.101  RMSE=294.014\n",
      "   48h  MAE=172.828  RMSE=328.903\n",
      "   72h  MAE=179.054  RMSE=340.562\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2036a427b9411c9c682351c6a7deb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/40:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2bc07ab43e43b98135d21e89775743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19: train_loss=0.074133  val_avg_MAE=154.503\n",
      "\n",
      "Validation metrics\n",
      "   12h  MAE=137.373  RMSE=262.704\n",
      "   24h  MAE=145.690  RMSE=282.779\n",
      "   48h  MAE=165.019  RMSE=313.960\n",
      "   72h  MAE=169.931  RMSE=324.774\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a2e997c336442faad8099e2635195d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/40:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0917a42da54d489d19fd5f75d304f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20: train_loss=0.074154  val_avg_MAE=154.026\n",
      "\n",
      "Validation metrics\n",
      "   12h  MAE=135.481  RMSE=266.801\n",
      "   24h  MAE=145.055  RMSE=289.757\n",
      "   48h  MAE=165.571  RMSE=322.917\n",
      "   72h  MAE=169.995  RMSE=331.973\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be45a6453024c6380ba33f9523d1c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 21/40:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a93038535d84a46a61c63b6c37497fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21: train_loss=0.073161  val_avg_MAE=142.855\n",
      "\n",
      "Validation metrics\n",
      "   12h  MAE=126.640  RMSE=256.993\n",
      "   24h  MAE=134.450  RMSE=273.870\n",
      "   48h  MAE=153.180  RMSE=302.537\n",
      "   72h  MAE=157.152  RMSE=309.364\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513e985d94e44993b6c2d1fe27df8150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 22/40:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d19adfec93644d5cb0e9cefd8172a3d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22: train_loss=0.073137  val_avg_MAE=148.385\n",
      "\n",
      "Validation metrics\n",
      "   12h  MAE=130.637  RMSE=263.231\n",
      "   24h  MAE=139.916  RMSE=283.505\n",
      "   48h  MAE=159.444  RMSE=314.125\n",
      "   72h  MAE=163.542  RMSE=323.153\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72277c13b22543f8a0ca72ce88ec53a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 23/40:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a6814e1481c472693eb0f04c7a2d4f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23: train_loss=0.072553  val_avg_MAE=147.939\n",
      "\n",
      "Validation metrics\n",
      "   12h  MAE=132.895  RMSE=261.557\n",
      "   24h  MAE=137.680  RMSE=276.316\n",
      "   48h  MAE=158.085  RMSE=309.733\n",
      "   72h  MAE=163.094  RMSE=321.161\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de752f9d684b42378fc6c05babffbec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 24/40:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EVAL_HORIZONS = [12, 24, 48, 72]\n",
    "\n",
    "flow_mean_t = torch.tensor(flow_mean, dtype=torch.float32, device=DEVICE).view(1, 1, -1)\n",
    "flow_std_t  = torch.tensor(flow_std,  dtype=torch.float32, device=DEVICE).view(1, 1, -1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_horizons(model, loader):\n",
    "    model.eval()\n",
    "    acc = {h: {\"abs\": 0.0, \"sq\": 0.0, \"count\": 0} for h in EVAL_HORIZONS}\n",
    "\n",
    "    for xb, yb, tfb in tqdm(loader, desc=\"Eval\", leave=False):\n",
    "        xb = xb.to(DEVICE, non_blocking=True)\n",
    "        yb = yb.to(DEVICE, non_blocking=True)\n",
    "        tfb = tfb.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        pred = model(xb, tfb)  # scaled\n",
    "\n",
    "        pred_u = pred * flow_std_t + flow_mean_t\n",
    "        true_u = yb   * flow_std_t + flow_mean_t\n",
    "\n",
    "        for h in EVAL_HORIZONS:\n",
    "            idx = h - 1\n",
    "            err = pred_u[:, idx, :] - true_u[:, idx, :]\n",
    "            acc[h][\"abs\"] += float(err.abs().sum())\n",
    "            acc[h][\"sq\"]  += float((err ** 2).sum())\n",
    "            acc[h][\"count\"] += err.numel()\n",
    "\n",
    "    metrics = {}\n",
    "    for h in EVAL_HORIZONS:\n",
    "        mae = acc[h][\"abs\"] / acc[h][\"count\"]\n",
    "        rmse = (acc[h][\"sq\"] / acc[h][\"count\"]) ** 0.5\n",
    "        metrics[h] = {\"MAE\": mae, \"RMSE\": rmse}\n",
    "    return metrics\n",
    "\n",
    "def print_metrics(title, metrics):\n",
    "    print(\"\\n\" + title)\n",
    "    for h in sorted(metrics.keys()):\n",
    "        print(f\"  {h:>3}h  MAE={metrics[h]['MAE']:.3f}  RMSE={metrics[h]['RMSE']:.3f}\")\n",
    "\n",
    "def avg_mae(metrics):\n",
    "    return float(np.mean([metrics[h][\"MAE\"] for h in metrics]))\n",
    "\n",
    "def train_gwn_baseline(\n",
    "    epochs=40,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    clip=5.0,\n",
    "    patience=8,\n",
    "):\n",
    "    model = GraphWaveNetTimeAware(\n",
    "        num_nodes=X.shape[1],\n",
    "        in_dim=X.shape[2],\n",
    "        out_len=OUT_LEN,\n",
    "        supports=supports,\n",
    "        residual_channels=32,\n",
    "        dilation_channels=32,\n",
    "        skip_channels=64,\n",
    "        end_channels=128,\n",
    "        kernel_size=2,\n",
    "        blocks=2,\n",
    "        layers_per_block=4,\n",
    "        gcn_order=1,\n",
    "        dropout=0.1,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    loss_fn = nn.SmoothL1Loss(beta=1.0)\n",
    "\n",
    "    best_score = float(\"inf\")\n",
    "    best_state = None\n",
    "    bad = 0\n",
    "\n",
    "    # IMPORTANT: disable AMP due to sparse graph ops\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "\n",
    "        for xb, yb, tfb in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\", leave=False):\n",
    "            xb = xb.to(DEVICE, non_blocking=True)\n",
    "            yb = yb.to(DEVICE, non_blocking=True)\n",
    "            tfb = tfb.to(DEVICE, non_blocking=True)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            pred = model(xb, tfb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            opt.step()\n",
    "\n",
    "            running += float(loss.item())\n",
    "\n",
    "        val_metrics = eval_horizons(model, val_loader)\n",
    "        score = avg_mae(val_metrics)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch}: train_loss={running/len(train_loader):.6f}  val_avg_MAE={score:.3f}\")\n",
    "        print_metrics(\"Validation metrics\", val_metrics)\n",
    "\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                print(f\"\\nEarly stopping. Best val_avg_MAE={best_score:.3f}\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "gwn = train_gwn_baseline()\n",
    "\n",
    "val_m = eval_horizons(gwn, val_loader)\n",
    "test_m = eval_horizons(gwn, test_loader)\n",
    "\n",
    "print_metrics(\"GraphWaveNet Baseline — Validation\", val_m)\n",
    "print_metrics(\"GraphWaveNet Baseline — Test\", test_m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf88281-66c5-4424-9a82-8dd204ab0f03",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

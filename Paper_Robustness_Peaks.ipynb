{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0a4a92c-82d9-4eb5-9208-2443da3ff6d6",
   "metadata": {},
   "source": [
    "# Paper Notebook: Robustness & Peak-Critical Evaluation\n",
    "\n",
    "This notebook is **evaluation-only** (publication-focused).\n",
    "It loads the pre-built strict dataset artifact and trained model weights, then produces:\n",
    "\n",
    "- Table A: clean MAE/RMSE (12/24/48/72h)\n",
    "- Table B: sensor outage robustness (overall + masked-only)\n",
    "- Table C: peak-only MAE/RMSE\n",
    "- Figure: masked-only MAE@72 vs outage rate\n",
    "\n",
    "Training is performed in the separate *STGCN training notebook* and saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2235d85-bd79-4fb7-bdd3-d785a4314dc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: Quadro P5000\n"
     ]
    }
   ],
   "source": [
    "import os, json, math, time, random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "if DEVICE == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81e08e97-a37a-402f-9fdc-30249bc62ecd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: artifacts/pems_graph_dataset_strict.npz\n",
      "Keys: ['X', 'Y', 'A', 'stations', 'timestamps', 'train_starts', 'val_starts', 'test_starts', 'in_len', 'out_len', 'flow_mean', 'flow_std', 'speed_mean', 'speed_std']\n",
      "Shapes:\n",
      "  X_raw: (2208, 1821, 6) (T,N,F)\n",
      "  Y_raw: (2208, 1821) (T,N)\n",
      "  A    : (1821, 1821) (N,N)\n",
      "  IN_LEN/OUT_LEN: 24 72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_526/2834048384.py:25: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  IN_LEN  = int(data[\"in_len\"])   # your L\n",
      "/tmp/ipykernel_526/2834048384.py:26: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  OUT_LEN = int(data[\"out_len\"])  # your H\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = Path(\"artifacts/pems_graph_dataset_strict.npz\")\n",
    "assert DATA_PATH.exists(), (\n",
    "    f\"Missing {DATA_PATH}.\\n\"\n",
    "    \"Fix:\\n\"\n",
    "    \"1) Run the dataset-build part of your STGCN notebook (it creates this file), OR\\n\"\n",
    "    \"2) Make sure you copied the entire 'artifacts/' folder into this project directory.\"\n",
    ")\n",
    "\n",
    "data = np.load(DATA_PATH, allow_pickle=True)\n",
    "print(\"Loaded:\", DATA_PATH)\n",
    "print(\"Keys:\", list(data.keys()))\n",
    "\n",
    "X_raw = data[\"X\"].astype(np.float32)   # (T, N, F) raw flow/speed + possibly other raw features\n",
    "Y_raw = data[\"Y\"].astype(np.float32)   # (T, N) raw flow target\n",
    "A     = data[\"A\"].astype(np.float32)   # (N, N) adjacency\n",
    "\n",
    "stations   = data[\"stations\"]          # (N,)\n",
    "timestamps = data[\"timestamps\"]        # (T,)\n",
    "\n",
    "flow_mean  = data[\"flow_mean\"].astype(np.float32)   # (N,)\n",
    "flow_std   = data[\"flow_std\"].astype(np.float32)    # (N,)\n",
    "speed_mean = data[\"speed_mean\"].astype(np.float32)  # (N,)\n",
    "speed_std  = data[\"speed_std\"].astype(np.float32)   # (N,)\n",
    "\n",
    "IN_LEN  = int(data[\"in_len\"])   # your L\n",
    "OUT_LEN = int(data[\"out_len\"])  # your H\n",
    "\n",
    "train_starts = data[\"train_starts\"].astype(np.int64)\n",
    "val_starts   = data[\"val_starts\"].astype(np.int64)\n",
    "test_starts  = data[\"test_starts\"].astype(np.int64)\n",
    "\n",
    "T, N, F_in = X_raw.shape\n",
    "print(\"Shapes:\")\n",
    "print(\"  X_raw:\", X_raw.shape, \"(T,N,F)\")\n",
    "print(\"  Y_raw:\", Y_raw.shape, \"(T,N)\")\n",
    "print(\"  A    :\", A.shape, \"(N,N)\")\n",
    "print(\"  IN_LEN/OUT_LEN:\", IN_LEN, OUT_LEN)\n",
    "\n",
    "assert T >= (IN_LEN + OUT_LEN), f\"T={T} is too small for IN_LEN+OUT_LEN={IN_LEN+OUT_LEN}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdac4fc4-86f6-4c35-b543-ca5009d84d57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_fnt: (6, 1821, 2208) | Y_scaled: (2208, 1821) | TF_all: (2208, 4)\n"
     ]
    }
   ],
   "source": [
    "def time_encoding(dt_index: pd.DatetimeIndex) -> np.ndarray:\n",
    "    hours = dt_index.hour.values\n",
    "    dow   = dt_index.dayofweek.values\n",
    "    hour_sin = np.sin(2*np.pi*hours/24.0)\n",
    "    hour_cos = np.cos(2*np.pi*hours/24.0)\n",
    "    dow_sin  = np.sin(2*np.pi*dow/7.0)\n",
    "    dow_cos  = np.cos(2*np.pi*dow/7.0)\n",
    "    return np.stack([hour_sin, hour_cos, dow_sin, dow_cos], axis=1).astype(np.float32)\n",
    "\n",
    "dt_idx = pd.to_datetime(timestamps)\n",
    "TF_all = time_encoding(dt_idx)         # (T,4)\n",
    "\n",
    "# Scale inputs\n",
    "X_scaled = X_raw.copy()\n",
    "# Assumption consistent with your STGCN notebook: channel 0=flow, channel 1=speed\n",
    "X_scaled[:, :, 0] = (X_scaled[:, :, 0] - flow_mean[None, :]) / (flow_std[None, :] + 1e-6)\n",
    "X_scaled[:, :, 1] = (X_scaled[:, :, 1] - speed_mean[None, :]) / (speed_std[None, :] + 1e-6)\n",
    "\n",
    "# Scale target (flow)\n",
    "Y_scaled = (Y_raw - flow_mean[None, :]) / (flow_std[None, :] + 1e-6)\n",
    "\n",
    "# For fast slicing: (F,N,T)\n",
    "X_fnt = np.transpose(X_scaled, (2, 1, 0)).copy()  # (F,N,T)\n",
    "\n",
    "print(\"X_fnt:\", X_fnt.shape, \"| Y_scaled:\", Y_scaled.shape, \"| TF_all:\", TF_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78d381d-5718-4c7d-b6b9-1e6aa6b9dc96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
